  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2025-02-20T17:24:38.733+0100] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2025-02-20T17:24:38.808+0100] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-02-20T17:24:38.808+0100] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-02-20T17:24:38.811+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 38220
[2025-02-20T17:24:38.813+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20 17:24:39 +0100] [38219] [INFO] Starting gunicorn 23.0.0
[2025-02-20 17:24:39 +0100] [38219] [ERROR] Connection in use: ('::', 8793)
[2025-02-20 17:24:39 +0100] [38219] [ERROR] connection to ('::', 8793) failed: [Errno 48] Address already in use
[2025-02-20T17:24:39.554+0100] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-20T17:24:39.562+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-20T17:24:40.520+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-20 00:00:00+00:00, run_after=2024-04-21 00:00:00+00:00
[2025-02-20 17:24:40 +0100] [38219] [ERROR] Connection in use: ('::', 8793)
[2025-02-20 17:24:40 +0100] [38219] [ERROR] connection to ('::', 8793) failed: [Errno 48] Address already in use
[2025-02-20 17:24:41 +0100] [38219] [ERROR] Connection in use: ('::', 8793)
[2025-02-20 17:24:41 +0100] [38219] [ERROR] connection to ('::', 8793) failed: [Errno 48] Address already in use
[2025-02-20 17:24:42 +0100] [38219] [ERROR] Connection in use: ('::', 8793)
[2025-02-20 17:24:42 +0100] [38219] [ERROR] connection to ('::', 8793) failed: [Errno 48] Address already in use
[2025-02-20T17:24:42.862+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-26 00:00:00+00:00, run_after=2024-04-27 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:24:42.856094+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:42.880+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-21 00:00:00+00:00: scheduled__2024-04-21T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:52:04.785954+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-21T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:24:42.881+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-21 00:00:00+00:00, run_id=scheduled__2024-04-21T00:00:00+00:00, run_start_date=2025-02-20 15:52:04.792220+00:00, run_end_date=2025-02-20 16:24:42.881285+00:00, run_duration=1958.089065, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-21 00:00:00+00:00, data_interval_end=2024-04-22 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:42.883+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-22 00:00:00+00:00, run_after=2024-04-23 00:00:00+00:00
[2025-02-20T17:24:42.884+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-20 00:00:00+00:00: scheduled__2024-04-20T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:52:02.358344+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-20T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:24:42.884+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-20 00:00:00+00:00, run_id=scheduled__2024-04-20T00:00:00+00:00, run_start_date=2025-02-20 15:52:02.363656+00:00, run_end_date=2025-02-20 16:24:42.884828+00:00, run_duration=1960.521172, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-20 00:00:00+00:00, data_interval_end=2024-04-21 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:42.886+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-21 00:00:00+00:00, run_after=2024-04-22 00:00:00+00:00
[2025-02-20T17:24:42.887+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-19 00:00:00+00:00: scheduled__2024-04-19T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:52:00.115549+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-19T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:24:42.887+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-19 00:00:00+00:00, run_id=scheduled__2024-04-19T00:00:00+00:00, run_start_date=2025-02-20 15:52:00.122155+00:00, run_end_date=2025-02-20 16:24:42.887240+00:00, run_duration=1962.765085, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-19 00:00:00+00:00, data_interval_end=2024-04-20 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:42.888+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-20 00:00:00+00:00, run_after=2024-04-21 00:00:00+00:00
[2025-02-20T17:24:42.893+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:42.893+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:24:42.893+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:42.896+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:24:42.896+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-25T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:24:42.896+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:42.897+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20 17:24:43 +0100] [38219] [ERROR] Connection in use: ('::', 8793)
[2025-02-20 17:24:43 +0100] [38219] [ERROR] connection to ('::', 8793) failed: [Errno 48] Address already in use
[2025-02-20T17:24:43.866+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:24:43.899+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:24:43.899+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:43.908+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:43.909+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:24:44.054+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:44.135+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:44.173+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20 17:24:44 +0100] [38219] [ERROR] Can't connect to ('::', 8793)
[2025-02-20T17:24:45.215+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-25T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:24:45.229+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:24:44.265179+00:00, run_end_date=2025-02-20 16:24:44.781279+00:00, run_duration=0.5161, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=99, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:24:42.894049+00:00, queued_by_job_id=94, pid=38248
[2025-02-20T17:24:47.416+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-21 00:00:00+00:00, run_after=2024-04-22 00:00:00+00:00
[2025-02-20T17:24:47.434+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-22 00:00:00+00:00: scheduled__2024-04-22T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:52:07.355126+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-22T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:24:47.434+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-22 00:00:00+00:00, run_id=scheduled__2024-04-22T00:00:00+00:00, run_start_date=2025-02-20 15:52:07.365708+00:00, run_end_date=2025-02-20 16:24:47.434572+00:00, run_duration=1960.068864, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-22 00:00:00+00:00, data_interval_end=2024-04-23 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:47.437+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-23 00:00:00+00:00, run_after=2024-04-24 00:00:00+00:00
[2025-02-20T17:24:49.711+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-28 00:00:00+00:00, run_after=2024-04-29 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:24:49.706461+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:49.732+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:49.733+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:24:49.733+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:49.734+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:24:49.734+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-27T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:24:49.735+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:49.736+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:50.615+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:24:50.647+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:24:50.647+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:50.663+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:50.663+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:24:50.806+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:50.870+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:50.915+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:24:51.391+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-27T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:24:51.396+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:24:50.993797+00:00, run_end_date=2025-02-20 16:24:51.097419+00:00, run_duration=0.103622, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=102, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:24:49.733612+00:00, queued_by_job_id=94, pid=38285
[2025-02-20T17:24:52.514+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-01 00:00:00+00:00, run_after=2024-05-02 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:24:52.512041+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:52.539+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:52.539+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:24:52.539+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:52.540+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:24:52.540+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-30T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:24:52.541+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:52.541+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:53.422+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:24:53.453+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:24:53.453+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:53.462+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:53.463+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:24:53.577+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:53.637+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:53.673+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:24:54.135+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-30T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:24:54.139+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:24:53.751021+00:00, run_end_date=2025-02-20 16:24:53.852247+00:00, run_duration=0.101226, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=105, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:24:52.540206+00:00, queued_by_job_id=94, pid=38308
[2025-02-20T17:24:55.216+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-04 00:00:00+00:00, run_after=2024-05-05 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:24:55.213450+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:24:55.245+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:55.245+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:24:55.245+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:24:55.246+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:24:55.246+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:24:55.247+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:55.247+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:24:56.442+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:24:56.473+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:24:56.473+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:56.482+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:56.482+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:24:56.663+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:24:57.030+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:24:57.169+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:24:58.848+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-03T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:24:58.858+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:24:57.332448+00:00, run_end_date=2025-02-20 16:24:57.965923+00:00, run_duration=0.633475, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=108, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:24:55.246016+00:00, queued_by_job_id=94, pid=38329
[2025-02-20T17:25:01.062+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-07 00:00:00+00:00, run_after=2024-05-08 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:25:01.056093+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:25:01.118+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:25:01.119+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:25:01.119+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:25:01.120+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:25:01.121+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:25:01.121+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:25:01.122+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:25:02.213+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:25:02.245+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:25:02.246+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:02.256+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:02.256+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:25:02.368+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:02.428+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:02.483+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:25:03.140+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:25:03.147+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:25:02.604112+00:00, run_end_date=2025-02-20 16:25:02.797363+00:00, run_duration=0.193251, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=111, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:25:01.119970+00:00, queued_by_job_id=94, pid=38351
[2025-02-20T17:25:04.334+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-10 00:00:00+00:00, run_after=2024-05-11 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:25:04.331064+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:25:04.385+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:25:04.385+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:25:04.385+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:25:04.386+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:25:04.387+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-09T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:25:04.387+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:25:04.388+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:25:05.246+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:25:05.279+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:25:05.279+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:05.288+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:05.289+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:25:05.403+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:05.458+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:05.493+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:25:06.248+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-09T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:25:06.259+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:25:05.594683+00:00, run_end_date=2025-02-20 16:25:05.887421+00:00, run_duration=0.292738, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=114, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:25:04.386171+00:00, queued_by_job_id=94, pid=38367
[2025-02-20T17:25:07.583+0100] {dag.py:4180} INFO - Setting next_dagrun for bash_operator_disk_usage to 2025-02-20 00:00:00+00:00, run_after=2025-02-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:25:07.577776+00:00 hash info: e0bbe7b12108063d65a47698d3e1fcd1
[2025-02-20T17:25:07.712+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: bash_operator_disk_usage.check_disk_usage scheduled__2025-02-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:25:07.712+0100] {scheduler_job_runner.py:507} INFO - DAG bash_operator_disk_usage has 0/16 running and queued tasks
[2025-02-20T17:25:07.713+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: bash_operator_disk_usage.check_disk_usage scheduled__2025-02-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:25:07.715+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: bash_operator_disk_usage.check_disk_usage scheduled__2025-02-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:25:07.715+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='bash_operator_disk_usage', task_id='check_disk_usage', run_id='scheduled__2025-02-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:25:07.715+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'bash_operator_disk_usage', 'check_disk_usage', 'scheduled__2025-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/bashTask1.py']
[2025-02-20T17:25:07.717+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'bash_operator_disk_usage', 'check_disk_usage', 'scheduled__2025-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/bashTask1.py']
[2025-02-20T17:25:08.589+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/bashTask1.py
[2025-02-20T17:25:08.632+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:25:08.632+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:08.646+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:08.647+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:25:08.761+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:08.816+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:08.851+0100] {task_command.py:467} INFO - Running <TaskInstance: bash_operator_disk_usage.check_disk_usage scheduled__2025-02-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:25:09.750+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='bash_operator_disk_usage', task_id='check_disk_usage', run_id='scheduled__2025-02-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:25:09.792+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=bash_operator_disk_usage, task_id=check_disk_usage, run_id=scheduled__2025-02-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:25:08.934865+00:00, run_end_date=2025-02-20 16:25:09.051681+00:00, run_duration=0.116816, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=3, job_id=116, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-02-20 16:25:07.714109+00:00, queued_by_job_id=94, pid=38384
[2025-02-20T17:25:18.288+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: bash_operator_disk_usage.display_current_time manual__2025-02-20T16:25:14.169707+00:00 [scheduled]>
[2025-02-20T17:25:18.289+0100] {scheduler_job_runner.py:507} INFO - DAG bash_operator_disk_usage has 0/16 running and queued tasks
[2025-02-20T17:25:18.289+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: bash_operator_disk_usage.display_current_time manual__2025-02-20T16:25:14.169707+00:00 [scheduled]>
[2025-02-20T17:25:18.290+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: bash_operator_disk_usage.display_current_time manual__2025-02-20T16:25:14.169707+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:25:18.290+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='bash_operator_disk_usage', task_id='display_current_time', run_id='manual__2025-02-20T16:25:14.169707+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-20T17:25:18.291+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'bash_operator_disk_usage', 'display_current_time', 'manual__2025-02-20T16:25:14.169707+00:00', '--local', '--subdir', 'DAGS_FOLDER/bashTask1.py']
[2025-02-20T17:25:18.292+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'bash_operator_disk_usage', 'display_current_time', 'manual__2025-02-20T16:25:14.169707+00:00', '--local', '--subdir', 'DAGS_FOLDER/bashTask1.py']
[2025-02-20T17:25:19.383+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/bashTask1.py
[2025-02-20T17:25:19.416+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:25:19.416+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:19.425+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:19.426+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:25:19.551+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:25:19.641+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:25:19.685+0100] {task_command.py:467} INFO - Running <TaskInstance: bash_operator_disk_usage.display_current_time manual__2025-02-20T16:25:14.169707+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:25:20.182+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='bash_operator_disk_usage', task_id='display_current_time', run_id='manual__2025-02-20T16:25:14.169707+00:00', try_number=1, map_index=-1)
[2025-02-20T17:25:20.190+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=bash_operator_disk_usage, task_id=display_current_time, run_id=manual__2025-02-20T16:25:14.169707+00:00, map_index=-1, run_start_date=2025-02-20 16:25:19.769508+00:00, run_end_date=2025-02-20 16:25:19.868461+00:00, run_duration=0.098953, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=3, job_id=120, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-20 16:25:18.290004+00:00, queued_by_job_id=94, pid=38445
[2025-02-20T17:25:24.082+0100] {dagrun.py:854} INFO - Marking run <DagRun bash_operator_disk_usage @ 2025-02-20 16:25:14.169707+00:00: manual__2025-02-20T16:25:14.169707+00:00, state:running, queued_at: 2025-02-20 16:25:14.179983+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 16:25:16.094956+00:00 end:2025-02-20 16:25:24.083279+00:00
[2025-02-20T17:25:24.083+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=bash_operator_disk_usage, execution_date=2025-02-20 16:25:14.169707+00:00, run_id=manual__2025-02-20T16:25:14.169707+00:00, run_start_date=2025-02-20 16:25:16.094956+00:00, run_end_date=2025-02-20 16:25:24.083279+00:00, run_duration=7.988323, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=e0bbe7b12108063d65a47698d3e1fcd1
[2025-02-20T17:29:38.943+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:29:48.747+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-25 00:00:00+00:00: scheduled__2024-04-25T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:24:42.856094+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-25T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:29:48.748+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-25 00:00:00+00:00, run_id=scheduled__2024-04-25T00:00:00+00:00, run_start_date=2025-02-20 16:24:42.867024+00:00, run_end_date=2025-02-20 16:29:48.748691+00:00, run_duration=305.881667, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-25 00:00:00+00:00, data_interval_end=2024-04-26 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:29:48.751+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-26 00:00:00+00:00, run_after=2024-04-27 00:00:00+00:00
[2025-02-20T17:29:50.534+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-28 00:00:00+00:00, run_after=2024-04-29 00:00:00+00:00
[2025-02-20T17:29:51.864+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-30 00:00:00+00:00, run_after=2024-05-01 00:00:00+00:00
[2025-02-20T17:29:51.903+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:29:51.904+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:29:51.904+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:29:51.904+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:29:51.905+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-04-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:29:51.905+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-27T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:29:51.905+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:29:51.905+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-28T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:29:51.905+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:29:51.907+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:29:52.887+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:29:52.921+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:29:52.921+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:29:52.930+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:29:52.930+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:29:53.045+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:29:53.114+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:29:53.165+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:29:53.735+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:29:54.636+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:29:54.684+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:29:54.685+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:29:54.694+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:29:54.695+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:29:54.806+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:29:54.870+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:29:54.906+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:29:55.437+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-27T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:29:55.438+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-28T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:29:55.446+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:29:53.254622+00:00, run_end_date=2025-02-20 16:29:53.350090+00:00, run_duration=0.095468, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=124, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:29:51.904645+00:00, queued_by_job_id=94, pid=39353
[2025-02-20T17:29:55.447+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:29:55.012377+00:00, run_end_date=2025-02-20 16:29:55.140003+00:00, run_duration=0.127626, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=126, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:29:51.904645+00:00, queued_by_job_id=94, pid=39364
[2025-02-20T17:29:56.428+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-29 00:00:00+00:00, run_after=2024-04-30 00:00:00+00:00
[2025-02-20T17:29:58.418+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-30 00:00:00+00:00, run_after=2024-05-01 00:00:00+00:00
[2025-02-20T17:29:58.424+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-02 00:00:00+00:00: scheduled__2024-05-02T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:24:53.337164+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-02T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:29:58.424+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-02 00:00:00+00:00, run_id=scheduled__2024-05-02T00:00:00+00:00, run_start_date=2025-02-20 16:24:53.343545+00:00, run_end_date=2025-02-20 16:29:58.424751+00:00, run_duration=305.081206, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-02 00:00:00+00:00, data_interval_end=2024-05-03 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:29:58.426+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-03 00:00:00+00:00, run_after=2024-05-04 00:00:00+00:00
[2025-02-20T17:29:58.439+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-01 00:00:00+00:00: scheduled__2024-05-01T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:24:52.744548+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-01T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:29:58.439+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-01 00:00:00+00:00, run_id=scheduled__2024-05-01T00:00:00+00:00, run_start_date=2025-02-20 16:24:52.751395+00:00, run_end_date=2025-02-20 16:29:58.439860+00:00, run_duration=305.688465, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-01 00:00:00+00:00, data_interval_end=2024-05-02 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:29:58.441+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-02 00:00:00+00:00, run_after=2024-05-03 00:00:00+00:00
[2025-02-20T17:29:58.445+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:29:58.445+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:29:58.445+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:29:58.446+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:29:58.446+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-03T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:29:58.446+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:29:58.447+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:29:59.376+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:29:59.409+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:29:59.409+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:29:59.418+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:29:59.419+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:29:59.548+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:29:59.661+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:29:59.725+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:00.334+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-03T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:30:00.341+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:29:59.820222+00:00, run_end_date=2025-02-20 16:29:59.993403+00:00, run_duration=0.173181, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=130, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:29:58.445792+00:00, queued_by_job_id=94, pid=39386
[2025-02-20T17:30:01.571+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-06 00:00:00+00:00, run_after=2024-05-07 00:00:00+00:00
[2025-02-20T17:30:01.581+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-03 00:00:00+00:00: scheduled__2024-05-03T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:24:55.213450+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-03T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:30:01.581+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-03 00:00:00+00:00, run_id=scheduled__2024-05-03T00:00:00+00:00, run_start_date=2025-02-20 16:24:55.220592+00:00, run_end_date=2025-02-20 16:30:01.581561+00:00, run_duration=306.360969, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-03 00:00:00+00:00, data_interval_end=2024-05-04 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:01.583+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-04 00:00:00+00:00, run_after=2024-05-05 00:00:00+00:00
[2025-02-20T17:30:03.826+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-06 00:00:00+00:00, run_after=2024-05-07 00:00:00+00:00
[2025-02-20T17:30:03.845+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-05 00:00:00+00:00: scheduled__2024-05-05T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:24:56.412286+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-05T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:30:03.846+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-05 00:00:00+00:00, run_id=scheduled__2024-05-05T00:00:00+00:00, run_start_date=2025-02-20 16:24:56.421041+00:00, run_end_date=2025-02-20 16:30:03.846138+00:00, run_duration=307.425097, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-05 00:00:00+00:00, data_interval_end=2024-05-06 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:03.847+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-06 00:00:00+00:00, run_after=2024-05-07 00:00:00+00:00
[2025-02-20T17:30:03.852+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:03.852+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:30:03.852+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:03.853+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:30:03.853+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-07T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:30:03.854+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:03.855+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:04.891+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:30:04.929+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:30:04.929+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:04.939+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:04.939+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:05.084+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:05.188+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:05.236+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:05.939+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-07T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:30:05.948+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:30:05.335132+00:00, run_end_date=2025-02-20 16:30:05.462500+00:00, run_duration=0.127368, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=134, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:30:03.852961+00:00, queued_by_job_id=94, pid=39412
[2025-02-20T17:30:07.345+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-10 00:00:00+00:00, run_after=2024-05-11 00:00:00+00:00
[2025-02-20T17:30:07.358+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-08 00:00:00+00:00: scheduled__2024-05-08T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:25:02.260633+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-08T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:30:07.358+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-08 00:00:00+00:00, run_id=scheduled__2024-05-08T00:00:00+00:00, run_start_date=2025-02-20 16:25:02.268499+00:00, run_end_date=2025-02-20 16:30:07.358367+00:00, run_duration=305.089868, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-08 00:00:00+00:00, data_interval_end=2024-05-09 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:07.360+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-09 00:00:00+00:00, run_after=2024-05-10 00:00:00+00:00
[2025-02-20T17:30:07.361+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-07 00:00:00+00:00: scheduled__2024-05-07T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:25:01.379927+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-07T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:30:07.361+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-07 00:00:00+00:00, run_id=scheduled__2024-05-07T00:00:00+00:00, run_start_date=2025-02-20 16:25:01.390191+00:00, run_end_date=2025-02-20 16:30:07.361445+00:00, run_duration=305.971254, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-07 00:00:00+00:00, data_interval_end=2024-05-08 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:07.362+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-08 00:00:00+00:00, run_after=2024-05-09 00:00:00+00:00
[2025-02-20T17:30:07.363+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-06 00:00:00+00:00: scheduled__2024-05-06T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:25:01.056093+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-06T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:30:07.364+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-06 00:00:00+00:00, run_id=scheduled__2024-05-06T00:00:00+00:00, run_start_date=2025-02-20 16:25:01.068847+00:00, run_end_date=2025-02-20 16:30:07.364060+00:00, run_duration=306.295213, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-06 00:00:00+00:00, data_interval_end=2024-05-07 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:07.365+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-07 00:00:00+00:00, run_after=2024-05-08 00:00:00+00:00
[2025-02-20T17:30:09.450+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-08 00:00:00+00:00, run_after=2024-05-09 00:00:00+00:00
[2025-02-20T17:30:11.678+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-11 00:00:00+00:00, run_after=2024-05-12 00:00:00+00:00
[2025-02-20T17:30:13.694+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-13 00:00:00+00:00, run_after=2024-05-14 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:30:13.690679+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:13.711+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:13.711+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:30:13.711+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:13.712+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:30:13.712+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:30:13.712+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:13.713+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:14.685+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:30:14.717+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:30:14.717+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:14.726+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:14.726+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:14.976+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:15.057+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:15.100+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:34:42.652+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-12T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:34:42.662+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:34:36.988869+00:00, run_end_date=2025-02-20 16:34:37.100930+00:00, run_duration=0.112061, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=154, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:34:35.771110+00:00, queued_by_job_id=3, pid=40047
[2025-02-20T17:34:42.675+0100] {manager.py:293} ERROR - DagFileProcessorManager (PID=38220) last sent a heartbeat 269.00 seconds ago! Restarting it
[2025-02-20T17:34:42.679+0100] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 38220. PIDs of all processes in the group: [38220]
[2025-02-20T17:34:42.679+0100] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 38220
[2025-02-20T17:34:43.052+0100] {process_utils.py:80} INFO - Process psutil.Process(pid=38220, status='terminated', exitcode=0, started='17:24:38') (38220) terminated with exit code 0
[2025-02-20T17:34:43.056+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 40066
[2025-02-20T17:34:43.062+0100] {job.py:229} INFO - Heartbeat recovered after 275.68 seconds
[2025-02-20T17:34:43.070+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:34:43.690+0100] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-20T17:34:43.699+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-20T17:34:45.079+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-21 00:00:00+00:00, run_after=2024-05-22 00:00:00+00:00
[2025-02-20T17:34:47.324+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:34:47.321752+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:34:47.362+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:34:47.362+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:34:47.362+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:34:47.363+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:34:47.363+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-27T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:34:47.363+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:34:47.364+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:34:48.276+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:34:48.306+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:34:48.307+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:34:48.315+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:34:48.316+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:34:48.441+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:34:48.513+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:34:48.566+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:34:49.061+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-27T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:34:49.065+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:34:48.675364+00:00, run_end_date=2025-02-20 16:34:48.782017+00:00, run_duration=0.106653, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=155, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:34:47.398150+00:00, queued_by_job_id=3, pid=40094
[2025-02-20T17:34:52.460+0100] {scheduler_job_runner.py:1387} ERROR - Failed creating DagRun for etl_example
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.IntegrityError: UNIQUE constraint failed: dag_run.dag_id, dag_run.run_id

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1371, in _create_dag_runs
    dag.create_dagrun(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 3188, in create_dagrun
    run = _create_orm_dagrun(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 348, in _create_orm_dagrun
    session.flush()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_run.dag_id, dag_run.run_id
[SQL: INSERT INTO dag_run (dag_id, queued_at, execution_date, start_date, end_date, state, run_id, creating_job_id, external_trigger, run_type, conf, data_interval_start, data_interval_end, last_scheduling_decision, dag_hash, log_template_id, updated_at, clear_number) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('etl_example', '2025-02-20 16:34:52.457914', '2024-05-28 00:00:00.000000', None, None, <DagRunState.QUEUED: 'queued'>, 'scheduled__2024-05-28T00:00:00+00:00', 94, 0, <DagRunType.SCHEDULED: 'scheduled'>, <memory at 0x10e6e3ac0>, '2024-05-28 00:00:00.000000', '2024-05-29 00:00:00.000000', None, '01075acecffac600a262bfe0b3565714', 1, '2025-02-20 16:34:52.459091', 0)]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2025-02-20T17:34:52.466+0100] {scheduler_job_runner.py:1016} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 999, in _execute
    self._run_scheduler_loop()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1138, in _run_scheduler_loop
    num_queued_tis = self._do_scheduling(session)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1244, in _do_scheduling
    self._create_dagruns_for_dags(guard, session)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/retries.py", line 93, in wrapped_function
    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/retries.py", line 102, in wrapped_function
    return func(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1325, in _create_dagruns_for_dags
    guard.commit()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/sqlalchemy.py", line 442, in commit
    self.session.commit()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 830, in commit
    self._assert_active(prepared_ok=True)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_run.dag_id, dag_run.run_id
[SQL: INSERT INTO dag_run (dag_id, queued_at, execution_date, start_date, end_date, state, run_id, creating_job_id, external_trigger, run_type, conf, data_interval_start, data_interval_end, last_scheduling_decision, dag_hash, log_template_id, updated_at, clear_number) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('etl_example', '2025-02-20 16:34:52.457914', '2024-05-28 00:00:00.000000', None, None, <DagRunState.QUEUED: 'queued'>, 'scheduled__2024-05-28T00:00:00+00:00', 94, 0, <DagRunType.SCHEDULED: 'scheduled'>, <memory at 0x10e6e3ac0>, '2024-05-28 00:00:00.000000', '2024-05-29 00:00:00.000000', None, '01075acecffac600a262bfe0b3565714', 1, '2025-02-20 16:34:52.459091', 0)]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
[2025-02-20T17:34:53.472+0100] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 40066. PIDs of all processes in the group: [40066]
[2025-02-20T17:34:53.473+0100] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 40066
[2025-02-20T17:34:53.839+0100] {process_utils.py:80} INFO - Process psutil.Process(pid=40066, status='terminated', exitcode=0, started='17:34:43') (40066) terminated with exit code 0
[2025-02-20T17:34:53.841+0100] {scheduler_job_runner.py:1029} INFO - Exited execute loop
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 56, in scheduler
    run_command_with_daemon_option(
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 47, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 999, in _execute
    self._run_scheduler_loop()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1138, in _run_scheduler_loop
    num_queued_tis = self._do_scheduling(session)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1244, in _do_scheduling
    self._create_dagruns_for_dags(guard, session)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/retries.py", line 93, in wrapped_function
    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/retries.py", line 102, in wrapped_function
    return func(*args, **kwargs)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1325, in _create_dagruns_for_dags
    guard.commit()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/utils/sqlalchemy.py", line 442, in commit
    self.session.commit()
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 830, in commit
    self._assert_active(prepared_ok=True)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 604, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: dag_run.dag_id, dag_run.run_id
[SQL: INSERT INTO dag_run (dag_id, queued_at, execution_date, start_date, end_date, state, run_id, creating_job_id, external_trigger, run_type, conf, data_interval_start, data_interval_end, last_scheduling_decision, dag_hash, log_template_id, updated_at, clear_number) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('etl_example', '2025-02-20 16:34:52.457914', '2024-05-28 00:00:00.000000', None, None, <DagRunState.QUEUED: 'queued'>, 'scheduled__2024-05-28T00:00:00+00:00', 94, 0, <DagRunType.SCHEDULED: 'scheduled'>, <memory at 0x10e6e3ac0>, '2024-05-28 00:00:00.000000', '2024-05-29 00:00:00.000000', None, '01075acecffac600a262bfe0b3565714', 1, '2025-02-20 16:34:52.459091', 0)]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
0:21.215+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:21.325+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:21.385+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:21.423+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:21.928+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-17T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:30:21.932+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:30:21.487412+00:00, run_end_date=2025-02-20 16:30:21.652731+00:00, run_duration=0.165319, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=144, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:30:20.319553+00:00, queued_by_job_id=91, pid=39471
[2025-02-20T17:30:23.477+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-20 00:00:00+00:00, run_after=2024-05-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:30:23.474419+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:23.527+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:23.528+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:30:23.528+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:23.530+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:30:23.531+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:30:23.531+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:23.533+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:24.994+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:30:25.039+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:30:25.041+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:25.056+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:25.057+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:25.181+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:25.258+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:25.303+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:26.504+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:30:26.533+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:30:25.395069+00:00, run_end_date=2025-02-20 16:30:26.133004+00:00, run_duration=0.737935, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=146, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:30:23.529148+00:00, queued_by_job_id=91, pid=39482
[2025-02-20T17:30:28.787+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-22 00:00:00+00:00, run_after=2024-05-23 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:30:28.779166+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:28.821+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:28.822+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:30:28.822+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:28.823+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:30:28.823+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-21T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:30:28.823+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:28.824+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:30.010+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:30:30.038+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:30:30.038+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:30.046+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:30.047+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:30.159+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:30.225+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:30.264+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:30.918+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-21T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:30:30.926+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:30:30.349474+00:00, run_end_date=2025-02-20 16:30:30.496774+00:00, run_duration=0.1473, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=148, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:30:28.822535+00:00, queued_by_job_id=91, pid=39493
[2025-02-20T17:30:32.361+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-24 00:00:00+00:00, run_after=2024-05-25 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:30:32.356386+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:32.404+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:32.404+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:30:32.404+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:32.405+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-23T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:30:32.405+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-23T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:30:32.405+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:32.406+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:33.663+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:30:33.750+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:30:33.751+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:33.776+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:33.777+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:33.951+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:34.060+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:34.107+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:34.861+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-23T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:30:34.871+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:30:34.262169+00:00, run_end_date=2025-02-20 16:30:34.442536+00:00, run_duration=0.180367, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=150, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:30:32.405064+00:00, queued_by_job_id=91, pid=39504
[2025-02-20T17:30:36.156+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-26 00:00:00+00:00, run_after=2024-05-27 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:30:36.153029+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:30:36.195+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:36.195+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:30:36.195+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:30:36.196+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:30:36.196+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-25T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:30:36.196+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:36.197+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:30:37.671+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:30:37.724+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:30:37.725+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:37.740+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:37.741+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:30:37.953+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:30:38.028+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:30:38.082+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:30:38.781+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-25T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:30:38.802+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:30:38.148391+00:00, run_end_date=2025-02-20 16:30:38.314982+00:00, run_duration=0.166591, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=152, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:30:36.195774+00:00, queued_by_job_id=91, pid=39514
[2025-02-20T17:34:35.244+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:34:35.246+0100] {scheduler_job_runner.py:1972} INFO - Marked 1 SchedulerJob instances as failed
[2025-02-20T17:34:35.247+0100] {scheduler_job_runner.py:2012} INFO - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-12T00:00:00+00:00 [running]>
[2025-02-20T17:34:39.480+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-14 00:00:00+00:00, run_after=2024-05-15 00:00:00+00:00
[2025-02-20T17:34:40.678+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-16 00:00:00+00:00, run_after=2024-05-17 00:00:00+00:00
[2025-02-20T17:34:42.702+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-18 00:00:00+00:00, run_after=2024-05-19 00:00:00+00:00
[2025-02-20T17:34:45.066+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-20 00:00:00+00:00, run_after=2024-05-21 00:00:00+00:00
[2025-02-20T17:34:46.359+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-23 00:00:00+00:00, run_after=2024-05-24 00:00:00+00:00
[2025-02-20T17:34:50.654+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-27 00:00:00+00:00: scheduled__2024-05-27T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:34:47.321752+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-27T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:34:50.656+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-27 00:00:00+00:00, run_id=scheduled__2024-05-27T00:00:00+00:00, run_start_date=2025-02-20 16:34:47.328941+00:00, run_end_date=2025-02-20 16:34:50.656088+00:00, run_duration=3.327147, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-27 00:00:00+00:00, data_interval_end=2024-05-28 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:34:50.659+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-28 00:00:00+00:00, run_after=2024-05-29 00:00:00+00:00
[2025-02-20T17:35:16.615+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:16.615+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:16.616+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:16.616+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:16.617+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-13T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:16.617+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:16.618+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:17.415+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:17.440+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:17.440+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:17.448+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:17.448+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:17.553+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:17.621+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:17.654+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:18.076+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-13T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:18.079+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:17.743683+00:00, run_end_date=2025-02-20 16:35:17.833304+00:00, run_duration=0.089621, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=159, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:16.616370+00:00, queued_by_job_id=91, pid=40184
[2025-02-20T17:35:18.968+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-13 00:00:00+00:00, run_after=2024-05-14 00:00:00+00:00
[2025-02-20T17:35:19.004+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:19.004+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:19.004+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:35:19.005+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:19.005+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-14T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:19.006+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-14T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:19.006+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:19.006+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:19.006+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:19.007+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:19.844+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:19.889+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:19.889+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:19.900+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:19.901+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:20.047+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:20.123+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:20.160+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:20.604+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:21.406+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:21.431+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:21.432+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:21.439+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:21.440+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:21.539+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:21.616+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:21.660+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:22.095+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-14T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:22.096+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:22.101+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:20.236255+00:00, run_end_date=2025-02-20 16:35:20.332335+00:00, run_duration=0.09608, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=160, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:19.005301+00:00, queued_by_job_id=91, pid=40192
[2025-02-20T17:35:22.102+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:21.737217+00:00, run_end_date=2025-02-20 16:35:21.842452+00:00, run_duration=0.105235, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=161, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:19.005301+00:00, queued_by_job_id=91, pid=40198
[2025-02-20T17:35:23.001+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-16 00:00:00+00:00, run_after=2024-05-17 00:00:00+00:00
[2025-02-20T17:35:23.034+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-14 00:00:00+00:00: scheduled__2024-05-14T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:15.918850+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-14T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:23.034+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-14 00:00:00+00:00, run_id=scheduled__2024-05-14T00:00:00+00:00, run_start_date=2025-02-20 16:30:15.928725+00:00, run_end_date=2025-02-20 16:35:23.034873+00:00, run_duration=307.106148, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-14 00:00:00+00:00, data_interval_end=2024-05-15 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:23.037+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-15 00:00:00+00:00, run_after=2024-05-16 00:00:00+00:00
[2025-02-20T17:35:23.044+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:23.044+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:23.044+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:23.045+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:23.045+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-17T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:23.045+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:23.046+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:23.892+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:23.918+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:23.918+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:23.926+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:23.926+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:24.027+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:24.087+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:24.122+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:24.662+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-17T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:24.665+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:24.197796+00:00, run_end_date=2025-02-20 16:35:24.376864+00:00, run_duration=0.179068, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=163, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:23.044776+00:00, queued_by_job_id=91, pid=40207
[2025-02-20T17:35:26.026+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-17 00:00:00+00:00, run_after=2024-05-18 00:00:00+00:00
[2025-02-20T17:35:28.202+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-19 00:00:00+00:00, run_after=2024-05-20 00:00:00+00:00
[2025-02-20T17:35:30.322+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-21 00:00:00+00:00, run_after=2024-05-22 00:00:00+00:00
[2025-02-20T17:35:30.329+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-19 00:00:00+00:00: scheduled__2024-05-19T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:23.474419+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-19T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:30.329+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-19 00:00:00+00:00, run_id=scheduled__2024-05-19T00:00:00+00:00, run_start_date=2025-02-20 16:30:23.482303+00:00, run_end_date=2025-02-20 16:35:30.329811+00:00, run_duration=306.847508, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-19 00:00:00+00:00, data_interval_end=2024-05-20 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:30.331+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-20 00:00:00+00:00, run_after=2024-05-21 00:00:00+00:00
[2025-02-20T17:35:32.311+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-22 00:00:00+00:00, run_after=2024-05-23 00:00:00+00:00
[2025-02-20T17:35:32.328+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-20 00:00:00+00:00: scheduled__2024-05-20T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:26.222033+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-20T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:32.328+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-20 00:00:00+00:00, run_id=scheduled__2024-05-20T00:00:00+00:00, run_start_date=2025-02-20 16:30:26.233132+00:00, run_end_date=2025-02-20 16:35:32.328232+00:00, run_duration=306.0951, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-20 00:00:00+00:00, data_interval_end=2024-05-21 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:32.329+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-21 00:00:00+00:00, run_after=2024-05-22 00:00:00+00:00
[2025-02-20T17:35:34.022+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-22 00:00:00+00:00, run_after=2024-05-23 00:00:00+00:00
[2025-02-20T17:35:34.044+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:34.045+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:34.045+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:34.046+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-22T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:34.046+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-22T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:34.046+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:34.047+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:35.216+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:35.264+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:35.265+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:35.274+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:35.274+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:35.401+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:35.456+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:35.489+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:36.005+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-22T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:36.013+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:35.574144+00:00, run_end_date=2025-02-20 16:35:35.677920+00:00, run_duration=0.103776, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=168, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:34.045541+00:00, queued_by_job_id=91, pid=40252
[2025-02-20T17:35:36.983+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-23 00:00:00+00:00, run_after=2024-05-24 00:00:00+00:00
[2025-02-20T17:35:37.005+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:37.005+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:37.005+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:37.006+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:37.006+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-24T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:37.006+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:37.007+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:37.869+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:37.895+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:37.895+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:37.902+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:37.903+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:38.120+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:38.275+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:38.319+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:38.771+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-24T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:38.776+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:38.404900+00:00, run_end_date=2025-02-20 16:35:38.508039+00:00, run_duration=0.103139, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=170, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:37.005629+00:00, queued_by_job_id=91, pid=40261
[2025-02-20T17:35:39.699+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-25 00:00:00+00:00, run_after=2024-05-26 00:00:00+00:00
[2025-02-20T17:35:39.711+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-24 00:00:00+00:00: scheduled__2024-05-24T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:34.705342+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-24T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:39.711+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-24 00:00:00+00:00, run_id=scheduled__2024-05-24T00:00:00+00:00, run_start_date=2025-02-20 16:30:34.717670+00:00, run_end_date=2025-02-20 16:35:39.711658+00:00, run_duration=304.993988, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-24 00:00:00+00:00, data_interval_end=2024-05-25 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:39.713+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-25 00:00:00+00:00, run_after=2024-05-26 00:00:00+00:00
[2025-02-20T17:35:41.744+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-26 00:00:00+00:00, run_after=2024-05-27 00:00:00+00:00
[2025-02-20T17:35:41.797+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:41.798+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:41.798+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:41.801+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:41.802+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-26T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:41.802+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:41.804+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:42.878+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:42.908+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:42.908+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:42.916+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:42.916+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:43.013+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:43.064+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:43.097+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:43.543+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-26T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:43.551+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:43.179585+00:00, run_end_date=2025-02-20 16:35:43.277266+00:00, run_duration=0.097681, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=172, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:41.799421+00:00, queued_by_job_id=91, pid=40275
[2025-02-20T17:35:44.486+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-28 00:00:00+00:00, run_after=2024-05-29 00:00:00+00:00
[2025-02-20T17:35:44.494+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-26 00:00:00+00:00: scheduled__2024-05-26T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:38.570067+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-26T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:44.494+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-26 00:00:00+00:00, run_id=scheduled__2024-05-26T00:00:00+00:00, run_start_date=2025-02-20 16:30:38.579179+00:00, run_end_date=2025-02-20 16:35:44.494278+00:00, run_duration=305.915099, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-26 00:00:00+00:00, data_interval_end=2024-05-27 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:44.496+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-27 00:00:00+00:00, run_after=2024-05-28 00:00:00+00:00
[2025-02-20T17:35:46.665+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-29 00:00:00+00:00, run_after=2024-05-30 00:00:00+00:00
[2025-02-20T17:35:48.646+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-31 00:00:00+00:00, run_after=2024-06-01 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:48.643703+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:48.662+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:48.662+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:35:48.662+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:48.663+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:48.663+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-30T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:48.663+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:48.664+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:49.572+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:49.610+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:49.611+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:49.619+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:49.621+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:49.738+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:49.804+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:49.847+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:50.439+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-30T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:50.446+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:49.948100+00:00, run_end_date=2025-02-20 16:35:50.072832+00:00, run_duration=0.124732, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=174, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:48.663058+00:00, queued_by_job_id=91, pid=40293
[2025-02-20T17:35:51.380+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-02 00:00:00+00:00, run_after=2024-06-03 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:51.378430+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:51.400+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:51.400+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:35:51.400+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:51.401+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:51.401+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-01T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:51.401+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:51.402+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:52.567+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:52.596+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:52.596+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:52.605+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:52.605+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:52.724+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:52.930+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:52.975+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:53.391+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-01T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:53.417+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:53.043828+00:00, run_end_date=2025-02-20 16:35:53.141668+00:00, run_duration=0.09784, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=176, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:51.401140+00:00, queued_by_job_id=91, pid=40304
[2025-02-20T17:35:54.890+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-04 00:00:00+00:00, run_after=2024-06-05 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:54.886205+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:54.922+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:54.922+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:35:54.923+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:54.923+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:54.924+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:54.924+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:54.925+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:55.894+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:55.970+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:55.970+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:55.989+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:55.992+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:56.134+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:56.195+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:56.232+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:56.692+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-03T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:56.696+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:56.310241+00:00, run_end_date=2025-02-20 16:35:56.407160+00:00, run_duration=0.096919, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=178, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:54.923336+00:00, queued_by_job_id=91, pid=40314
[2025-02-20T17:35:57.664+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-06 00:00:00+00:00, run_after=2024-06-07 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:57.662405+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:57.690+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:57.691+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:35:57.691+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:57.692+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:57.692+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-05T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:57.692+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:57.693+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:58.483+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:58.512+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:58.512+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:58.524+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:58.525+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:58.652+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:58.703+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:58.740+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:59.155+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-05T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:59.159+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:58.818407+00:00, run_end_date=2025-02-20 16:35:58.914619+00:00, run_duration=0.096212, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=180, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:57.691607+00:00, queued_by_job_id=91, pid=40324
[2025-02-20T17:36:00.201+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-08 00:00:00+00:00, run_after=2024-06-09 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:36:00.198802+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:36:00.229+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:00.229+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:36:00.229+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:00.230+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:36:00.230+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-07T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:36:00.230+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:00.231+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:01.104+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:01.131+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:01.131+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:01.140+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:01.140+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:01.257+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:01.317+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:01.352+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:02.013+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-07T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:02.022+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:01.432406+00:00, run_end_date=2025-02-20 16:36:01.555754+00:00, run_duration=0.123348, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=182, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:36:00.229793+00:00, queued_by_job_id=91, pid=40336
[2025-02-20T17:36:03.344+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-10 00:00:00+00:00, run_after=2024-06-11 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:36:03.340886+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:36:03.376+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:03.376+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:36:03.376+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:03.377+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:36:03.377+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-09T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:36:03.377+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:03.378+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:04.230+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:04.256+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:04.256+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:04.264+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:04.264+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:04.371+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:04.431+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:04.463+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:05.035+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-09T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:05.039+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:04.548608+00:00, run_end_date=2025-02-20 16:36:04.673710+00:00, run_duration=0.125102, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=184, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:36:03.377075+00:00, queued_by_job_id=91, pid=40353
[2025-02-20T17:36:05.987+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-12 00:00:00+00:00, run_after=2024-06-13 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:36:05.985501+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:36:06.022+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:06.022+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:36:06.022+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:06.023+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:36:06.023+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-11T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:36:06.023+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:06.024+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:06.883+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:06.909+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:06.909+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:06.918+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:06.919+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:07.145+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:07.266+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:07.384+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:08.121+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-11T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:08.130+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:07.463201+00:00, run_end_date=2025-02-20 16:36:07.713127+00:00, run_duration=0.249926, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=186, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:36:06.022928+00:00, queued_by_job_id=91, pid=40367
[2025-02-20T17:39:36.456+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:39:53.955+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:39:53.956+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:39:53.956+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:39:53.957+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:39:53.957+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-28T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:39:53.957+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:39:53.958+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:39:54.765+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:39:54.790+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:39:54.791+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:39:54.799+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:39:54.799+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:39:54.911+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:39:54.983+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:39:55.017+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:39:55.597+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-28T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:39:55.605+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:39:55.104212+00:00, run_end_date=2025-02-20 16:39:55.271006+00:00, run_duration=0.166794, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=188, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:39:53.956569+00:00, queued_by_job_id=91, pid=40886
[2025-02-20T17:39:56.580+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-28 00:00:00+00:00: scheduled__2024-05-28T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:34:52.450987+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-28T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:39:56.580+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-28 00:00:00+00:00, run_id=scheduled__2024-05-28T00:00:00+00:00, run_start_date=2025-02-20 16:34:52.463393+00:00, run_end_date=2025-02-20 16:39:56.580870+00:00, run_duration=304.117477, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-28 00:00:00+00:00, data_interval_end=2024-05-29 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:39:56.582+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-29 00:00:00+00:00, run_after=2024-05-30 00:00:00+00:00
[2025-02-20T17:39:58.515+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-31 00:00:00+00:00, run_after=2024-06-01 00:00:00+00:00
[2025-02-20T17:40:00.824+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-02 00:00:00+00:00, run_after=2024-06-03 00:00:00+00:00
[2025-02-20T17:40:03.198+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-04 00:00:00+00:00, run_after=2024-06-05 00:00:00+00:00
[2025-02-20T17:40:05.231+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-06 00:00:00+00:00, run_after=2024-06-07 00:00:00+00:00
[2025-02-20T17:40:06.827+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-08 00:00:00+00:00, run_after=2024-06-09 00:00:00+00:00
[2025-02-20T17:40:08.808+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-10 00:00:00+00:00, run_after=2024-06-11 00:00:00+00:00
[2025-02-20T17:40:11.208+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-12 00:00:00+00:00, run_after=2024-06-13 00:00:00+00:00
[2025-02-20T17:40:12.956+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-13 00:00:00+00:00, run_after=2024-06-14 00:00:00+00:00
[2025-02-20T17:40:52.920+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:52.920+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:40:52.921+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:52.921+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:40:52.922+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-31T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:52.922+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:52.922+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:53.730+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:40:53.758+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:40:53.758+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:53.766+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:53.767+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:40:53.884+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:53.937+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:53.969+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:40:54.627+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-31T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:40:54.657+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:40:54.043233+00:00, run_end_date=2025-02-20 16:40:54.132628+00:00, run_duration=0.089395, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=192, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:52.921280+00:00, queued_by_job_id=91, pid=41018
[2025-02-20T17:40:56.041+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-31 00:00:00+00:00, run_after=2024-06-01 00:00:00+00:00
[2025-02-20T17:40:56.069+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-31 00:00:00+00:00: scheduled__2024-05-31T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:51.028540+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-31T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:40:56.070+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-31 00:00:00+00:00, run_id=scheduled__2024-05-31T00:00:00+00:00, run_start_date=2025-02-20 16:35:51.034535+00:00, run_end_date=2025-02-20 16:40:56.070048+00:00, run_duration=305.035513, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-31 00:00:00+00:00, data_interval_end=2024-06-01 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:40:56.072+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-01 00:00:00+00:00, run_after=2024-06-02 00:00:00+00:00
[2025-02-20T17:40:56.073+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-30 00:00:00+00:00: scheduled__2024-05-30T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:48.643703+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-30T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:40:56.073+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-30 00:00:00+00:00, run_id=scheduled__2024-05-30T00:00:00+00:00, run_start_date=2025-02-20 16:35:48.650134+00:00, run_end_date=2025-02-20 16:40:56.073433+00:00, run_duration=307.423299, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-30 00:00:00+00:00, data_interval_end=2024-05-31 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:40:56.074+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-31 00:00:00+00:00, run_after=2024-06-01 00:00:00+00:00
[2025-02-20T17:40:58.193+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-01 00:00:00+00:00, run_after=2024-06-02 00:00:00+00:00
[2025-02-20T17:40:58.239+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:58.239+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:40:58.239+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:40:58.239+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:58.240+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:40:58.240+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-02T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:58.240+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:58.240+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-03T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:58.241+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:58.241+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:59.078+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:40:59.104+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:40:59.104+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:59.112+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:59.112+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:40:59.222+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:59.275+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:59.315+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:40:59.895+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:00.881+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:00.907+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:00.908+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:00.917+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:00.917+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:01.026+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:01.081+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:01.116+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:01.663+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-02T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:01.664+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-03T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:01.671+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:40:59.402802+00:00, run_end_date=2025-02-20 16:40:59.518092+00:00, run_duration=0.11529, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=194, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:58.239903+00:00, queued_by_job_id=91, pid=41038
[2025-02-20T17:41:01.672+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:01.198953+00:00, run_end_date=2025-02-20 16:41:01.304831+00:00, run_duration=0.105878, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=195, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:58.239903+00:00, queued_by_job_id=91, pid=41046
[2025-02-20T17:41:02.889+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-04 00:00:00+00:00, run_after=2024-06-05 00:00:00+00:00
[2025-02-20T17:41:02.918+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-02 00:00:00+00:00: scheduled__2024-06-02T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:54.284405+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-02T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:02.919+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-02 00:00:00+00:00, run_id=scheduled__2024-06-02T00:00:00+00:00, run_start_date=2025-02-20 16:35:54.290842+00:00, run_end_date=2025-02-20 16:41:02.918922+00:00, run_duration=308.62808, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-02 00:00:00+00:00, data_interval_end=2024-06-03 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:02.921+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-03 00:00:00+00:00, run_after=2024-06-04 00:00:00+00:00
[2025-02-20T17:41:02.925+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:02.925+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:02.925+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:41:02.926+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:02.926+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:02.927+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-06T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:02.927+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:02.927+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-07T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:02.927+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:02.928+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:03.804+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:03.833+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:03.833+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:03.841+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:03.841+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:03.961+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:04.023+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:04.058+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:04.675+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:05.679+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:05.713+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:05.713+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:05.723+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:05.724+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:05.833+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:05.895+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:05.930+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:06.405+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-06T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:06.406+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-07T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:06.411+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:04.144617+00:00, run_end_date=2025-02-20 16:41:04.265886+00:00, run_duration=0.121269, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=198, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:02.926319+00:00, queued_by_job_id=91, pid=41059
[2025-02-20T17:41:06.411+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:06.015445+00:00, run_end_date=2025-02-20 16:41:06.111640+00:00, run_duration=0.096195, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=200, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:02.926319+00:00, queued_by_job_id=91, pid=41066
[2025-02-20T17:41:07.700+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-05 00:00:00+00:00, run_after=2024-06-06 00:00:00+00:00
[2025-02-20T17:41:07.708+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-05 00:00:00+00:00: scheduled__2024-06-05T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:57.662405+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-05T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:07.709+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-05 00:00:00+00:00, run_id=scheduled__2024-06-05T00:00:00+00:00, run_start_date=2025-02-20 16:35:57.669278+00:00, run_end_date=2025-02-20 16:41:07.709148+00:00, run_duration=310.03987, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-05 00:00:00+00:00, data_interval_end=2024-06-06 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:07.711+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-06 00:00:00+00:00, run_after=2024-06-07 00:00:00+00:00
[2025-02-20T17:41:07.726+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-06 00:00:00+00:00: scheduled__2024-06-06T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:59.951889+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-06T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:07.726+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-06 00:00:00+00:00, run_id=scheduled__2024-06-06T00:00:00+00:00, run_start_date=2025-02-20 16:35:59.960595+00:00, run_end_date=2025-02-20 16:41:07.726493+00:00, run_duration=307.765898, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-06 00:00:00+00:00, data_interval_end=2024-06-07 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:07.728+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-07 00:00:00+00:00, run_after=2024-06-08 00:00:00+00:00
[2025-02-20T17:41:07.732+0100] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:07.732+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:41:07.732+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:07.732+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:41:07.732+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:07.733+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:07.734+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-09T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:07.734+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:07.734+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-10T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:07.734+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:07.734+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-11T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:07.734+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:07.735+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:08.470+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:08.496+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:08.496+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:08.507+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:08.508+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:08.632+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:08.691+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:08.739+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:09.417+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:10.342+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:10.366+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:10.366+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:10.374+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:10.374+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:10.471+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:10.546+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:10.587+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:11.061+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:12.150+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:12.177+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:12.177+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:12.185+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:12.186+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:12.290+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:12.348+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:12.384+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:12.871+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-09T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:12.872+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-10T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:12.872+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-11T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:12.878+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:08.815607+00:00, run_end_date=2025-02-20 16:41:08.926116+00:00, run_duration=0.110509, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=201, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:07.733000+00:00, queued_by_job_id=91, pid=41074
[2025-02-20T17:41:12.879+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:10.665505+00:00, run_end_date=2025-02-20 16:41:10.788314+00:00, run_duration=0.122809, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=202, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:07.733000+00:00, queued_by_job_id=91, pid=41079
[2025-02-20T17:41:12.879+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:12.451326+00:00, run_end_date=2025-02-20 16:41:12.604782+00:00, run_duration=0.153456, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=203, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:07.733000+00:00, queued_by_job_id=91, pid=41085
[2025-02-20T17:41:13.926+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-11 00:00:00+00:00, run_after=2024-06-12 00:00:00+00:00
[2025-02-20T17:41:13.939+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-10 00:00:00+00:00: scheduled__2024-06-10T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:36:05.794134+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-10T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:13.939+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-10 00:00:00+00:00, run_id=scheduled__2024-06-10T00:00:00+00:00, run_start_date=2025-02-20 16:36:05.799766+00:00, run_end_date=2025-02-20 16:41:13.939367+00:00, run_duration=308.139601, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-10 00:00:00+00:00, data_interval_end=2024-06-11 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:13.941+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-11 00:00:00+00:00, run_after=2024-06-12 00:00:00+00:00
[2025-02-20T17:41:15.979+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-13 00:00:00+00:00, run_after=2024-06-14 00:00:00+00:00
[2025-02-20T17:41:17.656+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-15 00:00:00+00:00, run_after=2024-06-16 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:17.653561+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:17.669+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:17.669+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:41:17.669+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:17.670+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:17.670+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-14T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:17.670+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:17.671+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:18.440+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:18.484+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:18.484+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:18.493+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:18.493+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:18.622+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:18.675+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:18.714+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:19.178+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-14T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:19.182+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:18.794370+00:00, run_end_date=2025-02-20 16:41:18.919576+00:00, run_duration=0.125206, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=205, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:17.669689+00:00, queued_by_job_id=91, pid=41106
[2025-02-20T17:41:20.174+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:20.171722+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:20.194+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:20.194+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:20.194+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:20.195+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:20.195+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-16T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:20.195+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:20.196+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:21.031+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:21.059+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:21.059+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:21.070+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:21.071+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:21.173+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:21.230+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:21.266+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:21.900+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-16T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:21.907+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:21.344378+00:00, run_end_date=2025-02-20 16:41:21.461480+00:00, run_duration=0.117102, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=207, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:20.194922+00:00, queued_by_job_id=91, pid=41117
[2025-02-20T17:41:23.142+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-19 00:00:00+00:00, run_after=2024-06-20 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:23.134163+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:23.164+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:23.164+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:23.164+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:23.165+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:23.165+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-18T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:23.166+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:23.167+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:24.082+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:24.114+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:24.114+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:24.123+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:24.123+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:24.227+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:24.301+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:24.346+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:24.874+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-18T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:24.879+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:24.425168+00:00, run_end_date=2025-02-20 16:41:24.602211+00:00, run_duration=0.177043, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=209, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:23.165170+00:00, queued_by_job_id=91, pid=41129
[2025-02-20T17:41:25.907+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-21 00:00:00+00:00, run_after=2024-06-22 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:25.905100+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:25.933+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:25.934+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:25.934+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:25.934+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:25.935+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-20T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:25.935+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:25.936+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:26.804+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:26.834+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:26.835+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:26.844+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:26.844+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:26.951+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:27.007+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:27.043+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:27.550+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-20T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:27.562+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-20T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:27.119429+00:00, run_end_date=2025-02-20 16:41:27.246663+00:00, run_duration=0.127234, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=211, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:25.934381+00:00, queued_by_job_id=91, pid=41140
[2025-02-20T17:41:29.088+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-23 00:00:00+00:00, run_after=2024-06-24 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:29.084266+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:29.127+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:29.128+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:29.128+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:29.129+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:29.129+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-22T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:29.129+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:29.130+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:30.108+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:30.136+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:30.137+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:30.145+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:30.146+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:30.441+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:30.532+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:30.569+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:31.039+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-22T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:31.046+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:30.639597+00:00, run_end_date=2025-02-20 16:41:30.758091+00:00, run_duration=0.118494, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=213, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:29.128543+00:00, queued_by_job_id=91, pid=41151
[2025-02-20T17:41:31.967+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-25 00:00:00+00:00, run_after=2024-06-26 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:31.964776+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:32.000+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:32.000+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:32.000+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:32.001+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:32.001+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-24T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:32.001+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:32.002+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:32.852+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:32.879+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:32.880+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:32.890+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:32.890+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:33.006+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:33.063+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:33.107+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:33.547+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-24T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:33.552+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:33.184292+00:00, run_end_date=2025-02-20 16:41:33.284119+00:00, run_duration=0.099827, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=215, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:32.000636+00:00, queued_by_job_id=91, pid=41161
[2025-02-20T17:41:34.919+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-27 00:00:00+00:00, run_after=2024-06-28 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:34.915874+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:34.960+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:34.961+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:34.961+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:34.962+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:34.962+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-26T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:34.962+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:34.963+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:35.824+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:35.853+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:35.853+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:35.861+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:35.862+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:35.965+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:36.023+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:36.062+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:36.532+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-26T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:36.536+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:36.127635+00:00, run_end_date=2025-02-20 16:41:36.257868+00:00, run_duration=0.130233, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=217, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:34.961602+00:00, queued_by_job_id=91, pid=41171
[2025-02-20T17:41:37.487+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:37.485645+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:37.524+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:37.524+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:41:37.524+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:37.525+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:37.525+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-28T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:37.525+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:37.526+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:38.329+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:38.354+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:38.354+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:38.361+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:38.362+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:38.465+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:38.546+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:38.597+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:39.032+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-28T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:39.036+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:38.675205+00:00, run_end_date=2025-02-20 16:41:38.760033+00:00, run_duration=0.084828, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=219, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:37.524913+00:00, queued_by_job_id=91, pid=41181
[2025-02-20T17:44:37.629+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:45:22.263+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-15 00:00:00+00:00, run_after=2024-06-16 00:00:00+00:00
[2025-02-20T17:45:24.863+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-18 00:00:00+00:00, run_after=2024-06-19 00:00:00+00:00
[2025-02-20T17:45:26.284+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-20 00:00:00+00:00, run_after=2024-06-21 00:00:00+00:00
[2025-02-20T17:45:28.622+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-22 00:00:00+00:00, run_after=2024-06-23 00:00:00+00:00
[2025-02-20T17:45:30.682+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-23 00:00:00+00:00, run_after=2024-06-24 00:00:00+00:00
[2025-02-20T17:45:32.277+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-24 00:00:00+00:00, run_after=2024-06-25 00:00:00+00:00
[2025-02-20T17:45:34.363+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-26 00:00:00+00:00, run_after=2024-06-27 00:00:00+00:00
[2025-02-20T17:45:36.378+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-28 00:00:00+00:00, run_after=2024-06-29 00:00:00+00:00
[2025-02-20T17:45:38.704+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:45:38.701443+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:45:38.766+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:45:38.766+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:45:38.766+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:45:38.767+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:45:38.767+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-29T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:45:38.767+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:45:38.768+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:45:39.910+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:45:39.942+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:45:39.942+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:45:39.950+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:45:39.950+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:45:40.057+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:45:40.127+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:45:40.165+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:45:40.735+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-29T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:45:40.743+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:45:40.284821+00:00, run_end_date=2025-02-20 16:45:40.386047+00:00, run_duration=0.101226, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=221, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:45:38.766733+00:00, queued_by_job_id=91, pid=41718
[2025-02-20T17:46:19.492+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:19.493+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:19.493+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:19.494+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:19.494+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-14T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:19.494+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:19.495+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:20.278+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:20.315+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:20.316+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:20.323+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:20.324+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:20.439+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:20.524+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:20.563+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:21.043+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-14T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:21.048+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:20.649812+00:00, run_end_date=2025-02-20 16:46:20.761185+00:00, run_duration=0.111373, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=222, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:19.493588+00:00, queued_by_job_id=91, pid=41808
[2025-02-20T17:46:22.030+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-14 00:00:00+00:00: scheduled__2024-06-14T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:17.653561+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-14T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:22.030+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-14 00:00:00+00:00, run_id=scheduled__2024-06-14T00:00:00+00:00, run_start_date=2025-02-20 16:41:17.659524+00:00, run_end_date=2025-02-20 16:46:22.030826+00:00, run_duration=304.371302, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-14 00:00:00+00:00, data_interval_end=2024-06-15 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:22.035+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-15 00:00:00+00:00, run_after=2024-06-16 00:00:00+00:00
[2025-02-20T17:46:23.241+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00
[2025-02-20T17:46:23.284+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:23.284+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:23.284+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:23.285+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:23.285+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-17T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:23.285+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:23.286+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:24.170+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:24.204+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:24.205+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:24.214+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:24.215+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:24.581+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:24.671+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:24.707+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:25.305+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-17T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:25.311+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:24.822891+00:00, run_end_date=2025-02-20 16:46:24.969383+00:00, run_duration=0.146492, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=225, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:23.284903+00:00, queued_by_job_id=91, pid=41823
[2025-02-20T17:46:26.792+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00
[2025-02-20T17:46:26.821+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-17 00:00:00+00:00: scheduled__2024-06-17T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:21.437520+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-17T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:26.822+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-17 00:00:00+00:00, run_id=scheduled__2024-06-17T00:00:00+00:00, run_start_date=2025-02-20 16:41:21.444881+00:00, run_end_date=2025-02-20 16:46:26.821875+00:00, run_duration=305.376994, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-17 00:00:00+00:00, data_interval_end=2024-06-18 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:26.824+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-18 00:00:00+00:00, run_after=2024-06-19 00:00:00+00:00
[2025-02-20T17:46:26.825+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-16 00:00:00+00:00: scheduled__2024-06-16T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:20.171722+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-16T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:26.825+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-16 00:00:00+00:00, run_id=scheduled__2024-06-16T00:00:00+00:00, run_start_date=2025-02-20 16:41:20.178998+00:00, run_end_date=2025-02-20 16:46:26.825209+00:00, run_duration=306.646211, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-16 00:00:00+00:00, data_interval_end=2024-06-17 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:26.826+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00
[2025-02-20T17:46:29.164+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-18 00:00:00+00:00, run_after=2024-06-19 00:00:00+00:00
[2025-02-20T17:46:29.210+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:29.210+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:29.210+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:46:29.210+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:29.211+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:29.212+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-20T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:29.212+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:29.212+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-21T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:29.212+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:29.213+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:30.185+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:30.214+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:30.215+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:30.224+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:30.225+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:30.335+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:30.389+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:30.425+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-20T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:30.905+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:31.920+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:31.954+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:31.954+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:31.968+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:31.969+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:32.103+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:32.164+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:32.212+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:32.681+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-20T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:32.682+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-21T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:32.686+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-20T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:30.512308+00:00, run_end_date=2025-02-20 16:46:30.621963+00:00, run_duration=0.109655, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=228, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:29.211182+00:00, queued_by_job_id=91, pid=41841
[2025-02-20T17:46:32.687+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:32.292130+00:00, run_end_date=2025-02-20 16:46:32.394847+00:00, run_duration=0.102717, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=229, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:29.211182+00:00, queued_by_job_id=91, pid=41847
[2025-02-20T17:46:33.666+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-20 00:00:00+00:00, run_after=2024-06-21 00:00:00+00:00
[2025-02-20T17:46:33.705+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-20 00:00:00+00:00: scheduled__2024-06-20T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:25.905100+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-20T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:33.705+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-20 00:00:00+00:00, run_id=scheduled__2024-06-20T00:00:00+00:00, run_start_date=2025-02-20 16:41:25.911435+00:00, run_end_date=2025-02-20 16:46:33.705706+00:00, run_duration=307.794271, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-20 00:00:00+00:00, data_interval_end=2024-06-21 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:33.707+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-21 00:00:00+00:00, run_after=2024-06-22 00:00:00+00:00
[2025-02-20T17:46:33.708+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-19 00:00:00+00:00: scheduled__2024-06-19T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:24.476741+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-19T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:33.709+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-19 00:00:00+00:00, run_id=scheduled__2024-06-19T00:00:00+00:00, run_start_date=2025-02-20 16:41:24.485140+00:00, run_end_date=2025-02-20 16:46:33.709037+00:00, run_duration=309.223897, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-19 00:00:00+00:00, data_interval_end=2024-06-20 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:33.710+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-20 00:00:00+00:00, run_after=2024-06-21 00:00:00+00:00
[2025-02-20T17:46:33.714+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:33.714+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:33.714+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:33.714+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:33.715+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:33.715+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-23T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:33.715+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:33.715+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-24T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:33.715+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:33.716+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:34.756+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:34.781+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:34.782+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:34.791+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:34.792+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:34.896+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:34.950+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:34.983+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:35.407+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:36.447+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:36.474+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:36.474+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:36.482+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:36.483+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:36.593+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:36.650+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:36.686+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:37.206+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-23T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:37.207+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-24T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:37.211+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:35.065951+00:00, run_end_date=2025-02-20 16:46:35.170486+00:00, run_duration=0.104535, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=231, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:33.714854+00:00, queued_by_job_id=91, pid=41855
[2025-02-20T17:46:37.211+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:36.769371+00:00, run_end_date=2025-02-20 16:46:36.890157+00:00, run_duration=0.120786, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=232, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:33.714854+00:00, queued_by_job_id=91, pid=41861
[2025-02-20T17:46:38.267+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-25 00:00:00+00:00, run_after=2024-06-26 00:00:00+00:00
[2025-02-20T17:46:38.289+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-23 00:00:00+00:00: scheduled__2024-06-23T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:30.765855+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-23T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:38.289+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-23 00:00:00+00:00, run_id=scheduled__2024-06-23T00:00:00+00:00, run_start_date=2025-02-20 16:41:30.775377+00:00, run_end_date=2025-02-20 16:46:38.289780+00:00, run_duration=307.514403, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-23 00:00:00+00:00, data_interval_end=2024-06-24 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:38.292+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-24 00:00:00+00:00, run_after=2024-06-25 00:00:00+00:00
[2025-02-20T17:46:38.297+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:38.298+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:38.298+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:38.300+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:38.300+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-27T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:38.300+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:38.302+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:39.471+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:39.516+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:39.516+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:39.550+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:39.551+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:39.804+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:39.862+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:39.900+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:40.371+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-27T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:40.376+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:39.981156+00:00, run_end_date=2025-02-20 16:46:40.107949+00:00, run_duration=0.126793, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=235, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:38.298890+00:00, queued_by_job_id=91, pid=41872
[2025-02-20T17:46:41.451+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-26 00:00:00+00:00, run_after=2024-06-27 00:00:00+00:00
[2025-02-20T17:46:41.463+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-26 00:00:00+00:00: scheduled__2024-06-26T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:34.915874+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-26T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:41.464+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-26 00:00:00+00:00, run_id=scheduled__2024-06-26T00:00:00+00:00, run_start_date=2025-02-20 16:41:34.924757+00:00, run_end_date=2025-02-20 16:46:41.464064+00:00, run_duration=306.539307, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-26 00:00:00+00:00, data_interval_end=2024-06-27 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:41.466+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-27 00:00:00+00:00, run_after=2024-06-28 00:00:00+00:00
[2025-02-20T17:46:41.467+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-27 00:00:00+00:00: scheduled__2024-06-27T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:36.183374+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-27T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:41.467+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-27 00:00:00+00:00, run_id=scheduled__2024-06-27T00:00:00+00:00, run_start_date=2025-02-20 16:41:36.191232+00:00, run_end_date=2025-02-20 16:46:41.467501+00:00, run_duration=305.276269, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-27 00:00:00+00:00, data_interval_end=2024-06-28 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:41.468+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-28 00:00:00+00:00, run_after=2024-06-29 00:00:00+00:00
[2025-02-20T17:46:43.428+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-29 00:00:00+00:00, run_after=2024-06-30 00:00:00+00:00
[2025-02-20T17:46:44.686+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-30 00:00:00+00:00, run_after=2024-07-01 00:00:00+00:00
[2025-02-20T17:46:46.355+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-02 00:00:00+00:00, run_after=2024-07-03 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:46.351878+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:46.371+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:46.371+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:46.371+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:46.372+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:46.372+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-01T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:46.372+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:46.373+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:47.307+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:47.333+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:47.333+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:47.342+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:47.343+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:47.451+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:47.507+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:47.540+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:48.022+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-01T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:48.026+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:47.615425+00:00, run_end_date=2025-02-20 16:46:47.729524+00:00, run_duration=0.114099, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=238, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:46.372024+00:00, queued_by_job_id=91, pid=41904
[2025-02-20T17:46:49.175+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-04 00:00:00+00:00, run_after=2024-07-05 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:49.172925+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:49.193+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:49.193+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:49.193+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:49.194+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:49.194+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:49.194+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:49.195+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:50.561+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:50.591+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:50.591+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:50.600+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:50.600+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:50.708+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:50.761+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:50.793+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:51.213+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-03T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:51.220+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:50.866473+00:00, run_end_date=2025-02-20 16:46:50.958531+00:00, run_duration=0.092058, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=240, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:49.194011+00:00, queued_by_job_id=91, pid=41917
[2025-02-20T17:46:52.213+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-06 00:00:00+00:00, run_after=2024-07-07 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:52.208966+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:52.238+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:52.238+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:52.238+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:52.239+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:52.239+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-05T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:52.239+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:52.240+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:53.123+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:53.155+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:53.156+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:53.164+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:53.165+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:53.281+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:53.336+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:53.371+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:54.043+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-05T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:54.056+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:53.572171+00:00, run_end_date=2025-02-20 16:46:53.705491+00:00, run_duration=0.13332, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=242, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:52.238781+00:00, queued_by_job_id=91, pid=41927
[2025-02-20T17:46:55.096+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-08 00:00:00+00:00, run_after=2024-07-09 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:55.094374+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:55.125+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:55.125+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:55.125+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:55.126+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:55.126+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-07T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:55.126+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:55.127+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:56.014+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:56.043+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:56.043+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:56.051+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:56.052+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:56.159+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:56.212+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:56.245+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:56.754+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-07T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:56.757+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:56.318906+00:00, run_end_date=2025-02-20 16:46:56.465558+00:00, run_duration=0.146652, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=244, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:55.125894+00:00, queued_by_job_id=91, pid=41937
[2025-02-20T17:46:57.717+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-10 00:00:00+00:00, run_after=2024-07-11 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:57.714329+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:57.748+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:57.748+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:57.748+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:57.749+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:57.749+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-09T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:57.749+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:57.750+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:58.639+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:58.675+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:58.676+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:58.684+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:58.685+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:58.812+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:58.872+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:58.909+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:59.380+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-09T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:59.386+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:58.982453+00:00, run_end_date=2025-02-20 16:46:59.093407+00:00, run_duration=0.110954, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=246, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:57.748802+00:00, queued_by_job_id=91, pid=41947
[2025-02-20T17:47:00.915+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-12 00:00:00+00:00, run_after=2024-07-13 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:47:00.912907+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:47:00.950+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:00.950+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:47:00.950+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:00.951+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:47:00.951+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-11T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:47:00.951+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:00.952+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:01.794+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:47:01.822+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:47:01.822+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:01.831+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:01.832+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:47:01.940+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:01.992+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:02.025+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:47:02.507+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-11T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:47:02.518+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:47:02.099398+00:00, run_end_date=2025-02-20 16:47:02.224283+00:00, run_duration=0.124885, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=248, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:47:00.950633+00:00, queued_by_job_id=91, pid=41957
[2025-02-20T17:47:03.781+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-14 00:00:00+00:00, run_after=2024-07-15 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:47:03.777034+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:47:03.825+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:03.825+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:47:03.825+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:03.826+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:47:03.826+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-13T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:47:03.826+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:03.827+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:04.955+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:47:04.987+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:47:04.987+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:04.996+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:04.997+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:47:05.341+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:05.429+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:05.464+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:47:05.973+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-13T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:47:05.982+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:47:05.555625+00:00, run_end_date=2025-02-20 16:47:05.698827+00:00, run_duration=0.143202, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=250, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:47:03.825945+00:00, queued_by_job_id=91, pid=41968
[2025-02-20T17:49:37.825+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:50:41.982+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:50:41.983+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:50:41.983+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:50:41.984+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:50:41.984+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-29T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:50:41.984+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:50:41.985+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:50:42.721+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:50:42.753+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:50:42.753+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:50:42.761+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:50:42.761+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:50:42.876+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:50:43.020+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:50:43.140+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:50:43.719+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-29T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:50:43.724+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:50:43.282950+00:00, run_end_date=2025-02-20 16:50:43.442614+00:00, run_duration=0.159664, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=252, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:50:41.983989+00:00, queued_by_job_id=91, pid=42476
[2025-02-20T17:50:44.652+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-29 00:00:00+00:00: scheduled__2024-06-29T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:45:38.701443+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-29T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:50:44.652+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-29 00:00:00+00:00, run_id=scheduled__2024-06-29T00:00:00+00:00, run_start_date=2025-02-20 16:45:38.710852+00:00, run_end_date=2025-02-20 16:50:44.652456+00:00, run_duration=305.941604, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-29 00:00:00+00:00, data_interval_end=2024-06-30 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:50:44.654+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-30 00:00:00+00:00, run_after=2024-07-01 00:00:00+00:00
[2025-02-20T17:50:46.468+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-02 00:00:00+00:00, run_after=2024-07-03 00:00:00+00:00
[2025-02-20T17:50:48.810+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-04 00:00:00+00:00, run_after=2024-07-05 00:00:00+00:00
[2025-02-20T17:50:50.879+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-06 00:00:00+00:00, run_after=2024-07-07 00:00:00+00:00
[2025-02-20T17:50:52.811+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-08 00:00:00+00:00, run_after=2024-07-09 00:00:00+00:00
[2025-02-20T17:50:54.724+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-10 00:00:00+00:00, run_after=2024-07-11 00:00:00+00:00
[2025-02-20T17:50:56.684+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-12 00:00:00+00:00, run_after=2024-07-13 00:00:00+00:00
[2025-02-20T17:50:57.966+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-14 00:00:00+00:00, run_after=2024-07-15 00:00:00+00:00
[2025-02-20T17:51:00.057+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2025-02-20T17:51:48.022+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:48.022+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:51:48.022+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:48.023+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:48.023+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-01T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:48.023+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:48.024+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:48.987+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:49.015+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:49.016+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:49.025+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:49.025+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:49.133+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:49.191+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:49.224+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:49.656+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-01T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:49.659+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:49.300646+00:00, run_end_date=2025-02-20 16:51:49.386437+00:00, run_duration=0.085791, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=255, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:48.023112+00:00, queued_by_job_id=91, pid=42652
[2025-02-20T17:51:50.584+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-01 00:00:00+00:00: scheduled__2024-07-01T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:46.351878+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-01T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:50.585+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-01 00:00:00+00:00, run_id=scheduled__2024-07-01T00:00:00+00:00, run_start_date=2025-02-20 16:46:46.359351+00:00, run_end_date=2025-02-20 16:51:50.585312+00:00, run_duration=304.225961, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-01 00:00:00+00:00, data_interval_end=2024-07-02 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:50.587+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-02 00:00:00+00:00, run_after=2024-07-03 00:00:00+00:00
[2025-02-20T17:51:50.589+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-30 00:00:00+00:00: scheduled__2024-06-30T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:45.659839+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-30T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:50.589+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-30 00:00:00+00:00, run_id=scheduled__2024-06-30T00:00:00+00:00, run_start_date=2025-02-20 16:46:45.668275+00:00, run_end_date=2025-02-20 16:51:50.589836+00:00, run_duration=304.921561, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-30 00:00:00+00:00, data_interval_end=2024-07-01 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:50.591+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-01 00:00:00+00:00, run_after=2024-07-02 00:00:00+00:00
[2025-02-20T17:51:50.595+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:50.596+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:51:50.596+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:50.597+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:50.597+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-02T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:50.597+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:50.598+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:51.477+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:51.507+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:51.507+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:51.514+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:51.515+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:51.618+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:51.688+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:51.730+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:52.265+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-02T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:52.272+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:51.821800+00:00, run_end_date=2025-02-20 16:51:51.945382+00:00, run_duration=0.123582, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=256, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:50.596576+00:00, queued_by_job_id=91, pid=42664
[2025-02-20T17:51:53.189+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-03 00:00:00+00:00, run_after=2024-07-04 00:00:00+00:00
[2025-02-20T17:51:53.223+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:53.223+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:51:53.223+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:53.224+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:53.224+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-04T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:53.224+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:53.225+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:54.089+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:54.116+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:54.116+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:54.123+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:54.124+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:54.236+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:54.292+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:54.334+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:54.884+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-04T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:54.888+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:54.409469+00:00, run_end_date=2025-02-20 16:51:54.522479+00:00, run_duration=0.11301, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=258, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:53.224075+00:00, queued_by_job_id=91, pid=42674
[2025-02-20T17:51:55.797+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-04 00:00:00+00:00, run_after=2024-07-05 00:00:00+00:00
[2025-02-20T17:51:55.846+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:55.846+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:51:55.846+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:55.847+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:55.847+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-06T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:55.847+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:55.849+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:56.728+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:56.759+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:56.760+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:56.770+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:56.771+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:56.900+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:56.962+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:57.008+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:57.498+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-06T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:57.505+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:57.098917+00:00, run_end_date=2025-02-20 16:51:57.211488+00:00, run_duration=0.112571, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=260, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:55.846817+00:00, queued_by_job_id=91, pid=42682
[2025-02-20T17:51:58.377+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-06 00:00:00+00:00, run_after=2024-07-07 00:00:00+00:00
[2025-02-20T17:51:58.412+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-06 00:00:00+00:00: scheduled__2024-07-06T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:54.393069+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-06T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:58.413+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-06 00:00:00+00:00, run_id=scheduled__2024-07-06T00:00:00+00:00, run_start_date=2025-02-20 16:46:54.400299+00:00, run_end_date=2025-02-20 16:51:58.412958+00:00, run_duration=304.012659, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-06 00:00:00+00:00, data_interval_end=2024-07-07 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:58.414+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-07 00:00:00+00:00, run_after=2024-07-08 00:00:00+00:00
[2025-02-20T17:52:00.708+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-08 00:00:00+00:00, run_after=2024-07-09 00:00:00+00:00
[2025-02-20T17:52:00.734+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:00.734+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:52:00.734+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:00.734+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:00.735+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:00.735+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-08T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:00.735+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:00.735+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-09T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:00.735+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:00.736+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:01.613+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:01.645+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:01.645+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:01.653+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:01.653+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:01.761+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:01.829+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:01.875+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:02.404+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:03.235+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:03.263+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:03.263+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:03.271+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:03.271+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:03.372+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:03.426+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:03.463+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:03.974+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-08T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:03.975+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-09T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:03.981+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:01.957493+00:00, run_end_date=2025-02-20 16:52:02.148757+00:00, run_duration=0.191264, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=262, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:00.734810+00:00, queued_by_job_id=91, pid=42699
[2025-02-20T17:52:03.981+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:03.549583+00:00, run_end_date=2025-02-20 16:52:03.683468+00:00, run_duration=0.133885, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=263, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:00.734810+00:00, queued_by_job_id=91, pid=42705
[2025-02-20T17:52:04.952+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-10 00:00:00+00:00, run_after=2024-07-11 00:00:00+00:00
[2025-02-20T17:52:04.974+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-08 00:00:00+00:00: scheduled__2024-07-08T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:57.100145+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-08T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:04.975+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-08 00:00:00+00:00, run_id=scheduled__2024-07-08T00:00:00+00:00, run_start_date=2025-02-20 16:46:57.106666+00:00, run_end_date=2025-02-20 16:52:04.975230+00:00, run_duration=307.868564, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-08 00:00:00+00:00, data_interval_end=2024-07-09 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:04.977+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-09 00:00:00+00:00, run_after=2024-07-10 00:00:00+00:00
[2025-02-20T17:52:04.982+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:04.982+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:04.982+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:04.983+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:04.983+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-12T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:04.983+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:04.984+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:06.049+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:06.085+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:06.086+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:06.094+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:06.094+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:06.220+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:06.289+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:06.322+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:06.836+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-12T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:06.842+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:06.378051+00:00, run_end_date=2025-02-20 16:52:06.483722+00:00, run_duration=0.105671, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=266, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:04.982917+00:00, queued_by_job_id=91, pid=42715
[2025-02-20T17:52:07.870+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-10 00:00:00+00:00, run_after=2024-07-11 00:00:00+00:00
[2025-02-20T17:52:07.924+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-10 00:00:00+00:00: scheduled__2024-07-10T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:47:00.033917+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-10T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:07.925+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-10 00:00:00+00:00, run_id=scheduled__2024-07-10T00:00:00+00:00, run_start_date=2025-02-20 16:47:00.042618+00:00, run_end_date=2025-02-20 16:52:07.925332+00:00, run_duration=307.882714, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-10 00:00:00+00:00, data_interval_end=2024-07-11 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:07.930+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-11 00:00:00+00:00, run_after=2024-07-12 00:00:00+00:00
[2025-02-20T17:52:07.934+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-09 00:00:00+00:00: scheduled__2024-07-09T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:57.714329+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-09T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:07.934+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-09 00:00:00+00:00, run_id=scheduled__2024-07-09T00:00:00+00:00, run_start_date=2025-02-20 16:46:57.721047+00:00, run_end_date=2025-02-20 16:52:07.934635+00:00, run_duration=310.213588, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-09 00:00:00+00:00, data_interval_end=2024-07-10 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:07.938+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-10 00:00:00+00:00, run_after=2024-07-11 00:00:00+00:00
[2025-02-20T17:52:07.950+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:07.950+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:52:07.951+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:07.951+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:07.954+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:07.954+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-13T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:07.954+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:07.954+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-14T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:07.954+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:07.956+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:08.915+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:08.942+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:08.942+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:08.951+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:08.952+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:09.065+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:09.127+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:09.161+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:09.629+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:10.348+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:10.372+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:10.373+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:10.380+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:10.380+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:10.476+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:10.528+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:10.573+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:11.254+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-13T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:11.255+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-14T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:11.260+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:09.249068+00:00, run_end_date=2025-02-20 16:52:09.344497+00:00, run_duration=0.095429, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=267, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:07.952667+00:00, queued_by_job_id=91, pid=42724
[2025-02-20T17:52:11.261+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:10.654306+00:00, run_end_date=2025-02-20 16:52:10.762094+00:00, run_duration=0.107788, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=268, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:07.952667+00:00, queued_by_job_id=91, pid=42728
[2025-02-20T17:52:12.233+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2025-02-20T17:52:12.242+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-14 00:00:00+00:00: scheduled__2024-07-14T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:47:06.304261+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-14T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:12.242+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-14 00:00:00+00:00, run_id=scheduled__2024-07-14T00:00:00+00:00, run_start_date=2025-02-20 16:47:06.310295+00:00, run_end_date=2025-02-20 16:52:12.242225+00:00, run_duration=305.93193, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-14 00:00:00+00:00, data_interval_end=2024-07-15 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:12.243+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-15 00:00:00+00:00, run_after=2024-07-16 00:00:00+00:00
[2025-02-20T17:52:13.427+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-17 00:00:00+00:00, run_after=2024-07-18 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:13.424647+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:13.440+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:13.440+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:52:13.440+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:13.441+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:13.441+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-16T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:13.441+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:13.442+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:14.526+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:14.580+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:14.581+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:14.593+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:14.595+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:14.699+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:14.763+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:14.800+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:15.226+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-16T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:15.230+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:14.882456+00:00, run_end_date=2025-02-20 16:52:14.990553+00:00, run_duration=0.108097, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=269, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:13.441108+00:00, queued_by_job_id=91, pid=42740
[2025-02-20T17:52:16.165+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-19 00:00:00+00:00, run_after=2024-07-20 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:16.161863+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:16.193+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:16.193+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:16.193+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:16.194+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:16.195+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-18T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:16.195+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:16.196+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:17.086+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:17.123+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:17.123+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:17.131+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:17.131+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:17.263+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:17.319+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:17.361+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:17.776+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-18T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:17.781+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:17.438988+00:00, run_end_date=2025-02-20 16:52:17.539214+00:00, run_duration=0.100226, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=271, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:16.194302+00:00, queued_by_job_id=91, pid=42750
[2025-02-20T17:52:18.732+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-21 00:00:00+00:00, run_after=2024-07-22 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:18.729653+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:18.760+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:18.760+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:18.766+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:18.767+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:18.767+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-20T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:18.768+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:18.769+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:19.793+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:19.822+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:19.822+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:19.831+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:19.831+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:19.938+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:20.000+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:20.037+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:20.502+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-20T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:20.505+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-20T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:20.116000+00:00, run_end_date=2025-02-20 16:52:20.224629+00:00, run_duration=0.108629, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=273, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:18.766806+00:00, queued_by_job_id=91, pid=42761
[2025-02-20T17:52:21.548+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-23 00:00:00+00:00, run_after=2024-07-24 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:21.545622+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:21.576+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:21.576+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:52:21.576+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:21.577+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:21.577+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-22T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:21.577+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:21.578+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:22.488+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:22.515+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:22.515+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:22.523+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:22.524+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:22.643+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:22.695+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:22.732+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:23.316+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-22T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:23.320+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:22.816382+00:00, run_end_date=2025-02-20 16:52:22.929712+00:00, run_duration=0.11333, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=275, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:21.577083+00:00, queued_by_job_id=91, pid=42774
[2025-02-20T17:52:24.333+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-25 00:00:00+00:00, run_after=2024-07-26 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:24.331043+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:24.367+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:24.367+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:24.367+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:24.369+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:24.369+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-24T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:24.369+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:24.371+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:25.211+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:25.239+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:25.239+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:25.247+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:25.248+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:25.353+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:25.417+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:25.455+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:26.087+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-24T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:26.094+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:25.533023+00:00, run_end_date=2025-02-20 16:52:25.758214+00:00, run_duration=0.225191, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=277, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:24.368260+00:00, queued_by_job_id=91, pid=42786
[2025-02-20T17:52:27.524+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-27 00:00:00+00:00, run_after=2024-07-28 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:27.521400+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:27.554+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:27.554+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:27.555+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:27.555+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:27.555+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-26T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:27.556+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:27.556+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:28.371+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:28.399+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:28.399+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:28.407+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:28.408+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:28.513+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:28.590+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:28.629+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:29.152+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-26T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:29.155+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:28.726478+00:00, run_end_date=2025-02-20 16:52:28.842173+00:00, run_duration=0.115695, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=279, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:27.555257+00:00, queued_by_job_id=91, pid=42798
[2025-02-20T17:52:30.112+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-29 00:00:00+00:00, run_after=2024-07-30 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:30.109714+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:30.144+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:30.144+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:30.144+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:30.145+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:30.145+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-28T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:30.145+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:30.146+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:30.980+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:31.016+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:31.016+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:31.024+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:31.026+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:31.163+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:31.247+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:31.299+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:31.793+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-28T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:31.798+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:31.390680+00:00, run_end_date=2025-02-20 16:52:31.546749+00:00, run_duration=0.156069, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=281, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:30.144736+00:00, queued_by_job_id=91, pid=42809
[2025-02-20T17:52:32.871+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:32.868969+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:32.914+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:32.914+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:32.914+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:32.915+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:32.915+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-30T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:32.915+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:32.916+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:33.757+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:33.786+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:33.786+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:33.798+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:33.799+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:33.921+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:33.990+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:34.040+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:34.907+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-30T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:34.911+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:34.145684+00:00, run_end_date=2025-02-20 16:52:34.261703+00:00, run_duration=0.116019, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=283, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:32.915108+00:00, queued_by_job_id=91, pid=42820
[2025-02-20T17:54:38.812+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:56:02.204+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:56:02.205+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:56:02.205+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:56:02.206+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:56:02.206+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-15T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:56:02.206+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:56:02.207+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:56:03.281+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:56:03.312+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:56:03.312+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:56:03.319+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:56:03.320+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:56:03.423+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:56:03.490+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:56:03.524+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:56:03.927+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-15T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:56:03.930+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:56:03.607875+00:00, run_end_date=2025-02-20 16:56:03.712841+00:00, run_duration=0.104966, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=284, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:56:02.205638+00:00, queued_by_job_id=91, pid=43326
[2025-02-20T17:56:07.033+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-18 00:00:00+00:00, run_after=2024-07-19 00:00:00+00:00
[2025-02-20T17:56:09.212+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-20 00:00:00+00:00, run_after=2024-07-21 00:00:00+00:00
[2025-02-20T17:56:11.179+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-22 00:00:00+00:00, run_after=2024-07-23 00:00:00+00:00
[2025-02-20T17:56:13.293+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2025-02-20T17:56:15.436+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-26 00:00:00+00:00, run_after=2024-07-27 00:00:00+00:00
[2025-02-20T17:56:17.710+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-28 00:00:00+00:00, run_after=2024-07-29 00:00:00+00:00
[2025-02-20T17:56:20.244+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-30 00:00:00+00:00, run_after=2024-07-31 00:00:00+00:00
[2025-02-20T17:56:21.735+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:56:21.732215+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:56:21.769+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [scheduled]>
[2025-02-20T17:56:21.769+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:56:21.769+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [scheduled]>
[2025-02-20T17:56:21.770+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:56:21.770+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-31T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:56:21.770+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:56:21.771+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:56:22.584+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:56:22.618+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:56:22.618+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:56:22.627+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:56:22.628+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:56:22.748+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:56:22.821+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:56:22.867+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:56:23.274+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-31T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:56:23.281+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:56:22.944483+00:00, run_end_date=2025-02-20 16:56:23.035490+00:00, run_duration=0.091007, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=285, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:56:21.769956+00:00, queued_by_job_id=91, pid=43368
[2025-02-20T17:57:17.826+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:17.827+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:17.827+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:17.827+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:17.828+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:17.828+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-17T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:17.828+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:17.828+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-18T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:17.828+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:17.829+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:18.980+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:19.005+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:19.005+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:19.013+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:19.013+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:19.109+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:19.223+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:19.267+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:19.805+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:20.583+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:20.615+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:20.615+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:20.622+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:20.623+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:20.722+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:20.780+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:20.812+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:21.277+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-17T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:21.278+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-18T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:21.283+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:19.340758+00:00, run_end_date=2025-02-20 16:57:19.458380+00:00, run_duration=0.117622, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=287, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:17.827605+00:00, queued_by_job_id=91, pid=43489
[2025-02-20T17:57:21.284+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:20.884482+00:00, run_end_date=2025-02-20 16:57:20.995389+00:00, run_duration=0.110907, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=288, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:17.827605+00:00, queued_by_job_id=91, pid=43495
[2025-02-20T17:57:22.204+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-19 00:00:00+00:00, run_after=2024-07-20 00:00:00+00:00
[2025-02-20T17:57:22.235+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-17 00:00:00+00:00: scheduled__2024-07-17T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:14.951874+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-17T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:22.235+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-17 00:00:00+00:00, run_id=scheduled__2024-07-17T00:00:00+00:00, run_start_date=2025-02-20 16:52:14.959618+00:00, run_end_date=2025-02-20 16:57:22.235314+00:00, run_duration=307.275696, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-17 00:00:00+00:00, data_interval_end=2024-07-18 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:22.237+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-18 00:00:00+00:00, run_after=2024-07-19 00:00:00+00:00
[2025-02-20T17:57:22.242+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:22.242+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:22.242+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:22.243+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:22.243+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-21T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:22.244+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:22.244+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:23.344+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:23.412+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:23.412+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:23.422+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:23.423+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:23.547+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:23.626+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:23.661+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:24.228+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-21T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:24.235+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:23.749385+00:00, run_end_date=2025-02-20 16:57:23.913716+00:00, run_duration=0.164331, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=291, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:22.243034+00:00, queued_by_job_id=91, pid=43505
[2025-02-20T17:57:25.156+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-20 00:00:00+00:00, run_after=2024-07-21 00:00:00+00:00
[2025-02-20T17:57:25.182+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-20 00:00:00+00:00: scheduled__2024-07-20T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:18.729653+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-20T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:25.182+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-20 00:00:00+00:00, run_id=scheduled__2024-07-20T00:00:00+00:00, run_start_date=2025-02-20 16:52:18.736597+00:00, run_end_date=2025-02-20 16:57:25.182742+00:00, run_duration=306.446145, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-20 00:00:00+00:00, data_interval_end=2024-07-21 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:25.184+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-21 00:00:00+00:00, run_after=2024-07-22 00:00:00+00:00
[2025-02-20T17:57:25.185+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-21 00:00:00+00:00: scheduled__2024-07-21T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:20.183118+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-21T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:25.186+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-21 00:00:00+00:00, run_id=scheduled__2024-07-21T00:00:00+00:00, run_start_date=2025-02-20 16:52:20.188907+00:00, run_end_date=2025-02-20 16:57:25.186048+00:00, run_duration=304.997141, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-21 00:00:00+00:00, data_interval_end=2024-07-22 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:25.187+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-22 00:00:00+00:00, run_after=2024-07-23 00:00:00+00:00
[2025-02-20T17:57:27.301+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-23 00:00:00+00:00, run_after=2024-07-24 00:00:00+00:00
[2025-02-20T17:57:27.335+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:27.335+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:27.335+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:27.336+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:27.336+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-24T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:27.336+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:27.338+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:28.786+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:28.836+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:28.837+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:28.847+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:28.848+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:28.977+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:29.066+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:29.121+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:29.601+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-24T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:29.610+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:29.215647+00:00, run_end_date=2025-02-20 16:57:29.315055+00:00, run_duration=0.099408, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=294, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:27.335881+00:00, queued_by_job_id=91, pid=43521
[2025-02-20T17:57:30.609+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2025-02-20T17:57:30.630+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-24 00:00:00+00:00: scheduled__2024-07-24T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:24.331043+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-24T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:30.630+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-24 00:00:00+00:00, run_id=scheduled__2024-07-24T00:00:00+00:00, run_start_date=2025-02-20 16:52:24.337170+00:00, run_end_date=2025-02-20 16:57:30.630333+00:00, run_duration=306.293163, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-24 00:00:00+00:00, data_interval_end=2024-07-25 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:30.632+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-25 00:00:00+00:00, run_after=2024-07-26 00:00:00+00:00
[2025-02-20T17:57:30.633+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-23 00:00:00+00:00: scheduled__2024-07-23T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:22.857520+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-23T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:30.633+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-23 00:00:00+00:00, run_id=scheduled__2024-07-23T00:00:00+00:00, run_start_date=2025-02-20 16:52:22.863955+00:00, run_end_date=2025-02-20 16:57:30.633314+00:00, run_duration=307.769359, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-23 00:00:00+00:00, data_interval_end=2024-07-24 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:30.634+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2025-02-20T17:57:30.639+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:30.639+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:57:30.639+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:30.640+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:30.640+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-27T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:30.640+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:30.641+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:31.439+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:31.465+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:31.465+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:31.473+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:31.473+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:31.576+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:31.631+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:31.669+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:32.368+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-27T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:32.372+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:31.765856+00:00, run_end_date=2025-02-20 16:57:32.052184+00:00, run_duration=0.286328, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=296, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:30.639616+00:00, queued_by_job_id=91, pid=43531
[2025-02-20T17:57:33.603+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-25 00:00:00+00:00, run_after=2024-07-26 00:00:00+00:00
[2025-02-20T17:57:33.640+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:33.640+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:33.641+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:33.641+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:33.642+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:33.643+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-28T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:33.643+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:33.643+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-29T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:33.643+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:33.644+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:34.608+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:34.637+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:34.638+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:34.649+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:34.649+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:34.755+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:34.806+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:34.837+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:35.241+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:36.068+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:36.098+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:36.098+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:36.106+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:36.107+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:36.224+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:36.280+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:36.312+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:37.066+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-28T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:37.067+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-29T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:37.072+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:34.920761+00:00, run_end_date=2025-02-20 16:57:35.018920+00:00, run_duration=0.098159, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=298, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:33.642020+00:00, queued_by_job_id=91, pid=43540
[2025-02-20T17:57:37.072+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:36.433431+00:00, run_end_date=2025-02-20 16:57:36.794566+00:00, run_duration=0.361135, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=299, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:33.642020+00:00, queued_by_job_id=91, pid=43561
[2025-02-20T17:57:38.171+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-28 00:00:00+00:00, run_after=2024-07-29 00:00:00+00:00
[2025-02-20T17:57:38.184+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-28 00:00:00+00:00: scheduled__2024-07-28T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:30.109714+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-28T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:38.185+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-28 00:00:00+00:00, run_id=scheduled__2024-07-28T00:00:00+00:00, run_start_date=2025-02-20 16:52:30.115175+00:00, run_end_date=2025-02-20 16:57:38.184886+00:00, run_duration=308.069711, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-28 00:00:00+00:00, data_interval_end=2024-07-29 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:38.187+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-29 00:00:00+00:00, run_after=2024-07-30 00:00:00+00:00
[2025-02-20T17:57:40.020+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-31 00:00:00+00:00, run_after=2024-08-01 00:00:00+00:00
[2025-02-20T17:57:42.072+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-02 00:00:00+00:00, run_after=2024-08-03 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:42.070170+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:42.086+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:42.086+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:42.087+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:42.087+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:42.088+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-01T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:42.088+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:42.089+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:42.918+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:42.949+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:42.950+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:42.959+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:42.959+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:43.101+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:43.197+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:43.233+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:43.799+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-01T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:43.802+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:43.317903+00:00, run_end_date=2025-02-20 16:57:43.443746+00:00, run_duration=0.125843, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=301, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:42.087246+00:00, queued_by_job_id=91, pid=43579
[2025-02-20T17:57:45.164+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-04 00:00:00+00:00, run_after=2024-08-05 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:45.155263+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:45.184+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:45.184+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:45.184+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:45.185+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:45.185+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:45.185+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:45.186+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:46.162+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:46.190+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:46.190+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:46.199+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:46.200+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:46.312+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:46.380+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:46.417+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:46.885+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-03T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:46.890+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:46.492623+00:00, run_end_date=2025-02-20 16:57:46.617330+00:00, run_duration=0.124707, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=303, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:45.184897+00:00, queued_by_job_id=91, pid=43589
[2025-02-20T17:57:47.762+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-06 00:00:00+00:00, run_after=2024-08-07 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:47.759815+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:47.787+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:47.787+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:47.787+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:47.788+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:47.788+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-05T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:47.788+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:47.789+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:48.623+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:48.666+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:48.666+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:48.678+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:48.679+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:48.841+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:48.909+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:48.952+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:49.479+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-05T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:49.483+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:49.038655+00:00, run_end_date=2025-02-20 16:57:49.144458+00:00, run_duration=0.105803, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=305, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:47.788026+00:00, queued_by_job_id=91, pid=43599
[2025-02-20T17:57:50.590+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-08 00:00:00+00:00, run_after=2024-08-09 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:50.587832+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:50.618+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:50.618+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:50.618+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:50.619+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:50.619+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-07T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:50.619+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:50.620+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:51.441+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:51.467+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:51.467+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:51.474+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:51.476+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:51.575+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:51.633+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:51.669+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:52.117+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-07T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:52.121+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:51.747408+00:00, run_end_date=2025-02-20 16:57:51.838378+00:00, run_duration=0.09097, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=307, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:50.618697+00:00, queued_by_job_id=91, pid=43609
[2025-02-20T17:57:53.059+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-10 00:00:00+00:00, run_after=2024-08-11 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:53.056119+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:53.087+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:53.087+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:53.088+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:53.088+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:53.089+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-09T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:53.089+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:53.090+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:53.911+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:53.939+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:53.939+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:53.948+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:53.948+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:54.054+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:54.113+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:54.149+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:54.721+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-09T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:54.726+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:54.230415+00:00, run_end_date=2025-02-20 16:57:54.337887+00:00, run_duration=0.107472, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=309, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:53.088309+00:00, queued_by_job_id=91, pid=43619
[2025-02-20T17:57:56.093+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-12 00:00:00+00:00, run_after=2024-08-13 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:56.085364+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:56.154+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:56.154+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:56.154+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:56.155+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:56.156+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-11T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:56.156+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:56.157+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:57.373+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:57.403+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:57.404+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:57.413+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:57.413+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:57.526+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:57.588+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:57.626+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:58.107+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-11T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:58.114+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:57.741608+00:00, run_end_date=2025-02-20 16:57:57.843473+00:00, run_duration=0.101865, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=311, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:56.155111+00:00, queued_by_job_id=91, pid=43629
[2025-02-20T17:57:59.041+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-14 00:00:00+00:00, run_after=2024-08-15 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:59.038013+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:59.076+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:59.076+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:59.076+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:59.077+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:59.077+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-13T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:59.077+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:59.078+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:59.929+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:59.957+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:59.957+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:59.965+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:59.965+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:58:00.075+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:58:00.130+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:58:00.175+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:58:00.787+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-13T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:58:00.792+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:58:00.239711+00:00, run_end_date=2025-02-20 16:58:00.355274+00:00, run_duration=0.115563, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=313, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:59.076717+00:00, queued_by_job_id=91, pid=43639
[2025-02-20T17:58:02.032+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:58:02.029450+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:58:02.072+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:58:02.073+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:58:02.073+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:58:02.073+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:58:02.074+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-15T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:58:02.074+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:58:02.075+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:58:02.926+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:58:02.954+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:58:02.954+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:58:02.963+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:58:02.964+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:58:03.071+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:58:03.127+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:58:03.161+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:58:03.628+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-15T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:58:03.633+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:58:03.246364+00:00, run_end_date=2025-02-20 16:58:03.342666+00:00, run_duration=0.096302, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=315, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:58:02.073397+00:00, queued_by_job_id=91, pid=43649
[2025-02-20T17:59:39.687+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T18:01:27.691+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-03 00:00:00+00:00, run_after=2024-08-04 00:00:00+00:00
[2025-02-20T18:01:29.662+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-05 00:00:00+00:00, run_after=2024-08-06 00:00:00+00:00
[2025-02-20T18:01:31.661+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-08 00:00:00+00:00, run_after=2024-08-09 00:00:00+00:00
[2025-02-20T18:01:33.544+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-10 00:00:00+00:00, run_after=2024-08-11 00:00:00+00:00
[2025-02-20T18:01:34.890+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-11 00:00:00+00:00, run_after=2024-08-12 00:00:00+00:00
[2025-02-20T18:01:36.860+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-13 00:00:00+00:00, run_after=2024-08-14 00:00:00+00:00
[2025-02-20T18:01:38.895+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-15 00:00:00+00:00, run_after=2024-08-16 00:00:00+00:00
[2025-02-20T18:01:40.869+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 17:01:40.865676+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:01:40.904+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-16T00:00:00+00:00 [scheduled]>
[2025-02-20T18:01:40.904+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:01:40.904+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-16T00:00:00+00:00 [scheduled]>
[2025-02-20T18:01:40.905+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:01:40.905+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-16T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:01:40.905+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:01:40.906+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:01:41.798+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:01:41.831+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:01:41.831+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:01:41.840+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:01:41.840+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:01:41.954+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:01:42.017+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:01:42.051+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:01:42.532+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-16T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:01:42.538+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:01:42.136149+00:00, run_end_date=2025-02-20 17:01:42.270479+00:00, run_duration=0.13433, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=317, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:01:40.905071+00:00, queued_by_job_id=91, pid=44167
[2025-02-20T18:02:43.604+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:43.605+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:43.605+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:43.606+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:43.607+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-01T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:43.608+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:43.609+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:44.426+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:44.461+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:44.461+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:44.470+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:44.470+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:44.606+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:44.697+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:44.733+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:45.293+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-01T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:45.298+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:44.844497+00:00, run_end_date=2025-02-20 17:02:44.977464+00:00, run_duration=0.132967, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=319, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:43.608176+00:00, queued_by_job_id=3, pid=44306
[2025-02-20T18:02:46.384+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-01 00:00:00+00:00: scheduled__2024-08-01T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:42.070170+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-01T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:46.384+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-01 00:00:00+00:00, run_id=scheduled__2024-08-01T00:00:00+00:00, run_start_date=2025-02-20 16:57:42.076579+00:00, run_end_date=2025-02-20 17:02:46.384529+00:00, run_duration=304.30795, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-01 00:00:00+00:00, data_interval_end=2024-08-02 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:46.387+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-02 00:00:00+00:00, run_after=2024-08-03 00:00:00+00:00
[2025-02-20T18:02:46.393+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:46.394+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:46.394+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:46.416+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:46.416+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-02T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:46.417+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:46.418+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:47.220+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:47.249+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:47.250+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:47.258+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:47.258+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:47.363+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:47.421+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:47.460+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:48.115+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-02T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:48.119+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:47.580481+00:00, run_end_date=2025-02-20 17:02:47.759246+00:00, run_duration=0.178765, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=321, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:46.394533+00:00, queued_by_job_id=91, pid=44316
[2025-02-20T18:02:49.118+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-03 00:00:00+00:00, run_after=2024-08-04 00:00:00+00:00
[2025-02-20T18:02:49.153+0100] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:49.154+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:49.154+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:02:49.154+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T18:02:49.154+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:49.155+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:49.155+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-03T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:49.155+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:49.155+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-04T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:49.155+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:49.155+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-05T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:49.155+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:49.156+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:49.941+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:49.968+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:49.968+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:49.976+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:49.977+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:50.086+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:50.142+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:50.177+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:50.777+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:51.598+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:51.629+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:51.629+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:51.638+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:51.639+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:51.814+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:51.897+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:51.947+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [failed]> on host ajlas-macbook-air.local
[2025-02-20T18:02:52.231+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:53.043+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:53.079+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:53.080+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:53.087+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:53.088+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:53.199+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:53.256+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:53.293+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:53.832+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-03T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:53.832+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-04T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:53.833+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-05T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:53.836+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:50.282364+00:00, run_end_date=2025-02-20 17:02:50.414976+00:00, run_duration=0.132612, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=323, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:49.154583+00:00, queued_by_job_id=91, pid=44325
[2025-02-20T18:02:53.837+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:50.282681+00:00, run_end_date=2025-02-20 17:02:50.404894+00:00, run_duration=0.122213, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=322, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:49.179203+00:00, queued_by_job_id=3, pid=44326
[2025-02-20T18:02:53.837+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:53.369237+00:00, run_end_date=2025-02-20 17:02:53.450786+00:00, run_duration=0.081549, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=326, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:49.154583+00:00, queued_by_job_id=91, pid=44338
[2025-02-20T18:02:54.773+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-05 00:00:00+00:00, run_after=2024-08-06 00:00:00+00:00
[2025-02-20T18:02:54.795+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-06 00:00:00+00:00: scheduled__2024-08-06T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:49.350678+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-06T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:54.795+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-06 00:00:00+00:00, run_id=scheduled__2024-08-06T00:00:00+00:00, run_start_date=2025-02-20 16:57:49.358833+00:00, run_end_date=2025-02-20 17:02:54.795438+00:00, run_duration=305.436605, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-06 00:00:00+00:00, data_interval_end=2024-08-07 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:54.797+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-07 00:00:00+00:00, run_after=2024-08-08 00:00:00+00:00
[2025-02-20T18:02:54.798+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-05 00:00:00+00:00: scheduled__2024-08-05T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:47.759815+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-05T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:54.798+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-05 00:00:00+00:00, run_id=scheduled__2024-08-05T00:00:00+00:00, run_start_date=2025-02-20 16:57:47.767818+00:00, run_end_date=2025-02-20 17:02:54.798418+00:00, run_duration=307.0306, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-05 00:00:00+00:00, data_interval_end=2024-08-06 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:54.799+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-06 00:00:00+00:00, run_after=2024-08-07 00:00:00+00:00
[2025-02-20T18:02:56.732+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-07 00:00:00+00:00, run_after=2024-08-08 00:00:00+00:00
[2025-02-20T18:02:56.760+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:56.760+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T18:02:56.760+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:56.761+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:56.761+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-10T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:56.761+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:56.762+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:57.555+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:57.600+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:57.601+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:57.613+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:57.614+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:57.772+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:57.852+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:57.890+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:58.501+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-10T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:58.507+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:57.974843+00:00, run_end_date=2025-02-20 17:02:58.083463+00:00, run_duration=0.10862, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=329, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:56.760646+00:00, queued_by_job_id=91, pid=44354
[2025-02-20T18:02:59.671+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-08 00:00:00+00:00, run_after=2024-08-09 00:00:00+00:00
[2025-02-20T18:02:59.714+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-07 00:00:00+00:00: scheduled__2024-08-07T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:50.587832+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-07T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:59.715+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-07 00:00:00+00:00, run_id=scheduled__2024-08-07T00:00:00+00:00, run_start_date=2025-02-20 16:57:50.594884+00:00, run_end_date=2025-02-20 17:02:59.714857+00:00, run_duration=309.119973, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-07 00:00:00+00:00, data_interval_end=2024-08-08 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:59.718+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-08 00:00:00+00:00, run_after=2024-08-09 00:00:00+00:00
[2025-02-20T18:02:59.724+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:59.724+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:59.724+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:02:59.725+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:59.726+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:59.726+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-11T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:59.726+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:59.726+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-12T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:59.727+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:59.728+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:00.617+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:00.641+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:00.642+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:00.649+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:00.649+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:00.743+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:00.793+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:00.823+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:01.238+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:02.085+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:02.113+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:02.113+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:02.122+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:02.122+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:02.225+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:02.277+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:02.308+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:02.820+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-11T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:03:02.820+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-12T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:03:02.824+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:00.915489+00:00, run_end_date=2025-02-20 17:03:01.010879+00:00, run_duration=0.09539, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=331, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:59.725701+00:00, queued_by_job_id=91, pid=44362
[2025-02-20T18:03:02.824+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:02.383757+00:00, run_end_date=2025-02-20 17:03:02.481029+00:00, run_duration=0.097272, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=332, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:59.725701+00:00, queued_by_job_id=91, pid=44368
[2025-02-20T18:03:03.890+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-11 00:00:00+00:00, run_after=2024-08-12 00:00:00+00:00
[2025-02-20T18:03:03.912+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-11 00:00:00+00:00: scheduled__2024-08-11T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:56.085364+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-11T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:03.912+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-11 00:00:00+00:00, run_id=scheduled__2024-08-11T00:00:00+00:00, run_start_date=2025-02-20 16:57:56.101825+00:00, run_end_date=2025-02-20 17:03:03.912396+00:00, run_duration=307.810571, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-11 00:00:00+00:00, data_interval_end=2024-08-12 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:03.914+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-12 00:00:00+00:00, run_after=2024-08-13 00:00:00+00:00
[2025-02-20T18:03:03.920+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:03.921+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:03.921+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:03.921+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:03.922+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:03.922+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-14T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:03.922+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:03.922+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-15T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:03.922+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:03.923+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:04.816+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:04.847+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:04.848+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:04.858+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:04.859+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:04.987+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:05.051+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:05.084+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:05.488+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:06.351+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:06.379+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:06.379+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:06.389+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:06.389+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:06.514+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:06.583+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:06.623+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:07.047+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-14T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:03:07.048+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-15T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:03:07.051+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:05.162974+00:00, run_end_date=2025-02-20 17:03:05.265994+00:00, run_duration=0.10302, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=334, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:03.921681+00:00, queued_by_job_id=91, pid=44378
[2025-02-20T18:03:07.051+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:06.725077+00:00, run_end_date=2025-02-20 17:03:06.811922+00:00, run_duration=0.086845, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=335, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:03.921681+00:00, queued_by_job_id=91, pid=44383
[2025-02-20T18:03:07.905+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-15 00:00:00+00:00, run_after=2024-08-16 00:00:00+00:00
[2025-02-20T18:03:07.916+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-14 00:00:00+00:00: scheduled__2024-08-14T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:58:00.658338+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-14T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:07.916+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-14 00:00:00+00:00, run_id=scheduled__2024-08-14T00:00:00+00:00, run_start_date=2025-02-20 16:58:00.669585+00:00, run_end_date=2025-02-20 17:03:07.916642+00:00, run_duration=307.247057, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-14 00:00:00+00:00, data_interval_end=2024-08-15 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:07.918+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-15 00:00:00+00:00, run_after=2024-08-16 00:00:00+00:00
[2025-02-20T18:03:09.364+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-17 00:00:00+00:00, run_after=2024-08-18 00:00:00+00:00
[2025-02-20T18:03:11.271+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-19 00:00:00+00:00, run_after=2024-08-20 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:11.268224+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:11.287+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-18T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:11.288+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:11.288+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-18T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:11.289+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-18T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:11.289+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-18T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:11.289+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:11.290+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:12.217+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:12.245+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:12.245+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:12.254+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:12.255+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:12.361+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:12.416+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:12.449+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:12.903+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-18T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:12.907+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:12.523771+00:00, run_end_date=2025-02-20 17:03:12.636287+00:00, run_duration=0.112516, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=337, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:11.288568+00:00, queued_by_job_id=91, pid=44400
[2025-02-20T18:03:13.951+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:13.948825+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:13.971+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-20T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:13.972+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:13.972+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-20T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:13.972+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-20T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:13.973+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-20T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:13.973+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:13.973+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:14.811+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:14.839+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:14.839+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:14.847+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:14.847+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:14.947+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:15.002+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:15.035+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-20T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:15.449+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-20T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:15.452+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-20T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:15.109912+00:00, run_end_date=2025-02-20 17:03:15.193774+00:00, run_duration=0.083862, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=339, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:13.972456+00:00, queued_by_job_id=91, pid=44410
[2025-02-20T18:03:16.456+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-23 00:00:00+00:00, run_after=2024-08-24 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:16.454338+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:16.486+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-22T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:16.486+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:16.486+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-22T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:16.487+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-22T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:16.488+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-22T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:16.488+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:16.489+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:17.344+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:17.371+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:17.371+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:17.378+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:17.379+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:17.482+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:17.544+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:17.602+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:18.079+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-22T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:18.084+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:17.689221+00:00, run_end_date=2025-02-20 17:03:17.820927+00:00, run_duration=0.131706, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=341, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:16.487118+00:00, queued_by_job_id=91, pid=44421
[2025-02-20T18:03:18.961+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-25 00:00:00+00:00, run_after=2024-08-26 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:18.958523+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:18.986+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-24T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:18.986+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:18.986+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-24T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:18.987+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:18.987+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-24T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:18.987+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:18.988+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:19.808+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:19.835+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:19.836+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:19.844+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:19.845+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:19.950+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:20.001+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:20.033+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:20.498+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-24T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:20.502+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:20.109815+00:00, run_end_date=2025-02-20 17:03:20.192657+00:00, run_duration=0.082842, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=343, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:18.986864+00:00, queued_by_job_id=91, pid=44431
[2025-02-20T18:03:21.539+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:21.536227+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:21.596+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-26T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:21.596+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:21.596+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-26T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:21.597+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:21.598+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-26T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:21.598+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:21.599+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:22.588+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:22.624+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:22.624+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:22.633+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:22.634+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:22.750+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:22.810+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:22.847+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:23.266+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-26T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:23.273+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:22.925775+00:00, run_end_date=2025-02-20 17:03:23.032335+00:00, run_duration=0.10656, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=345, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:21.597080+00:00, queued_by_job_id=91, pid=44441
[2025-02-20T18:03:24.284+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-29 00:00:00+00:00, run_after=2024-08-30 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:24.280654+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:24.319+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-28T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:24.319+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:24.319+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-28T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:24.320+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:24.321+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-28T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:24.321+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:24.322+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:25.186+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:25.214+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:25.215+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:25.223+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:25.224+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:25.330+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:25.381+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:25.415+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:25.961+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-28T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:25.965+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:25.490151+00:00, run_end_date=2025-02-20 17:03:25.714568+00:00, run_duration=0.224417, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=347, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:24.320283+00:00, queued_by_job_id=91, pid=44451
[2025-02-20T18:03:26.942+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-31 00:00:00+00:00, run_after=2024-09-01 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:26.940443+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:26.977+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-30T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:26.977+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:26.977+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-30T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:26.978+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:26.978+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-30T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:26.978+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:26.979+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:27.894+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:27.927+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:27.928+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:27.937+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:27.937+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:28.057+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:28.117+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:28.156+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:28.713+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-30T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:28.719+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:28.272552+00:00, run_end_date=2025-02-20 17:03:28.414675+00:00, run_duration=0.142123, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=349, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:26.977643+00:00, queued_by_job_id=91, pid=44462
[2025-02-20T18:04:40.605+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T18:04:57.423+0100] {scheduler_job_runner.py:272} INFO - Exiting gracefully upon receiving signal 15
[2025-02-20T18:04:57.649+0100] {process_utils.py:266} INFO - Waiting up to 5 seconds for processes to exit...
[2025-02-20T18:04:57.987+0100] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 38191. PIDs of all processes in the group: []
[2025-02-20T18:04:57.988+0100] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 38191
[2025-02-20T18:04:57.988+0100] {process_utils.py:101} INFO - Sending the signal Signals.SIGTERM to process 38191 as process group is missing.
[2025-02-20T18:04:57.988+0100] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 38191. PIDs of all processes in the group: []
[2025-02-20T18:04:57.989+0100] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 38191
[2025-02-20T18:04:57.989+0100] {process_utils.py:101} INFO - Sending the signal Signals.SIGTERM to process 38191 as process group is missing.
[2025-02-20T18:04:57.989+0100] {scheduler_job_runner.py:1029} INFO - Exited execute loop
mand.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:36.610+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-23T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:36.616+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:36.240856+00:00, run_end_date=2025-02-20 16:35:36.335672+00:00, run_duration=0.094816, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=169, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:34.904608+00:00, queued_by_job_id=3, pid=40255
[2025-02-20T17:35:37.567+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-24 00:00:00+00:00, run_after=2024-05-25 00:00:00+00:00
[2025-02-20T17:35:37.590+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-23 00:00:00+00:00: scheduled__2024-05-23T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:32.356386+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-23T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:37.590+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-23 00:00:00+00:00, run_id=scheduled__2024-05-23T00:00:00+00:00, run_start_date=2025-02-20 16:30:32.367160+00:00, run_end_date=2025-02-20 16:35:37.590483+00:00, run_duration=305.223323, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-23 00:00:00+00:00, data_interval_end=2024-05-24 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:37.592+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-24 00:00:00+00:00, run_after=2024-05-25 00:00:00+00:00
[2025-02-20T17:35:37.593+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-22 00:00:00+00:00: scheduled__2024-05-22T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:30.772668+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-22T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:37.594+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-22 00:00:00+00:00, run_id=scheduled__2024-05-22T00:00:00+00:00, run_start_date=2025-02-20 16:30:30.783995+00:00, run_end_date=2025-02-20 16:35:37.594014+00:00, run_duration=306.810019, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-22 00:00:00+00:00, data_interval_end=2024-05-23 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:37.595+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-23 00:00:00+00:00, run_after=2024-05-24 00:00:00+00:00
[2025-02-20T17:35:39.570+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-24 00:00:00+00:00, run_after=2024-05-25 00:00:00+00:00
[2025-02-20T17:35:39.596+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:39.596+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:39.596+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:39.597+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:39.597+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-25T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:39.597+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:39.598+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:40.483+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:40.515+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:40.515+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:40.523+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:40.523+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:40.621+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:40.676+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:40.707+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:41.169+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-25T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:35:41.172+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:40.777768+00:00, run_end_date=2025-02-20 16:35:40.889540+00:00, run_duration=0.111772, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=171, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:39.596685+00:00, queued_by_job_id=3, pid=40269
[2025-02-20T17:35:42.327+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-27 00:00:00+00:00, run_after=2024-05-28 00:00:00+00:00
[2025-02-20T17:35:42.337+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-25 00:00:00+00:00: scheduled__2024-05-25T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:30:36.153029+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-25T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:35:42.338+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-25 00:00:00+00:00, run_id=scheduled__2024-05-25T00:00:00+00:00, run_start_date=2025-02-20 16:30:36.160462+00:00, run_end_date=2025-02-20 16:35:42.338062+00:00, run_duration=306.1776, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-25 00:00:00+00:00, data_interval_end=2024-05-26 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:42.339+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-26 00:00:00+00:00, run_after=2024-05-27 00:00:00+00:00
[2025-02-20T17:35:44.323+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-27 00:00:00+00:00, run_after=2024-05-28 00:00:00+00:00
[2025-02-20T17:35:46.295+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-28 00:00:00+00:00, run_after=2024-05-29 00:00:00+00:00
[2025-02-20T17:35:48.271+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-30 00:00:00+00:00, run_after=2024-05-31 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:48.268266+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:48.285+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:48.285+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:48.285+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:48.286+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:48.286+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-29T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:48.286+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:48.287+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:49.157+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:49.184+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:49.185+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:49.193+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:49.193+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:49.305+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:49.373+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:49.416+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:49.964+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-29T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:49.975+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:49.502794+00:00, run_end_date=2025-02-20 16:35:49.642842+00:00, run_duration=0.140048, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=173, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:48.285745+00:00, queued_by_job_id=3, pid=40292
[2025-02-20T17:35:51.031+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-01 00:00:00+00:00, run_after=2024-06-02 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:51.028540+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:51.048+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:51.048+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:51.048+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:51.049+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:51.049+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-31T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:51.049+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:51.050+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:52.170+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:52.200+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:52.200+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:52.210+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:52.214+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:52.322+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:52.379+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:52.422+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:53.034+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-31T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:53.042+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:52.511904+00:00, run_end_date=2025-02-20 16:35:52.619855+00:00, run_duration=0.107951, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=175, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:51.049054+00:00, queued_by_job_id=3, pid=40303
[2025-02-20T17:35:54.287+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-03 00:00:00+00:00, run_after=2024-06-04 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:54.284405+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:54.308+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:54.309+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:54.309+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:54.310+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:54.310+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-02T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:54.310+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:54.311+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:55.447+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:55.476+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:55.477+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:55.485+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:55.486+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:55.648+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:55.743+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:55.797+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:56.379+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-02T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:56.385+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:55.898410+00:00, run_end_date=2025-02-20 16:35:56.096544+00:00, run_duration=0.198134, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=177, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:54.309581+00:00, queued_by_job_id=3, pid=40313
[2025-02-20T17:35:57.362+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-05 00:00:00+00:00, run_after=2024-06-06 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:57.359918+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:57.388+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:57.389+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:57.389+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:57.389+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:57.390+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-04T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:57.390+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:57.391+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:58.208+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:35:58.235+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:35:58.236+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:58.243+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:58.244+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:35:58.349+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:35:58.404+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:35:58.438+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:35:58.912+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-04T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:35:58.915+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:35:58.515657+00:00, run_end_date=2025-02-20 16:35:58.627426+00:00, run_duration=0.111769, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=179, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:57.389499+00:00, queued_by_job_id=3, pid=40323
[2025-02-20T17:35:59.956+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-07 00:00:00+00:00, run_after=2024-06-08 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:35:59.951889+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:35:59.987+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:59.987+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:35:59.987+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:35:59.988+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:35:59.988+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:35:59.988+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:35:59.989+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:00.863+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:00.889+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:00.889+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:00.897+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:00.897+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:01.006+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:01.064+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:01.102+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:01.563+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:01.572+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:01.186321+00:00, run_end_date=2025-02-20 16:36:01.293825+00:00, run_duration=0.107504, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=181, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:35:59.987926+00:00, queued_by_job_id=3, pid=40335
[2025-02-20T17:36:03.094+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-09 00:00:00+00:00, run_after=2024-06-10 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:36:03.091434+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:36:03.127+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:03.127+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:36:03.127+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:03.128+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:36:03.128+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-08T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:36:03.128+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:03.129+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:03.980+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:04.006+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:04.006+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:04.013+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:04.014+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:04.118+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:04.176+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:04.220+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:04.752+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-08T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:04.812+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:04.300809+00:00, run_end_date=2025-02-20 16:36:04.409122+00:00, run_duration=0.108313, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=183, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:36:03.127720+00:00, queued_by_job_id=3, pid=40352
[2025-02-20T17:36:05.796+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-11 00:00:00+00:00, run_after=2024-06-12 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:36:05.794134+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:36:05.828+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:05.828+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:36:05.829+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:05.829+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:36:05.829+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-10T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:36:05.830+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:05.831+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:06.706+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:06.742+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:06.742+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:06.750+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:06.751+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:06.854+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:06.907+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:06.947+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:07.924+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-10T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:07.936+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:07.164637+00:00, run_end_date=2025-02-20 16:36:07.398325+00:00, run_duration=0.233688, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=185, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:36:05.829268+00:00, queued_by_job_id=3, pid=40363
[2025-02-20T17:36:09.065+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:36:09.063515+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:36:09.103+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:09.103+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:36:09.103+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:36:09.104+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:36:09.104+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:36:09.104+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:09.105+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:36:09.918+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:36:09.943+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:36:09.944+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:09.951+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:09.952+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:36:10.051+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:36:10.126+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:36:10.169+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:36:10.822+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-12T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:36:10.830+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:36:10.246101+00:00, run_end_date=2025-02-20 16:36:10.353922+00:00, run_duration=0.107821, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=187, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:36:09.103720+00:00, queued_by_job_id=3, pid=40376
[2025-02-20T17:39:46.537+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:39:58.118+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-30 00:00:00+00:00, run_after=2024-05-31 00:00:00+00:00
[2025-02-20T17:40:00.300+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-01 00:00:00+00:00, run_after=2024-06-02 00:00:00+00:00
[2025-02-20T17:40:02.127+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-03 00:00:00+00:00, run_after=2024-06-04 00:00:00+00:00
[2025-02-20T17:40:04.040+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-05 00:00:00+00:00, run_after=2024-06-06 00:00:00+00:00
[2025-02-20T17:40:06.138+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-07 00:00:00+00:00, run_after=2024-06-08 00:00:00+00:00
[2025-02-20T17:40:08.324+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-09 00:00:00+00:00, run_after=2024-06-10 00:00:00+00:00
[2025-02-20T17:40:10.925+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-11 00:00:00+00:00, run_after=2024-06-12 00:00:00+00:00
[2025-02-20T17:40:12.954+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-13 00:00:00+00:00, run_after=2024-06-14 00:00:00+00:00
[2025-02-20T17:40:14.350+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:40:14.347149+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:40:14.397+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:14.397+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:40:14.397+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:14.398+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:40:14.398+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-13T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:14.398+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:14.399+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:15.944+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:40:15.984+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:40:15.984+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:15.994+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:15.995+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:40:16.118+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:16.190+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:16.233+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:40:16.998+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-13T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:40:17.006+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:40:16.363911+00:00, run_end_date=2025-02-20 16:40:16.469714+00:00, run_duration=0.105803, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=189, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:14.397605+00:00, queued_by_job_id=3, pid=40929
[2025-02-20T17:40:50.834+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:50.836+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:40:50.836+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:40:50.836+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:50.838+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:40:50.840+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-29T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:50.840+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:50.841+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-30T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:50.841+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:50.845+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:51.653+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:40:51.685+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:40:51.686+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:51.694+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:51.694+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:40:51.801+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:51.875+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:51.923+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:40:52.393+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-05-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:53.199+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:40:53.226+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:40:53.226+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:53.234+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:53.234+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:40:53.336+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:53.393+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:53.426+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-05-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:40:53.877+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-29T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:40:53.878+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-05-30T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:40:53.881+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:40:52.004359+00:00, run_end_date=2025-02-20 16:40:52.115362+00:00, run_duration=0.111003, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=190, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:50.837658+00:00, queued_by_job_id=3, pid=41006
[2025-02-20T17:40:53.881+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-05-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:40:53.501791+00:00, run_end_date=2025-02-20 16:40:53.602402+00:00, run_duration=0.100611, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=191, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:50.837658+00:00, queued_by_job_id=3, pid=41015
[2025-02-20T17:40:55.302+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-05-29 00:00:00+00:00: scheduled__2024-05-29T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:48.268266+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-05-29T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:40:55.302+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-05-29 00:00:00+00:00, run_id=scheduled__2024-05-29T00:00:00+00:00, run_start_date=2025-02-20 16:35:48.275019+00:00, run_end_date=2025-02-20 16:40:55.302639+00:00, run_duration=307.02762, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-29 00:00:00+00:00, data_interval_end=2024-05-30 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:40:55.306+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-05-30 00:00:00+00:00, run_after=2024-05-31 00:00:00+00:00
[2025-02-20T17:40:55.311+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:55.311+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:40:55.311+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [scheduled]>
[2025-02-20T17:40:55.312+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:40:55.312+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-01T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:40:55.313+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:55.314+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:40:56.381+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:40:56.405+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:40:56.405+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:56.412+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:56.413+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:40:56.519+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:40:56.591+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:40:56.631+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:40:57.094+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-01T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:40:57.098+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:40:56.717972+00:00, run_end_date=2025-02-20 16:40:56.835472+00:00, run_duration=0.1175, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=193, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:40:55.312139+00:00, queued_by_job_id=3, pid=41029
[2025-02-20T17:40:58.225+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-02 00:00:00+00:00, run_after=2024-06-03 00:00:00+00:00
[2025-02-20T17:40:58.257+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-01 00:00:00+00:00: scheduled__2024-06-01T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:51.378430+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-01T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:40:58.257+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-01 00:00:00+00:00, run_id=scheduled__2024-06-01T00:00:00+00:00, run_start_date=2025-02-20 16:35:51.385140+00:00, run_end_date=2025-02-20 16:40:58.257736+00:00, run_duration=306.872596, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-01 00:00:00+00:00, data_interval_end=2024-06-02 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:40:58.259+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-02 00:00:00+00:00, run_after=2024-06-03 00:00:00+00:00
[2025-02-20T17:40:59.980+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-03 00:00:00+00:00, run_after=2024-06-04 00:00:00+00:00
[2025-02-20T17:41:00.038+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:00.039+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:00.039+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:41:00.039+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:00.040+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:00.040+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-04T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:00.041+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:00.041+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-05T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:00.041+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:00.042+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:00.938+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:00.984+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:00.984+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:00.992+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:00.992+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:01.096+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:01.159+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:01.192+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:01.687+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:02.617+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:02.649+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:02.650+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:02.665+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:02.665+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:02.785+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:02.855+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:02.895+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:03.379+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-04T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:03.380+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-05T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:03.385+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:01.270995+00:00, run_end_date=2025-02-20 16:41:01.379122+00:00, run_duration=0.108127, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=196, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:00.039688+00:00, queued_by_job_id=3, pid=41047
[2025-02-20T17:41:03.386+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:02.973132+00:00, run_end_date=2025-02-20 16:41:03.078622+00:00, run_duration=0.10549, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=197, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:00.039688+00:00, queued_by_job_id=3, pid=41054
[2025-02-20T17:41:04.461+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-04 00:00:00+00:00, run_after=2024-06-05 00:00:00+00:00
[2025-02-20T17:41:04.489+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-04 00:00:00+00:00: scheduled__2024-06-04T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:57.359918+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-04T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:04.489+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-04 00:00:00+00:00, run_id=scheduled__2024-06-04T00:00:00+00:00, run_start_date=2025-02-20 16:35:57.367009+00:00, run_end_date=2025-02-20 16:41:04.489448+00:00, run_duration=307.122439, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-04 00:00:00+00:00, data_interval_end=2024-06-05 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:04.493+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-05 00:00:00+00:00, run_after=2024-06-06 00:00:00+00:00
[2025-02-20T17:41:04.495+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-03 00:00:00+00:00: scheduled__2024-06-03T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:35:54.886205+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-03T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:04.496+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-03 00:00:00+00:00, run_id=scheduled__2024-06-03T00:00:00+00:00, run_start_date=2025-02-20 16:35:54.896813+00:00, run_end_date=2025-02-20 16:41:04.496724+00:00, run_duration=309.599911, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-03 00:00:00+00:00, data_interval_end=2024-06-04 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:04.500+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-04 00:00:00+00:00, run_after=2024-06-05 00:00:00+00:00
[2025-02-20T17:41:04.520+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:04.521+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:04.521+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:04.522+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:04.522+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-08T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:04.522+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:04.524+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:05.577+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:05.617+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:05.617+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:05.627+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:05.627+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:05.762+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:05.817+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:05.861+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:06.396+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-08T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:06.404+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:05.959465+00:00, run_end_date=2025-02-20 16:41:06.073481+00:00, run_duration=0.114016, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=199, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:04.521608+00:00, queued_by_job_id=3, pid=41064
[2025-02-20T17:41:07.723+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-06 00:00:00+00:00, run_after=2024-06-07 00:00:00+00:00
[2025-02-20T17:41:07.759+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-08 00:00:00+00:00: scheduled__2024-06-08T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:36:03.091434+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-08T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:07.759+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-08 00:00:00+00:00, run_id=scheduled__2024-06-08T00:00:00+00:00, run_start_date=2025-02-20 16:36:03.098634+00:00, run_end_date=2025-02-20 16:41:07.759853+00:00, run_duration=304.661219, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-08 00:00:00+00:00, data_interval_end=2024-06-09 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:07.761+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-09 00:00:00+00:00, run_after=2024-06-10 00:00:00+00:00
[2025-02-20T17:41:07.762+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-07 00:00:00+00:00: scheduled__2024-06-07T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:36:00.198802+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-07T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:07.763+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-07 00:00:00+00:00, run_id=scheduled__2024-06-07T00:00:00+00:00, run_start_date=2025-02-20 16:36:00.205075+00:00, run_end_date=2025-02-20 16:41:07.763010+00:00, run_duration=307.557935, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-07 00:00:00+00:00, data_interval_end=2024-06-08 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:07.764+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-08 00:00:00+00:00, run_after=2024-06-09 00:00:00+00:00
[2025-02-20T17:41:09.598+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-09 00:00:00+00:00, run_after=2024-06-10 00:00:00+00:00
[2025-02-20T17:41:11.707+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-10 00:00:00+00:00, run_after=2024-06-11 00:00:00+00:00
[2025-02-20T17:41:11.768+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-09 00:00:00+00:00: scheduled__2024-06-09T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:36:03.340886+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-09T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:11.769+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-09 00:00:00+00:00, run_id=scheduled__2024-06-09T00:00:00+00:00, run_start_date=2025-02-20 16:36:03.347936+00:00, run_end_date=2025-02-20 16:41:11.769363+00:00, run_duration=308.421427, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-09 00:00:00+00:00, data_interval_end=2024-06-10 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:11.778+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-10 00:00:00+00:00, run_after=2024-06-11 00:00:00+00:00
[2025-02-20T17:41:11.800+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:11.801+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:11.802+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:11.805+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:11.806+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-12T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:11.807+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:11.809+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:12.710+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:12.739+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:12.739+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:12.748+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:12.748+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:12.862+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:12.919+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:12.953+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:13.388+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-12T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:41:13.395+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:13.025546+00:00, run_end_date=2025-02-20 16:41:13.112782+00:00, run_duration=0.087236, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=204, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:11.803327+00:00, queued_by_job_id=3, pid=41088
[2025-02-20T17:41:14.783+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-12 00:00:00+00:00, run_after=2024-06-13 00:00:00+00:00
[2025-02-20T17:41:14.792+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-12 00:00:00+00:00: scheduled__2024-06-12T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:36:09.063515+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-12T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:14.792+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-12 00:00:00+00:00, run_id=scheduled__2024-06-12T00:00:00+00:00, run_start_date=2025-02-20 16:36:09.069571+00:00, run_end_date=2025-02-20 16:41:14.792631+00:00, run_duration=305.72306, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-12 00:00:00+00:00, data_interval_end=2024-06-13 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:14.794+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-13 00:00:00+00:00, run_after=2024-06-14 00:00:00+00:00
[2025-02-20T17:41:14.795+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-11 00:00:00+00:00: scheduled__2024-06-11T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:36:05.985501+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-11T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:41:14.795+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-11 00:00:00+00:00, run_id=scheduled__2024-06-11T00:00:00+00:00, run_start_date=2025-02-20 16:36:05.991137+00:00, run_end_date=2025-02-20 16:41:14.795347+00:00, run_duration=308.80421, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-11 00:00:00+00:00, data_interval_end=2024-06-12 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:14.796+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-12 00:00:00+00:00, run_after=2024-06-13 00:00:00+00:00
[2025-02-20T17:41:16.971+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-14 00:00:00+00:00, run_after=2024-06-15 00:00:00+00:00
[2025-02-20T17:41:18.921+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:18.918686+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:18.941+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:18.941+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:41:18.941+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:18.942+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:18.942+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:18.942+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:18.943+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:19.867+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:19.895+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:19.895+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:19.902+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:19.903+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:20.007+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:20.066+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:20.105+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:20.548+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:20.553+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:20.181307+00:00, run_end_date=2025-02-20 16:41:20.271872+00:00, run_duration=0.090565, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=206, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:18.941645+00:00, queued_by_job_id=3, pid=41112
[2025-02-20T17:41:21.440+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-18 00:00:00+00:00, run_after=2024-06-19 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:21.437520+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:21.461+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:21.461+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:21.461+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:21.462+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:21.462+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-17T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:21.462+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:21.463+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:22.608+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:22.695+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:22.695+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:22.708+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:22.709+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:22.869+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:22.931+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:22.970+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:23.408+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-17T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:23.414+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:23.060700+00:00, run_end_date=2025-02-20 16:41:23.168559+00:00, run_duration=0.107859, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=208, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:21.461625+00:00, queued_by_job_id=3, pid=41123
[2025-02-20T17:41:24.480+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-20 00:00:00+00:00, run_after=2024-06-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:24.476741+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:24.567+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:24.567+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:24.568+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:24.571+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:24.572+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:24.572+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:24.576+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:25.410+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:25.439+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:25.439+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:25.447+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:25.448+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:25.650+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:25.714+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:25.759+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:26.229+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:26.235+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:25.839640+00:00, run_end_date=2025-02-20 16:41:25.947705+00:00, run_duration=0.108065, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=210, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:24.568704+00:00, queued_by_job_id=3, pid=41135
[2025-02-20T17:41:27.229+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-22 00:00:00+00:00, run_after=2024-06-23 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:27.220744+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:27.257+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:27.257+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:27.257+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:27.261+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:27.262+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-21T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:27.262+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:27.264+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:28.490+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:28.570+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:28.571+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:28.590+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:28.591+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:28.826+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:28.904+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:28.949+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:29.487+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-21T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:29.498+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:29.036723+00:00, run_end_date=2025-02-20 16:41:29.174447+00:00, run_duration=0.137724, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=212, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:27.257723+00:00, queued_by_job_id=3, pid=41146
[2025-02-20T17:41:30.771+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-24 00:00:00+00:00, run_after=2024-06-25 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:30.765855+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:30.800+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:30.801+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:41:30.801+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:30.801+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:30.802+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-23T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:30.802+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:30.802+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:31.626+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:31.653+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:31.653+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:31.662+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:31.663+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:31.769+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:31.825+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:31.863+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:32.295+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-23T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:32.298+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:31.941737+00:00, run_end_date=2025-02-20 16:41:32.048275+00:00, run_duration=0.106538, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=214, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:30.801446+00:00, queued_by_job_id=3, pid=41156
[2025-02-20T17:41:33.247+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-26 00:00:00+00:00, run_after=2024-06-27 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:33.245331+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:33.281+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:33.281+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:33.281+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:33.282+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:33.282+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-25T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:33.282+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:33.283+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:34.349+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:34.389+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:34.389+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:34.398+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:34.399+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:34.541+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:34.656+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:34.758+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:35.246+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-25T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:35.252+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:34.860917+00:00, run_end_date=2025-02-20 16:41:34.965776+00:00, run_duration=0.104859, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=216, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:33.281809+00:00, queued_by_job_id=3, pid=41166
[2025-02-20T17:41:36.186+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-28 00:00:00+00:00, run_after=2024-06-29 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:41:36.183374+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:41:36.222+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:36.222+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:41:36.222+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:41:36.226+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:41:36.227+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-27T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:41:36.227+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:36.227+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:41:37.049+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:41:37.079+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:41:37.080+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:37.089+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:37.089+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:41:37.195+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:41:37.253+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:41:37.290+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:41:37.733+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-27T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:41:37.737+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:41:37.366699+00:00, run_end_date=2025-02-20 16:41:37.472769+00:00, run_duration=0.10607, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=218, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:41:36.222597+00:00, queued_by_job_id=3, pid=41176
[2025-02-20T17:44:47.483+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:45:17.196+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:45:17.197+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:45:17.197+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [scheduled]>
[2025-02-20T17:45:17.198+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:45:17.198+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-13T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:45:17.198+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:45:17.199+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:45:18.250+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:45:18.299+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:45:18.299+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:45:18.309+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:45:18.310+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:45:18.427+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:45:18.508+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:45:18.555+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:45:19.196+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-13T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:45:19.204+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:45:18.683306+00:00, run_end_date=2025-02-20 16:45:18.807165+00:00, run_duration=0.123859, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=220, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:45:17.197491+00:00, queued_by_job_id=3, pid=41674
[2025-02-20T17:45:20.459+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-13 00:00:00+00:00: scheduled__2024-06-13T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:40:14.347149+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-13T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:45:20.459+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-13 00:00:00+00:00, run_id=scheduled__2024-06-13T00:00:00+00:00, run_start_date=2025-02-20 16:40:14.354408+00:00, run_end_date=2025-02-20 16:45:20.459789+00:00, run_duration=306.105381, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-13 00:00:00+00:00, data_interval_end=2024-06-14 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:45:20.462+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-14 00:00:00+00:00, run_after=2024-06-15 00:00:00+00:00
[2025-02-20T17:45:22.675+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00
[2025-02-20T17:45:24.843+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00
[2025-02-20T17:45:26.042+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-19 00:00:00+00:00, run_after=2024-06-20 00:00:00+00:00
[2025-02-20T17:45:28.572+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-21 00:00:00+00:00, run_after=2024-06-22 00:00:00+00:00
[2025-02-20T17:45:30.684+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-23 00:00:00+00:00, run_after=2024-06-24 00:00:00+00:00
[2025-02-20T17:45:32.638+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-25 00:00:00+00:00, run_after=2024-06-26 00:00:00+00:00
[2025-02-20T17:45:34.707+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-27 00:00:00+00:00, run_after=2024-06-28 00:00:00+00:00
[2025-02-20T17:45:36.872+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-29 00:00:00+00:00, run_after=2024-06-30 00:00:00+00:00
[2025-02-20T17:46:21.642+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:21.643+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:21.643+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:21.643+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:21.644+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:21.644+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:21.644+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:21.644+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-16T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:21.644+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:21.646+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:22.486+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:22.526+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:22.527+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:22.534+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:22.535+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:22.647+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:22.700+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:22.740+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:23.276+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:24.167+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:24.203+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:24.204+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:24.213+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:24.214+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:24.581+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:24.671+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:24.707+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:25.291+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-15T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:25.294+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-16T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:25.302+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:22.827450+00:00, run_end_date=2025-02-20 16:46:22.938134+00:00, run_duration=0.110684, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=223, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:21.643759+00:00, queued_by_job_id=3, pid=41817
[2025-02-20T17:46:25.303+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:24.822842+00:00, run_end_date=2025-02-20 16:46:24.968702+00:00, run_duration=0.14586, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=224, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:21.643759+00:00, queued_by_job_id=3, pid=41824
[2025-02-20T17:46:26.734+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-17 00:00:00+00:00, run_after=2024-06-18 00:00:00+00:00
[2025-02-20T17:46:26.784+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-15 00:00:00+00:00: scheduled__2024-06-15T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:18.918686+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-15T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:26.784+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-15 00:00:00+00:00, run_id=scheduled__2024-06-15T00:00:00+00:00, run_start_date=2025-02-20 16:41:18.926267+00:00, run_end_date=2025-02-20 16:46:26.784561+00:00, run_duration=307.858294, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-15 00:00:00+00:00, data_interval_end=2024-06-16 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:26.787+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-16 00:00:00+00:00, run_after=2024-06-17 00:00:00+00:00
[2025-02-20T17:46:26.791+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:26.792+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:26.792+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:26.792+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:26.793+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:26.793+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-18T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:26.793+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:26.793+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-19T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:26.793+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:26.794+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:27.610+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:27.641+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:27.641+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:27.650+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:27.650+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:27.748+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:27.802+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:27.843+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:28.602+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:29.607+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:29.638+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:29.638+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:29.646+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:29.646+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:29.755+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:29.819+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:29.860+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:30.346+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-18T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:30.347+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-19T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:30.352+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:27.921272+00:00, run_end_date=2025-02-20 16:46:28.025531+00:00, run_duration=0.104259, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=226, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:26.792566+00:00, queued_by_job_id=3, pid=41833
[2025-02-20T17:46:30.353+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:29.939425+00:00, run_end_date=2025-02-20 16:46:30.076912+00:00, run_duration=0.137487, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=227, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:26.792566+00:00, queued_by_job_id=3, pid=41839
[2025-02-20T17:46:31.501+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-19 00:00:00+00:00, run_after=2024-06-20 00:00:00+00:00
[2025-02-20T17:46:31.529+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-18 00:00:00+00:00: scheduled__2024-06-18T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:23.134163+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-18T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:31.529+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-18 00:00:00+00:00, run_id=scheduled__2024-06-18T00:00:00+00:00, run_start_date=2025-02-20 16:41:23.146577+00:00, run_end_date=2025-02-20 16:46:31.529670+00:00, run_duration=308.383093, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-18 00:00:00+00:00, data_interval_end=2024-06-19 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:31.532+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-19 00:00:00+00:00, run_after=2024-06-20 00:00:00+00:00
[2025-02-20T17:46:31.536+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:31.536+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:31.536+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:31.537+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:31.537+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-22T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:31.537+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:31.538+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:32.397+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:32.425+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:32.426+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:32.436+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:32.437+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:32.580+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:32.641+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:32.679+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:33.135+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-22T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:33.139+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:32.762709+00:00, run_end_date=2025-02-20 16:46:32.862876+00:00, run_duration=0.100167, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=230, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:31.536921+00:00, queued_by_job_id=3, pid=41849
[2025-02-20T17:46:34.132+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-21 00:00:00+00:00, run_after=2024-06-22 00:00:00+00:00
[2025-02-20T17:46:34.152+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-21 00:00:00+00:00: scheduled__2024-06-21T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:27.220744+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-21T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:34.152+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-21 00:00:00+00:00, run_id=scheduled__2024-06-21T00:00:00+00:00, run_start_date=2025-02-20 16:41:27.234440+00:00, run_end_date=2025-02-20 16:46:34.152715+00:00, run_duration=306.918275, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-21 00:00:00+00:00, data_interval_end=2024-06-22 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:34.154+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-22 00:00:00+00:00, run_after=2024-06-23 00:00:00+00:00
[2025-02-20T17:46:34.155+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-22 00:00:00+00:00: scheduled__2024-06-22T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:29.084266+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-22T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:34.155+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-22 00:00:00+00:00, run_id=scheduled__2024-06-22T00:00:00+00:00, run_start_date=2025-02-20 16:41:29.093292+00:00, run_end_date=2025-02-20 16:46:34.155597+00:00, run_duration=305.062305, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-22 00:00:00+00:00, data_interval_end=2024-06-23 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:34.157+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-23 00:00:00+00:00, run_after=2024-06-24 00:00:00+00:00
[2025-02-20T17:46:36.272+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-24 00:00:00+00:00, run_after=2024-06-25 00:00:00+00:00
[2025-02-20T17:46:36.334+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:36.334+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:46:36.334+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:46:36.334+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:36.335+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:36.335+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-25T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:36.335+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:36.335+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-26T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:36.336+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:36.336+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:37.222+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:37.354+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:37.354+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:37.363+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:37.363+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:37.488+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:37.545+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:37.581+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:38.012+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:38.909+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:38.943+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:38.943+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:38.951+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:38.952+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:39.173+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:39.389+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:39.432+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:40.119+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-25T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:40.121+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-26T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:40.128+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:37.655400+00:00, run_end_date=2025-02-20 16:46:37.752315+00:00, run_duration=0.096915, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=233, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:36.334868+00:00, queued_by_job_id=3, pid=41865
[2025-02-20T17:46:40.128+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:39.553785+00:00, run_end_date=2025-02-20 16:46:39.833283+00:00, run_duration=0.279498, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=234, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:36.334868+00:00, queued_by_job_id=3, pid=41871
[2025-02-20T17:46:41.062+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-25 00:00:00+00:00, run_after=2024-06-26 00:00:00+00:00
[2025-02-20T17:46:41.079+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-25 00:00:00+00:00: scheduled__2024-06-25T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:33.245331+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-25T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:41.080+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-25 00:00:00+00:00, run_id=scheduled__2024-06-25T00:00:00+00:00, run_start_date=2025-02-20 16:41:33.252588+00:00, run_end_date=2025-02-20 16:46:41.080117+00:00, run_duration=307.827529, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-25 00:00:00+00:00, data_interval_end=2024-06-26 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:41.082+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-26 00:00:00+00:00, run_after=2024-06-27 00:00:00+00:00
[2025-02-20T17:46:41.084+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-24 00:00:00+00:00: scheduled__2024-06-24T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:31.964776+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-24T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:41.084+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-24 00:00:00+00:00, run_id=scheduled__2024-06-24T00:00:00+00:00, run_start_date=2025-02-20 16:41:31.972555+00:00, run_end_date=2025-02-20 16:46:41.084203+00:00, run_duration=309.111648, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-24 00:00:00+00:00, data_interval_end=2024-06-25 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:41.085+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-25 00:00:00+00:00, run_after=2024-06-26 00:00:00+00:00
[2025-02-20T17:46:41.090+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:41.090+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:41.090+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:41.091+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:41.091+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-28T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:41.091+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:41.092+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:41.878+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:41.903+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:41.904+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:41.911+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:41.912+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:42.010+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:42.089+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:42.130+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:42.577+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-28T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:46:42.581+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:42.218642+00:00, run_end_date=2025-02-20 16:46:42.312447+00:00, run_duration=0.093805, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=236, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:41.090952+00:00, queued_by_job_id=3, pid=41880
[2025-02-20T17:46:43.526+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-30 00:00:00+00:00, run_after=2024-07-01 00:00:00+00:00
[2025-02-20T17:46:43.533+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-06-28 00:00:00+00:00: scheduled__2024-06-28T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:41:37.485645+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-06-28T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:46:43.534+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-06-28 00:00:00+00:00, run_id=scheduled__2024-06-28T00:00:00+00:00, run_start_date=2025-02-20 16:41:37.490931+00:00, run_end_date=2025-02-20 16:46:43.533979+00:00, run_duration=306.043048, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-06-28 00:00:00+00:00, data_interval_end=2024-06-29 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:43.535+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-06-29 00:00:00+00:00, run_after=2024-06-30 00:00:00+00:00
[2025-02-20T17:46:45.664+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-01 00:00:00+00:00, run_after=2024-07-02 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:45.659839+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:45.682+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:45.682+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:45.683+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:45.683+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:45.684+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-30T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:45.684+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:45.685+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:46.503+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:46.533+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:46.533+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:46.541+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:46.542+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:46.648+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:46.700+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:46.733+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:47.259+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-30T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:47.263+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:46.802860+00:00, run_end_date=2025-02-20 16:46:46.936209+00:00, run_duration=0.133349, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=237, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:45.683325+00:00, queued_by_job_id=3, pid=41900
[2025-02-20T17:46:48.518+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-03 00:00:00+00:00, run_after=2024-07-04 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:48.516381+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:48.536+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:48.536+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:48.536+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:48.537+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:48.537+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-02T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:48.537+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:48.538+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:49.732+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:49.764+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:49.764+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:49.773+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:49.774+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:50.027+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:50.083+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:50.131+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:50.656+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-02T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:50.664+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:50.218911+00:00, run_end_date=2025-02-20 16:46:50.346200+00:00, run_duration=0.127289, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=239, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:48.537155+00:00, queued_by_job_id=3, pid=41914
[2025-02-20T17:46:51.592+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-05 00:00:00+00:00, run_after=2024-07-06 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:51.588761+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:51.613+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:51.614+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:51.614+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:51.615+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:51.615+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-04T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:51.615+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:51.616+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:52.516+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:52.550+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:52.551+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:52.558+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:52.559+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:52.662+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:52.721+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:52.763+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:53.223+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-04T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:53.229+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:52.847027+00:00, run_end_date=2025-02-20 16:46:52.957826+00:00, run_duration=0.110799, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=241, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:51.614563+00:00, queued_by_job_id=3, pid=41925
[2025-02-20T17:46:54.396+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-07 00:00:00+00:00, run_after=2024-07-08 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:54.393069+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:54.431+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:54.431+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:54.431+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:54.433+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:54.433+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:54.433+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:54.434+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:55.341+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:55.369+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:55.370+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:55.378+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:55.379+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:55.502+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:55.575+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:55.622+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:56.106+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:56.109+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:55.703459+00:00, run_end_date=2025-02-20 16:46:55.830376+00:00, run_duration=0.126917, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=243, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:54.432329+00:00, queued_by_job_id=3, pid=41934
[2025-02-20T17:46:57.102+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-09 00:00:00+00:00, run_after=2024-07-10 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:46:57.100145+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:46:57.131+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:57.132+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:46:57.132+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:46:57.132+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:46:57.133+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-08T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:46:57.133+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:57.134+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:46:57.966+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:46:57.993+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:46:57.994+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:58.002+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:58.002+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:46:58.110+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:46:58.176+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:46:58.224+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:46:58.743+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-08T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:46:58.747+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:46:58.305356+00:00, run_end_date=2025-02-20 16:46:58.420687+00:00, run_duration=0.115331, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=245, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:46:57.132448+00:00, queued_by_job_id=3, pid=41944
[2025-02-20T17:47:00.038+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-11 00:00:00+00:00, run_after=2024-07-12 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:47:00.033917+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:47:00.072+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:00.072+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:47:00.072+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:00.073+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:47:00.073+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-10T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:47:00.073+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:00.074+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:01.159+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:47:01.203+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:47:01.203+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:01.211+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:01.212+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:47:01.318+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:01.376+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:01.413+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:47:01.851+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-10T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:47:01.858+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:47:01.496374+00:00, run_end_date=2025-02-20 16:47:01.603184+00:00, run_duration=0.10681, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=247, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:47:00.072838+00:00, queued_by_job_id=3, pid=41954
[2025-02-20T17:47:03.077+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-13 00:00:00+00:00, run_after=2024-07-14 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:47:03.075105+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:47:03.112+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:03.112+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:47:03.112+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:03.113+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:47:03.113+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:47:03.113+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:03.114+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:04.210+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:47:04.238+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:47:04.238+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:04.246+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:04.247+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:47:04.369+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:04.425+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:04.466+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:47:05.065+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-12T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:47:05.081+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:47:04.558659+00:00, run_end_date=2025-02-20 16:47:04.721551+00:00, run_duration=0.162892, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=249, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:47:03.112802+00:00, queued_by_job_id=3, pid=41964
[2025-02-20T17:47:06.306+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:47:06.304261+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:47:06.346+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:06.346+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:47:06.347+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:47:06.347+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:47:06.348+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-14T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:47:06.348+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:06.349+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:47:07.150+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:47:07.177+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:47:07.178+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:07.186+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:07.187+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:47:07.304+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:47:07.355+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:47:07.386+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:47:07.792+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-14T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:47:07.796+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:47:07.468602+00:00, run_end_date=2025-02-20 16:47:07.559771+00:00, run_duration=0.091169, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=251, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:47:06.347381+00:00, queued_by_job_id=3, pid=41975
[2025-02-20T17:49:48.641+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:50:46.011+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-01 00:00:00+00:00, run_after=2024-07-02 00:00:00+00:00
[2025-02-20T17:50:48.158+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-03 00:00:00+00:00, run_after=2024-07-04 00:00:00+00:00
[2025-02-20T17:50:50.189+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-05 00:00:00+00:00, run_after=2024-07-06 00:00:00+00:00
[2025-02-20T17:50:51.904+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-07 00:00:00+00:00, run_after=2024-07-08 00:00:00+00:00
[2025-02-20T17:50:54.192+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-09 00:00:00+00:00, run_after=2024-07-10 00:00:00+00:00
[2025-02-20T17:50:56.245+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-11 00:00:00+00:00, run_after=2024-07-12 00:00:00+00:00
[2025-02-20T17:50:57.957+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-13 00:00:00+00:00, run_after=2024-07-14 00:00:00+00:00
[2025-02-20T17:51:00.357+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 16:51:00.354040+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:00.395+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:00.395+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:51:00.395+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:00.396+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:00.396+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-15T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:00.396+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:00.397+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:01.335+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:01.371+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:01.371+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:01.379+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:01.380+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:01.486+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:01.561+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:01.602+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:02.121+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-15T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:51:02.126+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:01.678053+00:00, run_end_date=2025-02-20 16:51:01.780363+00:00, run_duration=0.10231, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=253, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:00.395832+00:00, queued_by_job_id=3, pid=42517
[2025-02-20T17:51:47.526+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:47.527+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:51:47.527+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:47.528+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:47.528+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-30T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:47.528+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:47.529+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-06-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:48.343+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:48.379+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:48.380+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:48.389+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:48.389+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:48.533+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:48.642+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:48.689+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-06-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:49.177+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-06-30T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:49.182+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-06-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:48.771487+00:00, run_end_date=2025-02-20 16:51:48.922058+00:00, run_duration=0.150571, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=254, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:47.527905+00:00, queued_by_job_id=3, pid=42649
[2025-02-20T17:51:51.179+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-02 00:00:00+00:00, run_after=2024-07-03 00:00:00+00:00
[2025-02-20T17:51:51.214+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:51.214+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:51:51.214+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:51.215+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:51.215+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-03T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:51.215+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:51.216+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:52.146+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:52.181+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:52.182+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:52.190+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:52.191+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:52.300+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:52.353+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:52.386+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:52.850+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-03T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:52.857+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:52.461361+00:00, run_end_date=2025-02-20 16:51:52.567661+00:00, run_duration=0.1063, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=257, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:51.214631+00:00, queued_by_job_id=3, pid=42666
[2025-02-20T17:51:53.817+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-04 00:00:00+00:00, run_after=2024-07-05 00:00:00+00:00
[2025-02-20T17:51:53.846+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-03 00:00:00+00:00: scheduled__2024-07-03T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:49.172925+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-03T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:53.846+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-03 00:00:00+00:00, run_id=scheduled__2024-07-03T00:00:00+00:00, run_start_date=2025-02-20 16:46:49.178389+00:00, run_end_date=2025-02-20 16:51:53.846582+00:00, run_duration=304.668193, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-03 00:00:00+00:00, data_interval_end=2024-07-04 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:53.848+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-04 00:00:00+00:00, run_after=2024-07-05 00:00:00+00:00
[2025-02-20T17:51:53.849+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-02 00:00:00+00:00: scheduled__2024-07-02T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:48.516381+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-02T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:53.849+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-02 00:00:00+00:00, run_id=scheduled__2024-07-02T00:00:00+00:00, run_start_date=2025-02-20 16:46:48.522351+00:00, run_end_date=2025-02-20 16:51:53.849709+00:00, run_duration=305.327358, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-02 00:00:00+00:00, data_interval_end=2024-07-03 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:53.850+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-03 00:00:00+00:00, run_after=2024-07-04 00:00:00+00:00
[2025-02-20T17:51:53.854+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:53.855+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:51:53.855+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:53.856+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:53.856+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-05T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:53.856+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:53.857+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:54.740+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:54.778+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:54.779+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:54.788+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:54.789+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:54.907+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:54.964+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:54.996+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:51:55.417+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-05T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:51:55.420+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:55.070246+00:00, run_end_date=2025-02-20 16:51:55.164899+00:00, run_duration=0.094653, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=259, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:53.855535+00:00, queued_by_job_id=3, pid=42676
[2025-02-20T17:51:56.390+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-05 00:00:00+00:00, run_after=2024-07-06 00:00:00+00:00
[2025-02-20T17:51:56.412+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-05 00:00:00+00:00: scheduled__2024-07-05T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:52.208966+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-05T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:56.412+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-05 00:00:00+00:00, run_id=scheduled__2024-07-05T00:00:00+00:00, run_start_date=2025-02-20 16:46:52.218375+00:00, run_end_date=2025-02-20 16:51:56.412540+00:00, run_duration=304.194165, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-05 00:00:00+00:00, data_interval_end=2024-07-06 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:56.414+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-06 00:00:00+00:00, run_after=2024-07-07 00:00:00+00:00
[2025-02-20T17:51:56.415+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-04 00:00:00+00:00: scheduled__2024-07-04T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:51.588761+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-04T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:51:56.415+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-04 00:00:00+00:00, run_id=scheduled__2024-07-04T00:00:00+00:00, run_start_date=2025-02-20 16:46:51.595892+00:00, run_end_date=2025-02-20 16:51:56.415438+00:00, run_duration=304.819546, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-04 00:00:00+00:00, data_interval_end=2024-07-05 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:51:56.416+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-05 00:00:00+00:00, run_after=2024-07-06 00:00:00+00:00
[2025-02-20T17:51:58.375+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-06 00:00:00+00:00, run_after=2024-07-07 00:00:00+00:00
[2025-02-20T17:51:58.411+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:58.411+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:51:58.411+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [scheduled]>
[2025-02-20T17:51:58.416+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:51:58.417+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-07T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:51:58.417+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:58.418+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:51:59.439+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:51:59.467+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:51:59.467+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:59.477+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:59.478+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:51:59.582+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:51:59.637+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:51:59.675+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:00.284+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-07T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:00.289+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:51:59.749646+00:00, run_end_date=2025-02-20 16:51:59.848685+00:00, run_duration=0.099039, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=261, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:51:58.412262+00:00, queued_by_job_id=3, pid=42691
[2025-02-20T17:52:01.300+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-09 00:00:00+00:00, run_after=2024-07-10 00:00:00+00:00
[2025-02-20T17:52:01.320+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-07 00:00:00+00:00: scheduled__2024-07-07T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:46:55.094374+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-07T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:01.320+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-07 00:00:00+00:00, run_id=scheduled__2024-07-07T00:00:00+00:00, run_start_date=2025-02-20 16:46:55.100266+00:00, run_end_date=2025-02-20 16:52:01.320797+00:00, run_duration=306.220531, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-07 00:00:00+00:00, data_interval_end=2024-07-08 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:01.322+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-08 00:00:00+00:00, run_after=2024-07-09 00:00:00+00:00
[2025-02-20T17:52:03.263+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-09 00:00:00+00:00, run_after=2024-07-10 00:00:00+00:00
[2025-02-20T17:52:03.290+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:03.290+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:03.290+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:52:03.290+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:03.291+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:03.292+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-10T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:03.292+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:03.292+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-11T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:03.292+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:03.293+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:04.188+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:04.214+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:04.214+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:04.221+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:04.222+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:04.325+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:04.391+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:04.424+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:04.916+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:05.994+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:06.025+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:06.026+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:06.035+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:06.036+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:06.158+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:06.230+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:06.278+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:06.768+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-10T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:06.769+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-11T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:52:06.779+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:04.499163+00:00, run_end_date=2025-02-20 16:52:04.639090+00:00, run_duration=0.139927, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=264, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:03.291196+00:00, queued_by_job_id=3, pid=42708
[2025-02-20T17:52:06.779+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:06.351876+00:00, run_end_date=2025-02-20 16:52:06.457139+00:00, run_duration=0.105263, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=265, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:03.291196+00:00, queued_by_job_id=3, pid=42714
[2025-02-20T17:52:07.947+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-11 00:00:00+00:00, run_after=2024-07-12 00:00:00+00:00
[2025-02-20T17:52:07.985+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-12 00:00:00+00:00: scheduled__2024-07-12T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:47:03.075105+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-12T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:07.986+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-12 00:00:00+00:00, run_id=scheduled__2024-07-12T00:00:00+00:00, run_start_date=2025-02-20 16:47:03.081574+00:00, run_end_date=2025-02-20 16:52:07.985699+00:00, run_duration=304.904125, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-12 00:00:00+00:00, data_interval_end=2024-07-13 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:07.992+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-13 00:00:00+00:00, run_after=2024-07-14 00:00:00+00:00
[2025-02-20T17:52:07.995+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-11 00:00:00+00:00: scheduled__2024-07-11T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:47:00.912907+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-11T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:07.996+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-11 00:00:00+00:00, run_id=scheduled__2024-07-11T00:00:00+00:00, run_start_date=2025-02-20 16:47:00.918922+00:00, run_end_date=2025-02-20 16:52:07.996561+00:00, run_duration=307.077639, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-11 00:00:00+00:00, data_interval_end=2024-07-12 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:08.002+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-12 00:00:00+00:00, run_after=2024-07-13 00:00:00+00:00
[2025-02-20T17:52:09.638+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-13 00:00:00+00:00, run_after=2024-07-14 00:00:00+00:00
[2025-02-20T17:52:11.576+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-14 00:00:00+00:00, run_after=2024-07-15 00:00:00+00:00
[2025-02-20T17:52:11.587+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-13 00:00:00+00:00: scheduled__2024-07-13T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:47:03.777034+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-13T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:52:11.588+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-13 00:00:00+00:00, run_id=scheduled__2024-07-13T00:00:00+00:00, run_start_date=2025-02-20 16:47:03.786598+00:00, run_end_date=2025-02-20 16:52:11.588045+00:00, run_duration=307.801447, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-13 00:00:00+00:00, data_interval_end=2024-07-14 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:11.590+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-14 00:00:00+00:00, run_after=2024-07-15 00:00:00+00:00
[2025-02-20T17:52:12.986+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-16 00:00:00+00:00, run_after=2024-07-17 00:00:00+00:00
[2025-02-20T17:52:14.954+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-18 00:00:00+00:00, run_after=2024-07-19 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:14.951874+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:14.973+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:14.973+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:14.973+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:14.974+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:14.974+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-17T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:14.974+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:14.975+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:15.763+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:15.790+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:15.790+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:15.798+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:15.799+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:15.911+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:15.975+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:16.017+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:16.511+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-17T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:16.515+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:16.110781+00:00, run_end_date=2025-02-20 16:52:16.258852+00:00, run_duration=0.148071, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=270, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:14.973603+00:00, queued_by_job_id=3, pid=42745
[2025-02-20T17:52:17.498+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-20 00:00:00+00:00, run_after=2024-07-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:17.496039+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:17.519+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:17.519+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:17.519+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:17.520+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:17.520+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:17.520+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:17.521+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:18.321+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:18.349+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:18.350+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:18.357+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:18.358+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:18.460+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:18.524+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:18.583+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:19.287+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:19.292+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:18.668208+00:00, run_end_date=2025-02-20 16:52:19.006689+00:00, run_duration=0.338481, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=272, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:17.520071+00:00, queued_by_job_id=3, pid=42755
[2025-02-20T17:52:20.185+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-22 00:00:00+00:00, run_after=2024-07-23 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:20.183118+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:20.207+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:20.208+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:20.208+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:20.209+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:20.209+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-21T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:20.209+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:20.210+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:21.040+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:21.069+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:21.069+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:21.077+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:21.078+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:21.226+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:21.305+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:21.347+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:21.835+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-21T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:21.839+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:21.437859+00:00, run_end_date=2025-02-20 16:52:21.545618+00:00, run_duration=0.107759, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=274, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:20.208568+00:00, queued_by_job_id=3, pid=42767
[2025-02-20T17:52:22.860+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:22.857520+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:22.887+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:22.887+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:22.888+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:22.888+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:22.888+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-23T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:22.889+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:22.889+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:23.903+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:23.935+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:23.936+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:23.945+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:23.947+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:24.076+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:24.132+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:24.181+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:24.673+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-23T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:24.677+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:24.261781+00:00, run_end_date=2025-02-20 16:52:24.373917+00:00, run_duration=0.112136, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=276, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:22.888298+00:00, queued_by_job_id=3, pid=42780
[2025-02-20T17:52:25.577+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-26 00:00:00+00:00, run_after=2024-07-27 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:25.572476+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:25.697+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:25.697+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:25.697+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:25.698+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:25.698+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-25T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:25.698+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:25.701+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:27.012+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:27.048+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:27.049+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:27.057+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:27.058+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:27.205+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:27.287+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:27.338+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:27.807+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-25T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:27.814+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:27.417457+00:00, run_end_date=2025-02-20 16:52:27.534766+00:00, run_duration=0.117309, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=278, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:25.697815+00:00, queued_by_job_id=3, pid=42792
[2025-02-20T17:52:28.802+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-28 00:00:00+00:00, run_after=2024-07-29 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:28.799666+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:28.838+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:28.838+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:28.838+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:28.839+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:28.839+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-27T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:28.839+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:28.842+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:29.692+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:29.720+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:29.721+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:29.729+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:29.730+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:29.837+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:29.895+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:29.938+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:30.399+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-27T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:30.403+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:30.015167+00:00, run_end_date=2025-02-20 16:52:30.120551+00:00, run_duration=0.105384, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=280, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:28.838994+00:00, queued_by_job_id=3, pid=42804
[2025-02-20T17:52:31.442+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-30 00:00:00+00:00, run_after=2024-07-31 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:52:31.439971+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:52:31.479+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:31.479+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:52:31.479+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [scheduled]>
[2025-02-20T17:52:31.479+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:52:31.480+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-29T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:52:31.480+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:31.480+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:52:32.401+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:52:32.429+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:52:32.430+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:32.439+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:32.440+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:52:32.557+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:52:32.628+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:52:32.671+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:52:33.199+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-29T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:52:33.205+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:52:32.757550+00:00, run_end_date=2025-02-20 16:52:32.877860+00:00, run_duration=0.12031, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=282, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:52:31.479591+00:00, queued_by_job_id=3, pid=42814
[2025-02-20T17:54:49.236+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T17:56:05.026+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-15 00:00:00+00:00: scheduled__2024-07-15T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:51:00.354040+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-15T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:56:05.028+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-15 00:00:00+00:00, run_id=scheduled__2024-07-15T00:00:00+00:00, run_start_date=2025-02-20 16:51:00.361052+00:00, run_end_date=2025-02-20 16:56:05.028048+00:00, run_duration=304.666996, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-15 00:00:00+00:00, data_interval_end=2024-07-16 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:56:05.030+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-16 00:00:00+00:00, run_after=2024-07-17 00:00:00+00:00
[2025-02-20T17:56:06.848+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-17 00:00:00+00:00, run_after=2024-07-18 00:00:00+00:00
[2025-02-20T17:56:08.941+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-19 00:00:00+00:00, run_after=2024-07-20 00:00:00+00:00
[2025-02-20T17:56:10.410+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-21 00:00:00+00:00, run_after=2024-07-22 00:00:00+00:00
[2025-02-20T17:56:12.326+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-23 00:00:00+00:00, run_after=2024-07-24 00:00:00+00:00
[2025-02-20T17:56:14.140+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-25 00:00:00+00:00, run_after=2024-07-26 00:00:00+00:00
[2025-02-20T17:56:16.144+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-27 00:00:00+00:00, run_after=2024-07-28 00:00:00+00:00
[2025-02-20T17:56:18.304+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-29 00:00:00+00:00, run_after=2024-07-30 00:00:00+00:00
[2025-02-20T17:56:20.825+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-31 00:00:00+00:00, run_after=2024-08-01 00:00:00+00:00
[2025-02-20T17:57:15.551+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:15.552+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:15.552+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:15.553+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:15.553+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-16T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:15.553+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:15.554+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:16.306+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:16.339+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:16.339+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:16.347+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:16.347+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:16.455+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:16.522+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:16.583+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:17.235+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-16T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:17.249+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:16.666489+00:00, run_end_date=2025-02-20 16:57:16.843532+00:00, run_duration=0.177043, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=286, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:15.552682+00:00, queued_by_job_id=3, pid=43482
[2025-02-20T17:57:18.851+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-16 00:00:00+00:00: scheduled__2024-07-16T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:13.424647+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-16T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:18.851+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-16 00:00:00+00:00, run_id=scheduled__2024-07-16T00:00:00+00:00, run_start_date=2025-02-20 16:52:13.431029+00:00, run_end_date=2025-02-20 16:57:18.851818+00:00, run_duration=305.420789, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-16 00:00:00+00:00, data_interval_end=2024-07-17 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:18.854+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-17 00:00:00+00:00, run_after=2024-07-18 00:00:00+00:00
[2025-02-20T17:57:20.206+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-18 00:00:00+00:00, run_after=2024-07-19 00:00:00+00:00
[2025-02-20T17:57:20.242+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:20.242+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:20.242+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T17:57:20.242+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:20.243+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:20.243+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-19T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:20.243+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:20.244+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-20T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:20.244+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:20.244+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:21.028+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:21.053+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:21.053+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:21.061+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:21.061+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:21.168+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:21.224+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:21.260+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:21.717+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:22.491+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:22.518+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:22.519+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:22.529+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:22.529+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:22.709+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:22.821+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:22.875+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-20T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:23.499+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-19T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:23.500+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-20T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:23.506+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:21.338034+00:00, run_end_date=2025-02-20 16:57:21.423548+00:00, run_duration=0.085514, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=289, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:20.243056+00:00, queued_by_job_id=3, pid=43497
[2025-02-20T17:57:23.507+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-20T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:22.983265+00:00, run_end_date=2025-02-20 16:57:23.148363+00:00, run_duration=0.165098, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=290, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:20.243056+00:00, queued_by_job_id=3, pid=43503
[2025-02-20T17:57:24.564+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-19 00:00:00+00:00, run_after=2024-07-20 00:00:00+00:00
[2025-02-20T17:57:24.593+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-19 00:00:00+00:00: scheduled__2024-07-19T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:17.496039+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-19T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:24.593+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-19 00:00:00+00:00, run_id=scheduled__2024-07-19T00:00:00+00:00, run_start_date=2025-02-20 16:52:17.503751+00:00, run_end_date=2025-02-20 16:57:24.593374+00:00, run_duration=307.089623, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-19 00:00:00+00:00, data_interval_end=2024-07-20 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:24.595+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-20 00:00:00+00:00, run_after=2024-07-21 00:00:00+00:00
[2025-02-20T17:57:24.596+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-18 00:00:00+00:00: scheduled__2024-07-18T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:16.161863+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-18T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:24.597+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-18 00:00:00+00:00, run_id=scheduled__2024-07-18T00:00:00+00:00, run_start_date=2025-02-20 16:52:16.170686+00:00, run_end_date=2025-02-20 16:57:24.597008+00:00, run_duration=308.426322, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-18 00:00:00+00:00, data_interval_end=2024-07-19 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:24.598+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-19 00:00:00+00:00, run_after=2024-07-20 00:00:00+00:00
[2025-02-20T17:57:24.603+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:24.603+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:24.603+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:24.603+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:24.604+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:24.604+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-22T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:24.604+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:24.604+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-23T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:24.604+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:24.605+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:25.417+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:25.442+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:25.442+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:25.449+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:25.450+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:25.548+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:25.604+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:25.639+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:26.208+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:27.190+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:27.220+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:27.220+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:27.228+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:27.229+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:27.349+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:27.401+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:27.434+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:28.391+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-22T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:28.394+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-23T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:28.402+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:25.716823+00:00, run_end_date=2025-02-20 16:57:25.819463+00:00, run_duration=0.10264, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=292, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:24.603723+00:00, queued_by_job_id=3, pid=43511
[2025-02-20T17:57:28.403+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:27.502249+00:00, run_end_date=2025-02-20 16:57:27.986492+00:00, run_duration=0.484243, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=293, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:24.603723+00:00, queued_by_job_id=3, pid=43517
[2025-02-20T17:57:29.634+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-24 00:00:00+00:00, run_after=2024-07-25 00:00:00+00:00
[2025-02-20T17:57:29.659+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-22 00:00:00+00:00: scheduled__2024-07-22T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:21.545622+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-22T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:29.660+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-22 00:00:00+00:00, run_id=scheduled__2024-07-22T00:00:00+00:00, run_start_date=2025-02-20 16:52:21.552351+00:00, run_end_date=2025-02-20 16:57:29.660148+00:00, run_duration=308.107797, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-22 00:00:00+00:00, data_interval_end=2024-07-23 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:29.662+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-23 00:00:00+00:00, run_after=2024-07-24 00:00:00+00:00
[2025-02-20T17:57:29.666+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:29.666+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:29.666+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:29.667+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:29.667+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:29.667+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-25T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:29.668+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:29.668+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-26T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:29.668+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:29.669+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:30.476+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:30.503+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:30.503+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:30.512+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:30.513+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:30.631+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:30.686+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:30.722+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:31.143+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:32.206+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:32.237+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:32.237+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:32.245+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:32.246+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:32.357+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:32.411+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:32.446+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:32.908+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-25T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:32.910+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-26T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:32.916+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:30.799081+00:00, run_end_date=2025-02-20 16:57:30.891954+00:00, run_duration=0.092873, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=295, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:29.667237+00:00, queued_by_job_id=3, pid=43527
[2025-02-20T17:57:32.917+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:32.521363+00:00, run_end_date=2025-02-20 16:57:32.621672+00:00, run_duration=0.100309, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=297, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:29.667237+00:00, queued_by_job_id=3, pid=43533
[2025-02-20T17:57:34.251+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-26 00:00:00+00:00, run_after=2024-07-27 00:00:00+00:00
[2025-02-20T17:57:34.264+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-27 00:00:00+00:00: scheduled__2024-07-27T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:28.799666+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-27T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:34.264+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-27 00:00:00+00:00, run_id=scheduled__2024-07-27T00:00:00+00:00, run_start_date=2025-02-20 16:52:28.806405+00:00, run_end_date=2025-02-20 16:57:34.264827+00:00, run_duration=305.458422, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-27 00:00:00+00:00, data_interval_end=2024-07-28 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:34.267+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-28 00:00:00+00:00, run_after=2024-07-29 00:00:00+00:00
[2025-02-20T17:57:34.268+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-26 00:00:00+00:00: scheduled__2024-07-26T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:27.521400+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-26T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:34.269+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-26 00:00:00+00:00, run_id=scheduled__2024-07-26T00:00:00+00:00, run_start_date=2025-02-20 16:52:27.528555+00:00, run_end_date=2025-02-20 16:57:34.269094+00:00, run_duration=306.740539, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-26 00:00:00+00:00, data_interval_end=2024-07-27 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:34.270+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-27 00:00:00+00:00, run_after=2024-07-28 00:00:00+00:00
[2025-02-20T17:57:34.271+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-25 00:00:00+00:00: scheduled__2024-07-25T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:25.572476+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-25T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:34.271+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-25 00:00:00+00:00, run_id=scheduled__2024-07-25T00:00:00+00:00, run_start_date=2025-02-20 16:52:25.586509+00:00, run_end_date=2025-02-20 16:57:34.271650+00:00, run_duration=308.685141, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-25 00:00:00+00:00, data_interval_end=2024-07-26 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:34.272+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-26 00:00:00+00:00, run_after=2024-07-27 00:00:00+00:00
[2025-02-20T17:57:36.219+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-27 00:00:00+00:00, run_after=2024-07-28 00:00:00+00:00
[2025-02-20T17:57:36.239+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:36.239+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:36.239+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:36.240+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:36.240+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-30T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:36.240+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:36.241+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:37.393+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:37.423+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:37.423+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:37.431+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:37.431+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:37.537+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:37.609+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:37.643+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:38.231+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-30T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T17:57:38.236+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:37.739182+00:00, run_end_date=2025-02-20 16:57:37.868734+00:00, run_duration=0.129552, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=300, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:36.240006+00:00, queued_by_job_id=3, pid=43566
[2025-02-20T17:57:39.424+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-30 00:00:00+00:00, run_after=2024-07-31 00:00:00+00:00
[2025-02-20T17:57:39.433+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-30 00:00:00+00:00: scheduled__2024-07-30T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:32.868969+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-30T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:39.433+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-30 00:00:00+00:00, run_id=scheduled__2024-07-30T00:00:00+00:00, run_start_date=2025-02-20 16:52:32.875010+00:00, run_end_date=2025-02-20 16:57:39.433717+00:00, run_duration=306.558707, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-30 00:00:00+00:00, data_interval_end=2024-07-31 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:39.435+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-31 00:00:00+00:00, run_after=2024-08-01 00:00:00+00:00
[2025-02-20T17:57:39.436+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-29 00:00:00+00:00: scheduled__2024-07-29T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:52:31.439971+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-29T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T17:57:39.436+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-29 00:00:00+00:00, run_id=scheduled__2024-07-29T00:00:00+00:00, run_start_date=2025-02-20 16:52:31.446144+00:00, run_end_date=2025-02-20 16:57:39.436588+00:00, run_duration=307.990444, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-29 00:00:00+00:00, data_interval_end=2024-07-30 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:39.437+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-07-30 00:00:00+00:00, run_after=2024-07-31 00:00:00+00:00
[2025-02-20T17:57:41.361+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-01 00:00:00+00:00, run_after=2024-08-02 00:00:00+00:00
[2025-02-20T17:57:43.400+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-03 00:00:00+00:00, run_after=2024-08-04 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:43.397561+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:43.422+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:43.422+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:43.423+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:43.423+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:43.424+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-02T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:43.424+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:43.425+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:44.682+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:44.727+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:44.727+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:44.737+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:44.738+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:44.898+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:44.972+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:45.012+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:45.463+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-02T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:45.470+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:45.088664+00:00, run_end_date=2025-02-20 16:57:45.191900+00:00, run_duration=0.103236, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=302, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:43.423294+00:00, queued_by_job_id=3, pid=43584
[2025-02-20T17:57:46.556+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-05 00:00:00+00:00, run_after=2024-08-06 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:46.550953+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:46.579+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:46.579+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T17:57:46.579+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:46.580+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:46.580+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-04T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:46.580+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:46.581+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:47.382+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:47.413+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:47.414+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:47.423+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:47.423+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:47.530+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:47.590+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:47.626+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:48.087+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-04T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:48.093+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:47.705488+00:00, run_end_date=2025-02-20 16:57:47.832962+00:00, run_duration=0.127474, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=304, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:46.579814+00:00, queued_by_job_id=3, pid=43594
[2025-02-20T17:57:49.353+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-07 00:00:00+00:00, run_after=2024-08-08 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:49.350678+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:49.395+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:49.396+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:49.396+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:49.396+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:49.397+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:49.397+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:49.398+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:50.450+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:50.479+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:50.479+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:50.488+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:50.488+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:50.610+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:50.674+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:50.714+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:51.173+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:51.177+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:50.788010+00:00, run_end_date=2025-02-20 16:57:50.895730+00:00, run_duration=0.10772, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=306, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:49.396389+00:00, queued_by_job_id=3, pid=43605
[2025-02-20T17:57:52.071+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-09 00:00:00+00:00, run_after=2024-08-10 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:52.068707+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:52.102+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:52.102+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:52.102+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:52.103+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:52.103+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-08T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:52.103+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:52.104+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:52.929+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:52.956+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:52.956+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:52.964+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:52.965+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:53.074+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:53.127+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:53.159+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:53.619+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-08T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:53.623+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:53.232418+00:00, run_end_date=2025-02-20 16:57:53.328895+00:00, run_duration=0.096477, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=308, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:52.103036+00:00, queued_by_job_id=3, pid=43615
[2025-02-20T17:57:54.531+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-11 00:00:00+00:00, run_after=2024-08-12 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:54.528022+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:54.596+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:54.597+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:54.597+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:54.599+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:54.599+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-10T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:54.599+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:54.601+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:55.745+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:55.791+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:55.792+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:55.805+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:55.806+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:56.041+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:56.142+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:56.182+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:57.005+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-10T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:57.014+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:56.346271+00:00, run_end_date=2025-02-20 16:57:56.481551+00:00, run_duration=0.13528, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=310, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:54.597748+00:00, queued_by_job_id=3, pid=43625
[2025-02-20T17:57:57.999+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-13 00:00:00+00:00, run_after=2024-08-14 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:57:57.996215+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:57:58.037+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:58.037+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:57:58.037+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [scheduled]>
[2025-02-20T17:57:58.038+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:57:58.038+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:57:58.038+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:58.039+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:57:58.868+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:57:58.896+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:57:58.897+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:58.905+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:58.906+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:57:59.013+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:57:59.068+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:57:59.102+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:57:59.510+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-12T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:57:59.514+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:57:59.174152+00:00, run_end_date=2025-02-20 16:57:59.265252+00:00, run_duration=0.0911, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=312, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:57:58.037811+00:00, queued_by_job_id=3, pid=43635
[2025-02-20T17:58:00.662+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-15 00:00:00+00:00, run_after=2024-08-16 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 16:58:00.658338+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T17:58:00.726+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:58:00.726+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T17:58:00.726+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [scheduled]>
[2025-02-20T17:58:00.728+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T17:58:00.728+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-14T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T17:58:00.728+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:58:00.730+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T17:58:01.879+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T17:58:01.919+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T17:58:01.919+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:58:01.929+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:58:01.929+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T17:58:02.059+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T17:58:02.116+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T17:58:02.150+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T17:58:02.647+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-14T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T17:58:02.653+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 16:58:02.223876+00:00, run_end_date=2025-02-20 16:58:02.332558+00:00, run_duration=0.108682, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=314, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 16:58:00.727096+00:00, queued_by_job_id=3, pid=43645
[2025-02-20T17:59:50.109+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T18:01:23.222+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [scheduled]>
[2025-02-20T18:01:23.222+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:01:23.222+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [scheduled]>
[2025-02-20T18:01:23.223+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:01:23.223+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-31T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:01:23.224+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:01:23.225+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-07-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:01:24.090+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:01:24.124+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:01:24.124+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:01:24.131+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:01:24.132+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:01:24.234+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:01:24.301+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:01:24.335+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-07-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:01:24.811+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-07-31T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:01:24.815+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-07-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:01:24.424127+00:00, run_end_date=2025-02-20 17:01:24.512256+00:00, run_duration=0.088129, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=316, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:01:23.223169+00:00, queued_by_job_id=3, pid=44125
[2025-02-20T18:01:25.987+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-07-31 00:00:00+00:00: scheduled__2024-07-31T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:56:21.732215+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-07-31T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:01:25.987+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-07-31 00:00:00+00:00, run_id=scheduled__2024-07-31T00:00:00+00:00, run_start_date=2025-02-20 16:56:21.738824+00:00, run_end_date=2025-02-20 17:01:25.987699+00:00, run_duration=304.248875, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-31 00:00:00+00:00, data_interval_end=2024-08-01 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:01:25.990+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-01 00:00:00+00:00, run_after=2024-08-02 00:00:00+00:00
[2025-02-20T18:01:27.180+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-02 00:00:00+00:00, run_after=2024-08-03 00:00:00+00:00
[2025-02-20T18:01:28.309+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-04 00:00:00+00:00, run_after=2024-08-05 00:00:00+00:00
[2025-02-20T18:01:30.276+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-06 00:00:00+00:00, run_after=2024-08-07 00:00:00+00:00
[2025-02-20T18:01:31.295+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-07 00:00:00+00:00, run_after=2024-08-08 00:00:00+00:00
[2025-02-20T18:01:33.120+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-09 00:00:00+00:00, run_after=2024-08-10 00:00:00+00:00
[2025-02-20T18:01:35.273+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-12 00:00:00+00:00, run_after=2024-08-13 00:00:00+00:00
[2025-02-20T18:01:37.226+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-14 00:00:00+00:00, run_after=2024-08-15 00:00:00+00:00
[2025-02-20T18:01:39.111+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-16 00:00:00+00:00, run_after=2024-08-17 00:00:00+00:00
[2025-02-20T18:02:43.606+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:43.607+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:43.607+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:43.610+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:43.610+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-01T00:00:00+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:43.610+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:43.612+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:44.426+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:44.461+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:44.461+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:44.470+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:44.470+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:44.606+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:44.697+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:44.733+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:45.271+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-01T00:00:00+00:00', try_number=3, map_index=-1)
[2025-02-20T18:02:45.278+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:44.844497+00:00, run_end_date=2025-02-20 17:02:44.977464+00:00, run_duration=0.132967, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=1, job_id=319, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:43.608176+00:00, queued_by_job_id=3, pid=44306
[2025-02-20T18:02:46.395+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-01 00:00:00+00:00: scheduled__2024-08-01T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:42.070170+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-01T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:46.395+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-01 00:00:00+00:00, run_id=scheduled__2024-08-01T00:00:00+00:00, run_start_date=2025-02-20 16:57:42.076579+00:00, run_end_date=2025-02-20 17:02:46.395770+00:00, run_duration=304.319191, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-01 00:00:00+00:00, data_interval_end=2024-08-02 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:46.398+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-02 00:00:00+00:00, run_after=2024-08-03 00:00:00+00:00
[2025-02-20T18:02:46.403+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:46.403+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:46.403+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:46.404+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:46.404+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-02T00:00:00+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:46.404+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:46.405+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:47.220+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:47.249+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:47.250+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:47.258+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:47.258+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:47.363+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:47.421+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:47.460+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:48.115+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-02T00:00:00+00:00', try_number=3, map_index=-1)
[2025-02-20T18:02:48.119+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:47.580481+00:00, run_end_date=2025-02-20 17:02:47.759246+00:00, run_duration=0.178765, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=1, job_id=321, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:46.394533+00:00, queued_by_job_id=91, pid=44316
[2025-02-20T18:02:49.119+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-03 00:00:00+00:00, run_after=2024-08-04 00:00:00+00:00
[2025-02-20T18:02:49.172+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-02 00:00:00+00:00: scheduled__2024-08-02T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:43.397561+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-02T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:49.172+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-02 00:00:00+00:00, run_id=scheduled__2024-08-02T00:00:00+00:00, run_start_date=2025-02-20 16:57:43.403740+00:00, run_end_date=2025-02-20 17:02:49.172668+00:00, run_duration=305.768928, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-02 00:00:00+00:00, data_interval_end=2024-08-03 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:49.174+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-03 00:00:00+00:00, run_after=2024-08-04 00:00:00+00:00
[2025-02-20T18:02:49.178+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:49.178+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T18:02:49.179+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:49.179+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:49.179+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-04T00:00:00+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:49.179+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:49.180+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:49.945+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:49.971+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:49.971+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:49.979+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:49.980+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:50.086+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:50.142+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:50.177+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:50.777+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-04T00:00:00+00:00', try_number=3, map_index=-1)
[2025-02-20T18:02:50.781+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:50.282681+00:00, run_end_date=2025-02-20 17:02:50.404894+00:00, run_duration=0.122213, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=1, job_id=322, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:49.179203+00:00, queued_by_job_id=3, pid=44326
[2025-02-20T18:02:51.747+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-04 00:00:00+00:00, run_after=2024-08-05 00:00:00+00:00
[2025-02-20T18:02:51.807+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:51.807+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:02:51.807+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:51.808+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:51.808+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-06T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:51.809+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:51.810+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:52.658+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:52.687+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:52.687+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:52.696+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:52.696+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:52.812+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:52.868+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:52.903+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:53.329+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-06T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:53.333+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:52.975784+00:00, run_end_date=2025-02-20 17:02:53.060889+00:00, run_duration=0.085105, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=325, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:51.808116+00:00, queued_by_job_id=3, pid=44336
[2025-02-20T18:02:54.332+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-05 00:00:00+00:00, run_after=2024-08-06 00:00:00+00:00
[2025-02-20T18:02:54.358+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-04 00:00:00+00:00: scheduled__2024-08-04T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:46.550953+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-04T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:54.358+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-04 00:00:00+00:00, run_id=scheduled__2024-08-04T00:00:00+00:00, run_start_date=2025-02-20 16:57:46.563653+00:00, run_end_date=2025-02-20 17:02:54.358464+00:00, run_duration=307.794811, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-04 00:00:00+00:00, data_interval_end=2024-08-05 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:54.360+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-05 00:00:00+00:00, run_after=2024-08-06 00:00:00+00:00
[2025-02-20T18:02:54.363+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-03 00:00:00+00:00: scheduled__2024-08-03T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:45.155263+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-03T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:02:54.363+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-03 00:00:00+00:00, run_id=scheduled__2024-08-03T00:00:00+00:00, run_start_date=2025-02-20 16:57:45.169234+00:00, run_end_date=2025-02-20 17:02:54.363310+00:00, run_duration=309.194076, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-03 00:00:00+00:00, data_interval_end=2024-08-04 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:02:54.364+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-04 00:00:00+00:00, run_after=2024-08-05 00:00:00+00:00
[2025-02-20T18:02:54.369+0100] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:54.369+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:02:54.369+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:02:54.370+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 2/16 running and queued tasks
[2025-02-20T18:02:54.370+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [scheduled]>
[2025-02-20T18:02:54.370+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:02:54.371+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-07T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:54.371+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:54.371+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-08T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:54.371+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:54.371+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-09T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:02:54.371+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:54.372+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:55.145+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:55.169+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:55.169+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:55.177+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:55.177+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:55.271+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:55.321+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:55.352+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:55.775+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:56.575+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:56.618+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:56.618+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:56.631+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:56.632+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:56.744+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:56.796+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:56.828+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:57.246+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:02:58.205+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:02:58.235+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:02:58.242+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:58.265+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:58.266+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:02:58.494+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:02:58.559+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:02:58.709+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:02:59.136+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-07T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:59.137+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-08T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:59.138+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-09T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:02:59.143+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:55.437380+00:00, run_end_date=2025-02-20 17:02:55.516817+00:00, run_duration=0.079437, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=327, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:54.370352+00:00, queued_by_job_id=3, pid=44344
[2025-02-20T18:02:59.144+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:56.899998+00:00, run_end_date=2025-02-20 17:02:56.983538+00:00, run_duration=0.08354, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=328, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:54.370352+00:00, queued_by_job_id=3, pid=44350
[2025-02-20T18:02:59.144+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:02:58.791319+00:00, run_end_date=2025-02-20 17:02:58.885067+00:00, run_duration=0.093748, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=330, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:02:54.370352+00:00, queued_by_job_id=3, pid=44356
[2025-02-20T18:03:00.173+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-09 00:00:00+00:00, run_after=2024-08-10 00:00:00+00:00
[2025-02-20T18:03:00.223+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-10 00:00:00+00:00: scheduled__2024-08-10T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:54.528022+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-10T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:00.224+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-10 00:00:00+00:00, run_id=scheduled__2024-08-10T00:00:00+00:00, run_start_date=2025-02-20 16:57:54.535670+00:00, run_end_date=2025-02-20 17:03:00.224055+00:00, run_duration=305.688385, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-10 00:00:00+00:00, data_interval_end=2024-08-11 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:00.226+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-11 00:00:00+00:00, run_after=2024-08-12 00:00:00+00:00
[2025-02-20T18:03:00.228+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-09 00:00:00+00:00: scheduled__2024-08-09T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:53.056119+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-09T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:00.228+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-09 00:00:00+00:00, run_id=scheduled__2024-08-09T00:00:00+00:00, run_start_date=2025-02-20 16:57:53.064033+00:00, run_end_date=2025-02-20 17:03:00.228900+00:00, run_duration=307.164867, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-09 00:00:00+00:00, data_interval_end=2024-08-10 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:00.230+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-10 00:00:00+00:00, run_after=2024-08-11 00:00:00+00:00
[2025-02-20T18:03:00.232+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-08 00:00:00+00:00: scheduled__2024-08-08T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:52.068707+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-08T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:00.232+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-08 00:00:00+00:00, run_id=scheduled__2024-08-08T00:00:00+00:00, run_start_date=2025-02-20 16:57:52.076751+00:00, run_end_date=2025-02-20 17:03:00.232687+00:00, run_duration=308.155936, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-08 00:00:00+00:00, data_interval_end=2024-08-09 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:00.234+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-09 00:00:00+00:00, run_after=2024-08-10 00:00:00+00:00
[2025-02-20T18:03:02.164+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-10 00:00:00+00:00, run_after=2024-08-11 00:00:00+00:00
[2025-02-20T18:03:02.185+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:02.185+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T18:03:02.185+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:02.186+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:02.186+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-13T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:02.186+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:02.187+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:03.058+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:03.086+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:03.086+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:03.094+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:03.095+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:03.198+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:03.253+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:03.288+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:03.912+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-13T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T18:03:03.919+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:03.364694+00:00, run_end_date=2025-02-20 17:03:03.482062+00:00, run_duration=0.117368, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=333, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:02.185704+00:00, queued_by_job_id=3, pid=44371
[2025-02-20T18:03:04.904+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-13 00:00:00+00:00, run_after=2024-08-14 00:00:00+00:00
[2025-02-20T18:03:04.920+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-13 00:00:00+00:00: scheduled__2024-08-13T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:59.038013+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-13T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:04.920+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-13 00:00:00+00:00, run_id=scheduled__2024-08-13T00:00:00+00:00, run_start_date=2025-02-20 16:57:59.044880+00:00, run_end_date=2025-02-20 17:03:04.920603+00:00, run_duration=305.875723, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-13 00:00:00+00:00, data_interval_end=2024-08-14 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:04.922+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-14 00:00:00+00:00, run_after=2024-08-15 00:00:00+00:00
[2025-02-20T18:03:04.923+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-12 00:00:00+00:00: scheduled__2024-08-12T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:57:57.996215+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-12T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:04.923+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-12 00:00:00+00:00, run_id=scheduled__2024-08-12T00:00:00+00:00, run_start_date=2025-02-20 16:57:58.004553+00:00, run_end_date=2025-02-20 17:03:04.923661+00:00, run_duration=306.919108, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-12 00:00:00+00:00, data_interval_end=2024-08-13 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:04.925+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-13 00:00:00+00:00, run_after=2024-08-14 00:00:00+00:00
[2025-02-20T18:03:06.743+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-14 00:00:00+00:00, run_after=2024-08-15 00:00:00+00:00
[2025-02-20T18:03:08.620+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-16 00:00:00+00:00, run_after=2024-08-17 00:00:00+00:00
[2025-02-20T18:03:08.629+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-08-15 00:00:00+00:00: scheduled__2024-08-15T00:00:00+00:00, state:running, queued_at: 2025-02-20 16:58:02.029450+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-08-15T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T18:03:08.630+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-08-15 00:00:00+00:00, run_id=scheduled__2024-08-15T00:00:00+00:00, run_start_date=2025-02-20 16:58:02.036577+00:00, run_end_date=2025-02-20 17:03:08.630028+00:00, run_duration=306.593451, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-08-15 00:00:00+00:00, data_interval_end=2024-08-16 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:08.632+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-16 00:00:00+00:00, run_after=2024-08-17 00:00:00+00:00
[2025-02-20T18:03:10.816+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-18 00:00:00+00:00, run_after=2024-08-19 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:10.813104+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:10.829+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-17T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:10.829+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:10.829+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-17T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:10.830+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:10.830+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-17T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:10.830+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:10.831+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:11.666+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:11.709+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:11.709+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:11.720+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:11.721+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:11.862+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:11.925+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:11.964+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:12.399+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-17T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:12.402+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:12.052640+00:00, run_end_date=2025-02-20 17:03:12.159625+00:00, run_duration=0.106985, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=336, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:10.830100+00:00, queued_by_job_id=3, pid=44398
[2025-02-20T18:03:13.299+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:13.297002+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:13.315+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-19T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:13.316+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:13.316+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-19T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:13.316+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:13.317+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:13.317+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:13.317+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:14.103+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:14.130+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:14.130+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:14.138+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:14.138+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:14.240+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:14.297+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:14.332+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:14.837+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:14.841+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:14.406757+00:00, run_end_date=2025-02-20 17:03:14.513344+00:00, run_duration=0.106587, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=338, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:13.316416+00:00, queued_by_job_id=3, pid=44407
[2025-02-20T18:03:15.798+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-22 00:00:00+00:00, run_after=2024-08-23 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:15.795219+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:15.833+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-21T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:15.833+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:15.833+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-21T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:15.834+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:15.834+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-21T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:15.835+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:15.836+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:16.699+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:16.743+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:16.743+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:16.751+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:16.751+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:16.850+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:16.904+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:16.948+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:17.362+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-21T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:17.366+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:17.025514+00:00, run_end_date=2025-02-20 17:03:17.129072+00:00, run_duration=0.103558, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=340, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:15.834101+00:00, queued_by_job_id=3, pid=44418
[2025-02-20T18:03:18.335+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-24 00:00:00+00:00, run_after=2024-08-25 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:18.332882+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:18.361+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-23T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:18.361+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:18.361+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-23T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:18.361+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-23T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:18.362+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-23T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:18.362+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:18.363+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:19.175+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:19.200+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:19.201+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:19.208+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:19.209+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:19.311+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:19.369+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:19.405+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:19.869+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-23T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:19.873+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:19.479567+00:00, run_end_date=2025-02-20 17:03:19.613083+00:00, run_duration=0.133516, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=342, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:18.361557+00:00, queued_by_job_id=3, pid=44428
[2025-02-20T18:03:20.934+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:20.931334+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:20.975+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-25T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:20.975+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:20.975+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-25T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:20.976+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:20.976+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-25T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:20.976+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:20.977+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:21.892+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:21.922+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:21.922+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:21.931+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:21.931+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:22.065+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:22.168+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:22.220+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:22.699+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-25T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:22.704+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:22.312188+00:00, run_end_date=2025-02-20 17:03:22.416659+00:00, run_duration=0.104471, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=344, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:20.975945+00:00, queued_by_job_id=3, pid=44439
[2025-02-20T18:03:23.701+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-28 00:00:00+00:00, run_after=2024-08-29 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:23.697596+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:23.760+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-27T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:23.760+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:23.760+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-27T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:23.762+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:23.762+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-27T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:23.762+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:23.763+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:24.600+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:24.630+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:24.630+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:24.640+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:24.641+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:24.773+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:24.832+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:24.867+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:25.286+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-27T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:25.290+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:24.942456+00:00, run_end_date=2025-02-20 17:03:25.043848+00:00, run_duration=0.101392, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=346, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:23.761242+00:00, queued_by_job_id=3, pid=44449
[2025-02-20T18:03:26.311+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-08-30 00:00:00+00:00, run_after=2024-08-31 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:26.309094+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:26.345+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-29T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:26.346+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:26.346+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-29T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:26.347+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:26.347+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-29T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:26.347+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:26.348+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:27.244+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:27.271+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:27.271+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:27.279+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:27.279+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:27.381+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:27.435+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:27.473+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:28.034+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-29T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:28.042+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:27.590966+00:00, run_end_date=2025-02-20 17:03:27.740865+00:00, run_duration=0.149899, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=348, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:26.346630+00:00, queued_by_job_id=3, pid=44460
[2025-02-20T18:03:29.126+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 17:03:29.124280+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T18:03:29.162+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-31T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:29.162+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T18:03:29.162+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-08-31T00:00:00+00:00 [scheduled]>
[2025-02-20T18:03:29.163+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-08-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T18:03:29.163+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-31T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T18:03:29.163+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:29.164+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-08-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T18:03:30.049+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T18:03:30.074+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T18:03:30.074+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:30.082+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:30.082+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T18:03:30.178+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T18:03:30.229+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T18:03:30.261+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-08-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T18:03:30.719+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-08-31T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T18:03:30.724+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-08-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 17:03:30.341993+00:00, run_end_date=2025-02-20 17:03:30.427946+00:00, run_duration=0.085953, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=350, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 17:03:29.163052+00:00, queued_by_job_id=3, pid=44468
[2025-02-20 18:04:43 +0100] [6141] [ERROR] Worker (pid:6147) was sent SIGHUP!
[2025-02-20 18:04:43 +0100] [6141] [ERROR] Worker (pid:6148) was sent SIGHUP!
[2025-02-20 18:04:43 +0100] [44639] [INFO] Booting worker with pid: 44639
[2025-02-20 18:04:44 +0100] [44641] [INFO] Booting worker with pid: 44641
[2025-02-20 18:04:44 +0100] [6141] [INFO] Handling signal: hup
[2025-02-20 18:04:44 +0100] [6141] [INFO] Hang up: Master
[2025-02-20 18:04:44 +0100] [44639] [INFO] Worker exiting (pid: 44639)
[2025-02-20 18:04:44 +0100] [44641] [INFO] Worker exiting (pid: 44641)
[2025-02-20 18:04:44 +0100] [44644] [INFO] Booting worker with pid: 44644
[2025-02-20 18:04:44 +0100] [44645] [INFO] Booting worker with pid: 44645
[2025-02-20 18:04:44 +0100] [6141] [ERROR] Worker (pid:44639) was sent SIGTERM!
[2025-02-20T18:04:50.674+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T18:04:57.421+0100] {scheduler_job_runner.py:272} INFO - Exiting gracefully upon receiving signal 15
[2025-02-20 18:04:57 +0100] [6141] [INFO] Handling signal: term
[2025-02-20 18:04:57 +0100] [44644] [INFO] Worker exiting (pid: 44644)
[2025-02-20 18:04:57 +0100] [44645] [INFO] Worker exiting (pid: 44645)
[2025-02-20T18:04:57.978+0100] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 38245. PIDs of all processes in the group: []
[2025-02-20T18:04:57.979+0100] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 38245
[2025-02-20T18:04:57.979+0100] {process_utils.py:101} INFO - Sending the signal Signals.SIGTERM to process 38245 as process group is missing.
[2025-02-20T18:04:57.980+0100] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 38245. PIDs of all processes in the group: []
[2025-02-20T18:04:57.980+0100] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 38245
[2025-02-20T18:04:57.980+0100] {process_utils.py:101} INFO - Sending the signal Signals.SIGTERM to process 38245 as process group is missing.
[2025-02-20T18:04:57.981+0100] {scheduler_job_runner.py:1029} INFO - Exited execute loop
[2025-02-20 18:04:58 +0100] [6141] [INFO] Shutting down: Master
