  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2025-02-20T10:51:43.218+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T10:51:43.220+0100] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2025-02-20T10:51:43.762+0100] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-02-20T10:51:43.762+0100] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-02-20T10:51:43.773+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 6142
[2025-02-20T10:51:43.774+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20 10:51:44 +0100] [6141] [INFO] Starting gunicorn 23.0.0
[2025-02-20 10:51:44 +0100] [6141] [INFO] Listening at: http://[::]:8793 (6141)
[2025-02-20 10:51:44 +0100] [6141] [INFO] Using worker: sync
[2025-02-20 10:51:44 +0100] [6147] [INFO] Booting worker with pid: 6147
[2025-02-20T10:51:44.547+0100] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-20T10:51:44.557+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-20 10:51:44 +0100] [6148] [INFO] Booting worker with pid: 6148
[2025-02-20T10:52:08.649+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T10:56:45.331+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:01:46.217+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:06:46.292+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:11:47.095+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:16:47.915+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:21:48.396+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-20 10:22:06.777317+00:00 hash info: 3450ef91f3d80ed6998b7a12898ae148
[2025-02-20T11:22:08.402+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:22:08.403+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:22:08.403+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:22:08.404+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:22:08.405+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:22:08.405+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:22:08.406+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:22:09.072+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:22:09.077+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:22:09.107+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:22:09.107+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:22:09.115+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:22:09.116+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:22:09.192+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:22:09.260+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:22:09.279+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:22:09.293+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:22:09.688+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=1, map_index=-1)
[2025-02-20T11:22:09.693+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:22:06.760027+00:00, map_index=-1, run_start_date=2025-02-20 10:22:09.376931+00:00, run_end_date=2025-02-20 10:22:09.464082+00:00, run_duration=0.087151, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:22:08.403737+00:00, queued_by_job_id=3, pid=9826
[2025-02-20T11:24:10.599+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:24:10.600+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:24:10.600+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:24:10.600+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:24:10.601+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:24:10.601+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:24:10.602+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:24:11.335+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:24:11.341+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:24:11.377+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:24:11.377+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:24:11.385+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:24:11.385+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:24:11.457+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:24:11.651+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:24:11.778+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:24:11.798+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:24:12.213+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=2, map_index=-1)
[2025-02-20T11:24:12.219+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:22:06.760027+00:00, map_index=-1, run_start_date=2025-02-20 10:24:11.867444+00:00, run_end_date=2025-02-20 10:24:11.949798+00:00, run_duration=0.082354, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=5, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:24:10.600441+00:00, queued_by_job_id=3, pid=10050
[2025-02-20T11:26:13.412+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:26:13.413+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:26:13.413+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:26:13.414+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:26:13.414+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:26:13.414+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:26:13.415+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:26:14.001+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:26:14.006+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:26:14.035+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:26:14.035+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:26:14.043+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:26:14.043+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:26:14.112+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:26:14.197+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:26:14.226+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:26:14.241+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:26:14.641+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=3, map_index=-1)
[2025-02-20T11:26:14.644+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:22:06.760027+00:00, map_index=-1, run_start_date=2025-02-20 10:26:14.323915+00:00, run_end_date=2025-02-20 10:26:14.405339+00:00, run_duration=0.081424, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=5, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:26:13.413618+00:00, queued_by_job_id=3, pid=10312
Dag run  in running state
Dag information Queued at: 2025-02-20 10:26:44.450725+00:00 hash info: 3450ef91f3d80ed6998b7a12898ae148
[2025-02-20T11:26:46.767+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:26:46.768+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:26:46.768+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:26:46.769+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:26:46.769+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:26:46.769+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:26:46.770+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:26:47.355+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:26:47.360+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:26:47.392+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:26:47.392+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:26:47.400+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:26:47.400+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:26:47.469+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:26:47.538+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:26:47.556+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:26:47.570+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:26:47.981+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=1, map_index=-1)
[2025-02-20T11:26:47.984+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:26:44.438022+00:00, map_index=-1, run_start_date=2025-02-20 10:26:47.654499+00:00, run_end_date=2025-02-20 10:26:47.735322+00:00, run_duration=0.080823, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:26:46.768718+00:00, queued_by_job_id=3, pid=10375
[2025-02-20T11:26:48.844+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:28:15.025+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:28:15.026+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:28:15.026+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:28:15.027+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:28:15.027+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:28:15.027+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:28:15.028+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:28:15.654+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:28:15.659+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:28:15.690+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:28:15.691+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:28:15.699+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:28:15.699+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:28:15.776+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:28:15.847+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:28:15.865+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:28:15.891+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:28:16.361+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=4, map_index=-1)
[2025-02-20T11:28:16.366+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:22:06.760027+00:00, map_index=-1, run_start_date=2025-02-20 10:28:15.978039+00:00, run_end_date=2025-02-20 10:28:16.062682+00:00, run_duration=0.084643, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=5, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:28:15.026687+00:00, queued_by_job_id=3, pid=10554
[2025-02-20T11:28:48.691+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:28:48.692+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:28:48.692+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:28:48.692+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:28:48.693+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:28:48.693+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:28:48.694+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:28:49.280+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:28:49.286+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:28:49.313+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:28:49.314+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:28:49.321+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:28:49.322+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:28:49.386+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:28:49.450+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:28:49.468+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:28:49.482+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:28:49.864+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=2, map_index=-1)
[2025-02-20T11:28:49.867+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:26:44.438022+00:00, map_index=-1, run_start_date=2025-02-20 10:28:49.560050+00:00, run_end_date=2025-02-20 10:28:49.639934+00:00, run_duration=0.079884, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=5, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:28:48.692417+00:00, queued_by_job_id=3, pid=10624
[2025-02-20T11:30:17.423+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:30:17.424+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:30:17.424+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:30:17.424+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:30:17.425+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=5, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:30:17.425+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:30:17.425+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:30:18.007+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:30:18.012+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:30:18.041+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:30:18.041+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:30:18.049+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:30:18.049+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:30:18.131+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:30:18.207+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:30:18.226+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:30:18.240+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:30:18.617+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=5, map_index=-1)
[2025-02-20T11:30:18.620+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:22:06.760027+00:00, map_index=-1, run_start_date=2025-02-20 10:30:18.324820+00:00, run_end_date=2025-02-20 10:30:18.407251+00:00, run_duration=0.082431, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=5, max_tries=5, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:30:17.424466+00:00, queued_by_job_id=3, pid=10840
[2025-02-20T11:30:50.467+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:30:50.468+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:30:50.468+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:30:50.468+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:30:50.469+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:30:50.469+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:30:50.470+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:30:51.052+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:30:51.057+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:30:51.087+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:30:51.087+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:30:51.095+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:30:51.095+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:30:51.176+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:30:51.245+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:30:51.263+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:30:51.277+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:30:51.706+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=3, map_index=-1)
[2025-02-20T11:30:51.709+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:26:44.438022+00:00, map_index=-1, run_start_date=2025-02-20 10:30:51.361469+00:00, run_end_date=2025-02-20 10:30:51.483729+00:00, run_duration=0.12226, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=5, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:30:50.468444+00:00, queued_by_job_id=3, pid=10896
[2025-02-20T11:31:49.338+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:32:19.786+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:32:19.786+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:32:19.786+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>
[2025-02-20T11:32:19.787+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:32:19.787+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=6, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:32:19.787+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:32:19.788+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:22:06.760027+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:32:20.363+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:32:20.368+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:32:20.397+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:32:20.397+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:32:20.405+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:32:20.406+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:32:20.479+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:32:20.548+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:32:20.565+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:32:20.579+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:22:06.760027+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:32:20.952+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:22:06.760027+00:00', try_number=6, map_index=-1)
[2025-02-20T11:32:20.956+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:22:06.760027+00:00, map_index=-1, run_start_date=2025-02-20 10:32:20.667302+00:00, run_end_date=2025-02-20 10:32:20.741282+00:00, run_duration=0.07398, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=6, max_tries=5, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:32:19.787000+00:00, queued_by_job_id=3, pid=11048
[2025-02-20T11:32:21.822+0100] {dagrun.py:823} ERROR - Marking run <DagRun our_first_dag @ 2025-02-20 10:22:06.760027+00:00: manual__2025-02-20T10:22:06.760027+00:00, state:running, queued_at: 2025-02-20 10:22:06.777317+00:00. externally triggered: True> failed
Dag run  in failure state
Dag information:our_first_dag Run id: manual__2025-02-20T10:22:06.760027+00:00 external trigger: True
Failed with message: task_failure
[2025-02-20T11:32:21.823+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=our_first_dag, execution_date=2025-02-20 10:22:06.760027+00:00, run_id=manual__2025-02-20T10:22:06.760027+00:00, run_start_date=2025-02-20 10:22:08.381736+00:00, run_end_date=2025-02-20 10:32:21.822947+00:00, run_duration=613.441211, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=3450ef91f3d80ed6998b7a12898ae148
[2025-02-20T11:32:52.629+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:32:52.630+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:32:52.630+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:32:52.631+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:32:52.631+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:32:52.631+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:32:52.632+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:32:53.201+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:32:53.206+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:32:53.239+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:32:53.240+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:32:53.247+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:32:53.248+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:32:53.321+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:32:53.389+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:32:53.407+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:32:53.420+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:32:53.792+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=4, map_index=-1)
[2025-02-20T11:32:53.795+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:26:44.438022+00:00, map_index=-1, run_start_date=2025-02-20 10:32:53.504774+00:00, run_end_date=2025-02-20 10:32:53.596963+00:00, run_duration=0.092189, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=5, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:32:52.630751+00:00, queued_by_job_id=3, pid=11090
[2025-02-20T11:34:54.492+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:34:54.493+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:34:54.493+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:34:54.494+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:34:54.494+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=5, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:34:54.494+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:34:54.495+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:34:55.088+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:34:55.094+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:34:55.125+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:34:55.126+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:34:55.137+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:34:55.138+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:34:55.203+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:34:55.274+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:34:55.292+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:34:55.306+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:34:55.675+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=5, map_index=-1)
[2025-02-20T11:34:55.679+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:26:44.438022+00:00, map_index=-1, run_start_date=2025-02-20 10:34:55.386657+00:00, run_end_date=2025-02-20 10:34:55.465870+00:00, run_duration=0.079213, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=5, max_tries=5, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:34:54.493889+00:00, queued_by_job_id=3, pid=11295
[2025-02-20T11:36:50.185+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T11:36:56.084+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:36:56.085+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:36:56.085+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>
[2025-02-20T11:36:56.086+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:36:56.086+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=6, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:36:56.086+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:36:56.087+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:26:44.438022+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:36:57.235+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:36:57.240+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:36:57.264+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:36:57.265+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:36:57.274+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:36:57.275+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:36:57.350+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:36:57.424+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:36:57.444+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:36:57.462+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:26:44.438022+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:36:57.863+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:26:44.438022+00:00', try_number=6, map_index=-1)
[2025-02-20T11:36:57.871+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:26:44.438022+00:00, map_index=-1, run_start_date=2025-02-20 10:36:57.564483+00:00, run_end_date=2025-02-20 10:36:57.651734+00:00, run_duration=0.087251, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=6, max_tries=5, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:36:56.085692+00:00, queued_by_job_id=3, pid=11522
Dag run  in running state
Dag information Queued at: 2025-02-20 10:36:56.339342+00:00 hash info: 8e22049b9ba84cc870a9e45266714b92
[2025-02-20T11:36:58.897+0100] {dagrun.py:854} INFO - Marking run <DagRun our_first_dag @ 2025-02-20 10:26:44.438022+00:00: manual__2025-02-20T10:26:44.438022+00:00, state:running, queued_at: 2025-02-20 10:26:44.450725+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 10:26:46.758594+00:00 end:2025-02-20 10:36:58.898173+00:00
[2025-02-20T11:36:58.898+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=our_first_dag, execution_date=2025-02-20 10:26:44.438022+00:00, run_id=manual__2025-02-20T10:26:44.438022+00:00, run_start_date=2025-02-20 10:26:46.758594+00:00, run_end_date=2025-02-20 10:36:58.898173+00:00, run_duration=612.139579, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=8e22049b9ba84cc870a9e45266714b92
[2025-02-20T11:36:58.901+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:36:56.313471+00:00 [scheduled]>
[2025-02-20T11:36:58.901+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag has 0/16 running and queued tasks
[2025-02-20T11:36:58.902+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:36:56.313471+00:00 [scheduled]>
[2025-02-20T11:36:58.902+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag.first_task manual__2025-02-20T10:36:56.313471+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T11:36:58.902+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:36:56.313471+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T11:36:58.902+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:36:56.313471+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:36:58.903+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag', 'first_task', 'manual__2025-02-20T10:36:56.313471+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T11:36:59.500+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:36:59.505+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T11:36:59.536+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T11:36:59.537+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:36:59.545+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:36:59.546+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T11:36:59.612+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T11:36:59.662+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T11:36:59.679+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T11:36:59.692+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag.first_task manual__2025-02-20T10:36:56.313471+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T11:37:00.174+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag', task_id='first_task', run_id='manual__2025-02-20T10:36:56.313471+00:00', try_number=1, map_index=-1)
[2025-02-20T11:37:00.179+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag, task_id=first_task, run_id=manual__2025-02-20T10:36:56.313471+00:00, map_index=-1, run_start_date=2025-02-20 10:36:59.772067+00:00, run_end_date=2025-02-20 10:36:59.901912+00:00, run_duration=0.129845, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 10:36:58.902250+00:00, queued_by_job_id=3, pid=11528
[2025-02-20T11:37:01.093+0100] {dagrun.py:854} INFO - Marking run <DagRun our_first_dag @ 2025-02-20 10:36:56.313471+00:00: manual__2025-02-20T10:36:56.313471+00:00, state:running, queued_at: 2025-02-20 10:36:56.339342+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 10:36:58.892263+00:00 end:2025-02-20 10:37:01.094612+00:00
[2025-02-20T11:37:01.094+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=our_first_dag, execution_date=2025-02-20 10:36:56.313471+00:00, run_id=manual__2025-02-20T10:36:56.313471+00:00, run_start_date=2025-02-20 10:36:58.892263+00:00, run_end_date=2025-02-20 10:37:01.094612+00:00, run_duration=2.202349, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=8e22049b9ba84cc870a9e45266714b92
[2025-02-20T11:49:02.342+0100] {job.py:229} INFO - Heartbeat recovered after 479.53 seconds
[2025-02-20T11:58:03.415+0100] {job.py:229} INFO - Heartbeat recovered after 534.33 seconds
[2025-02-20T12:07:04.469+0100] {job.py:229} INFO - Heartbeat recovered after 535.79 seconds
[2025-02-20T12:16:05.807+0100] {job.py:229} INFO - Heartbeat recovered after 529.99 seconds
[2025-02-20T12:18:47.823+0100] {job.py:229} INFO - Heartbeat recovered after 155.84 seconds
[2025-02-20T12:18:50.958+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T12:23:51.120+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T12:26:53.046+0100] {manager.py:537} INFO - DAG our_first_dag is missing and will be deactivated.
[2025-02-20T12:26:53.057+0100] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-20T12:26:53.060+0100] {manager.py:553} INFO - Deleted DAG our_first_dag in serialized_dag table
Dag run  in running state
Dag information Queued at: 2025-02-20 11:27:50.954033+00:00 hash info: 7e8dfae156548cf8af7501ec06259e6e
[2025-02-20T12:27:52.446+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag_v2.first_task manual__2025-02-20T11:27:50.933435+00:00 [scheduled]>
[2025-02-20T12:27:52.447+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag_v2 has 0/16 running and queued tasks
[2025-02-20T12:27:52.447+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag_v2.first_task manual__2025-02-20T11:27:50.933435+00:00 [scheduled]>
[2025-02-20T12:27:52.448+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag_v2.first_task manual__2025-02-20T11:27:50.933435+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T12:27:52.448+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag_v2', task_id='first_task', run_id='manual__2025-02-20T11:27:50.933435+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-20T12:27:52.448+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag_v2', 'first_task', 'manual__2025-02-20T11:27:50.933435+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T12:27:52.449+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag_v2', 'first_task', 'manual__2025-02-20T11:27:50.933435+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T12:27:53.126+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T12:27:53.132+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T12:27:53.173+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T12:27:53.173+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T12:27:53.182+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T12:27:53.182+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T12:27:53.258+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T12:27:53.328+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T12:27:53.346+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T12:27:53.367+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag_v2.first_task manual__2025-02-20T11:27:50.933435+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T12:27:53.738+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag_v2', task_id='first_task', run_id='manual__2025-02-20T11:27:50.933435+00:00', try_number=1, map_index=-1)
[2025-02-20T12:27:53.741+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag_v2, task_id=first_task, run_id=manual__2025-02-20T11:27:50.933435+00:00, map_index=-1, run_start_date=2025-02-20 11:27:53.435210+00:00, run_end_date=2025-02-20 11:27:53.509434+00:00, run_duration=0.074224, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-20 11:27:52.447855+00:00, queued_by_job_id=3, pid=12963
[2025-02-20T12:27:54.677+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag_v2.second_task manual__2025-02-20T11:27:50.933435+00:00 [scheduled]>
[2025-02-20T12:27:54.678+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag_v2 has 0/16 running and queued tasks
[2025-02-20T12:27:54.678+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag_v2.second_task manual__2025-02-20T11:27:50.933435+00:00 [scheduled]>
[2025-02-20T12:27:54.678+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag_v2.second_task manual__2025-02-20T11:27:50.933435+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T12:27:54.679+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag_v2', task_id='second_task', run_id='manual__2025-02-20T11:27:50.933435+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T12:27:54.679+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag_v2', 'second_task', 'manual__2025-02-20T11:27:50.933435+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T12:27:54.679+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag_v2', 'second_task', 'manual__2025-02-20T11:27:50.933435+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag.py']
[2025-02-20T12:27:55.272+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T12:27:55.277+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag.py
[2025-02-20T12:27:55.301+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T12:27:55.301+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T12:27:55.309+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T12:27:55.310+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T12:27:55.375+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T12:27:55.427+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T12:27:55.444+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T12:27:55.457+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag_v2.second_task manual__2025-02-20T11:27:50.933435+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T12:27:56.021+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag_v2', task_id='second_task', run_id='manual__2025-02-20T11:27:50.933435+00:00', try_number=1, map_index=-1)
[2025-02-20T12:27:56.027+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag_v2, task_id=second_task, run_id=manual__2025-02-20T11:27:50.933435+00:00, map_index=-1, run_start_date=2025-02-20 11:27:55.526538+00:00, run_end_date=2025-02-20 11:27:55.603515+00:00, run_duration=0.076977, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 11:27:54.678508+00:00, queued_by_job_id=3, pid=12970
[2025-02-20T12:27:56.875+0100] {dagrun.py:854} INFO - Marking run <DagRun our_first_dag_v2 @ 2025-02-20 11:27:50.933435+00:00: manual__2025-02-20T11:27:50.933435+00:00, state:running, queued_at: 2025-02-20 11:27:50.954033+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 11:27:52.435695+00:00 end:2025-02-20 11:27:56.875958+00:00
[2025-02-20T12:27:56.876+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=our_first_dag_v2, execution_date=2025-02-20 11:27:50.933435+00:00, run_id=manual__2025-02-20T11:27:50.933435+00:00, run_start_date=2025-02-20 11:27:52.435695+00:00, run_end_date=2025-02-20 11:27:56.875958+00:00, run_duration=4.440263, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=7e8dfae156548cf8af7501ec06259e6e
[2025-02-20T12:28:51.998+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T12:29:54.389+0100] {manager.py:537} INFO - DAG our_first_dag_v2 is missing and will be deactivated.
[2025-02-20T12:29:54.394+0100] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-20T12:29:54.397+0100] {manager.py:553} INFO - Deleted DAG our_first_dag_v2 in serialized_dag table
[2025-02-20T12:33:52.020+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T12:36:33.996+0100] {cli_parser.py:80} WARNING - cannot load CLI commands from auth manager: Failed to convert value to bool. Please check "LOAD_EXAMPLES" key in "core" section. Current value: "".
[2025-02-20T12:36:33.998+0100] {cli_parser.py:81} WARNING - Authentication manager is not configured and webserver will not be able to start.
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py", line 126, in _main
    self = reduction.pickle.load(from_parent)
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/dag_processing/processor.py", line 45, in <module>
    from airflow.models.dag import DAG, DagModel
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 87, in <module>
    from airflow.datasets.manager import dataset_manager
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/datasets/manager.py", line 30, in <module>
    from airflow.models.dagbag import DagPriorityParsingRequest
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 99, in <module>
    class DagBag(LoggingMixin):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 570, in DagBag
    include_examples: bool = conf.getboolean("core", "LOAD_EXAMPLES"),
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/configuration.py", line 1172, in getboolean
    raise AirflowConfigException(
airflow.exceptions.AirflowConfigException: Failed to convert value to bool. Please check "LOAD_EXAMPLES" key in "core" section. Current value: "".
[2025-02-20T12:37:01.390+0100] {manager.py:537} INFO - DAG example_dynamic_task_mapping_with_no_taskflow_operators is missing and will be deactivated.
[2025-02-20T12:37:01.391+0100] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-20T12:37:01.392+0100] {manager.py:553} INFO - Deleted DAG example_dynamic_task_mapping_with_no_taskflow_operators in serialized_dag table
[2025-02-20T12:38:52.847+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T12:45:26.226+0100] {job.py:229} INFO - Heartbeat recovered after 372.62 seconds
[2025-02-20T12:54:27.220+0100] {job.py:229} INFO - Heartbeat recovered after 535.60 seconds
[2025-02-20T12:59:04.005+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:06:25.252+0100] {job.py:229} INFO - Heartbeat recovered after 173.74 seconds
[2025-02-20T13:06:52.632+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:11:53.727+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:16:56.644+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:21:57.282+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-20 12:22:49.482012+00:00 hash info: 0bb2b4cc54b7e824bacc5239fe1d876e
[2025-02-20T13:22:51.429+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: our_first_dag_v3.first_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>
[2025-02-20T13:22:51.430+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag_v3 has 0/16 running and queued tasks
[2025-02-20T13:22:51.430+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag_v3.first_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>
[2025-02-20T13:22:51.430+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag_v3.first_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T13:22:51.431+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag_v3', task_id='first_task', run_id='manual__2025-02-20T12:22:49.470115+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T13:22:51.431+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag_v3', 'first_task', 'manual__2025-02-20T12:22:49.470115+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag_v3.py']
[2025-02-20T13:22:51.432+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag_v3', 'first_task', 'manual__2025-02-20T12:22:49.470115+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag_v3.py']
[2025-02-20T13:22:52.037+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:22:52.042+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag_v3.py
[2025-02-20T13:22:52.071+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T13:22:52.071+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:22:52.079+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:22:52.079+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T13:22:52.148+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:22:52.222+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:22:52.240+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:22:52.254+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag_v3.first_task manual__2025-02-20T12:22:49.470115+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T13:22:52.638+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag_v3', task_id='first_task', run_id='manual__2025-02-20T12:22:49.470115+00:00', try_number=1, map_index=-1)
[2025-02-20T13:22:52.641+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag_v3, task_id=first_task, run_id=manual__2025-02-20T12:22:49.470115+00:00, map_index=-1, run_start_date=2025-02-20 12:22:52.337897+00:00, run_end_date=2025-02-20 12:22:52.412861+00:00, run_duration=0.074964, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=19, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-02-20 12:22:51.430360+00:00, queued_by_job_id=3, pid=17313
[2025-02-20T13:22:53.487+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: our_first_dag_v3.second_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>
	<TaskInstance: our_first_dag_v3.third_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>
[2025-02-20T13:22:53.487+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag_v3 has 0/16 running and queued tasks
[2025-02-20T13:22:53.487+0100] {scheduler_job_runner.py:507} INFO - DAG our_first_dag_v3 has 1/16 running and queued tasks
[2025-02-20T13:22:53.487+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: our_first_dag_v3.second_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>
	<TaskInstance: our_first_dag_v3.third_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>
[2025-02-20T13:22:53.488+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: our_first_dag_v3.second_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>, <TaskInstance: our_first_dag_v3.third_task manual__2025-02-20T12:22:49.470115+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T13:22:53.488+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag_v3', task_id='second_task', run_id='manual__2025-02-20T12:22:49.470115+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T13:22:53.488+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag_v3', 'second_task', 'manual__2025-02-20T12:22:49.470115+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag_v3.py']
[2025-02-20T13:22:53.488+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='our_first_dag_v3', task_id='third_task', run_id='manual__2025-02-20T12:22:49.470115+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T13:22:53.489+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'our_first_dag_v3', 'third_task', 'manual__2025-02-20T12:22:49.470115+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag_v3.py']
[2025-02-20T13:22:53.489+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag_v3', 'second_task', 'manual__2025-02-20T12:22:49.470115+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag_v3.py']
[2025-02-20T13:22:54.086+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:22:54.091+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag_v3.py
[2025-02-20T13:22:54.115+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T13:22:54.116+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:22:54.123+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:22:54.124+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T13:22:54.189+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:22:54.239+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:22:54.257+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:22:54.269+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag_v3.second_task manual__2025-02-20T12:22:49.470115+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T13:22:54.650+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'our_first_dag_v3', 'third_task', 'manual__2025-02-20T12:22:49.470115+00:00', '--local', '--subdir', 'DAGS_FOLDER/our_first_dag_v3.py']
[2025-02-20T13:22:55.272+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:22:55.277+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/our_first_dag_v3.py
[2025-02-20T13:22:55.301+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T13:22:55.302+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:22:55.309+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:22:55.310+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T13:22:55.374+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:22:55.424+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:22:55.441+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:22:55.453+0100] {task_command.py:467} INFO - Running <TaskInstance: our_first_dag_v3.third_task manual__2025-02-20T12:22:49.470115+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T13:22:55.839+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag_v3', task_id='second_task', run_id='manual__2025-02-20T12:22:49.470115+00:00', try_number=1, map_index=-1)
[2025-02-20T13:22:55.840+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='our_first_dag_v3', task_id='third_task', run_id='manual__2025-02-20T12:22:49.470115+00:00', try_number=1, map_index=-1)
[2025-02-20T13:22:55.843+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag_v3, task_id=second_task, run_id=manual__2025-02-20T12:22:49.470115+00:00, map_index=-1, run_start_date=2025-02-20 12:22:54.353303+00:00, run_end_date=2025-02-20 12:22:54.425045+00:00, run_duration=0.071742, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 12:22:53.487704+00:00, queued_by_job_id=3, pid=17321
[2025-02-20T13:22:55.843+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=our_first_dag_v3, task_id=third_task, run_id=manual__2025-02-20T12:22:49.470115+00:00, map_index=-1, run_start_date=2025-02-20 12:22:55.538344+00:00, run_end_date=2025-02-20 12:22:55.615113+00:00, run_duration=0.076769, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-20 12:22:53.487704+00:00, queued_by_job_id=3, pid=17325
[2025-02-20T13:22:56.678+0100] {dagrun.py:854} INFO - Marking run <DagRun our_first_dag_v3 @ 2025-02-20 12:22:49.470115+00:00: manual__2025-02-20T12:22:49.470115+00:00, state:running, queued_at: 2025-02-20 12:22:49.482012+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 12:22:51.417662+00:00 end:2025-02-20 12:22:56.678518+00:00
[2025-02-20T13:22:56.678+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=our_first_dag_v3, execution_date=2025-02-20 12:22:49.470115+00:00, run_id=manual__2025-02-20T12:22:49.470115+00:00, run_start_date=2025-02-20 12:22:51.417662+00:00, run_end_date=2025-02-20 12:22:56.678518+00:00, run_duration=5.260856, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=0bb2b4cc54b7e824bacc5239fe1d876e
[2025-02-20T13:26:58.216+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:31:59.144+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:36:59.427+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:41:59.504+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-20 12:45:47.695066+00:00 hash info: f3b0a9ec300636fb76f5feb57b982032
[2025-02-20T13:45:49.164+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [scheduled]>
[2025-02-20T13:45:49.165+0100] {scheduler_job_runner.py:507} INFO - DAG python_operator_dag_v01 has 0/16 running and queued tasks
[2025-02-20T13:45:49.165+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [scheduled]>
[2025-02-20T13:45:49.166+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T13:45:49.166+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag_v01', task_id='greet', run_id='manual__2025-02-20T12:45:47.672325+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T13:45:49.166+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag_v01', 'greet', 'manual__2025-02-20T12:45:47.672325+00:00', '--local', '--subdir', 'DAGS_FOLDER/python_operator_dag.py']
[2025-02-20T13:45:49.167+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag_v01', 'greet', 'manual__2025-02-20T12:45:47.672325+00:00', '--local', '--subdir', 'DAGS_FOLDER/python_operator_dag.py']
[2025-02-20T13:45:49.801+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:45:49.806+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/python_operator_dag.py
[2025-02-20T13:45:49.837+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T13:45:49.837+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:45:49.845+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:45:49.846+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T13:45:49.916+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:45:50.024+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:45:50.047+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:45:50.063+0100] {task_command.py:467} INFO - Running <TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T13:45:50.463+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag_v01', task_id='greet', run_id='manual__2025-02-20T12:45:47.672325+00:00', try_number=1, map_index=-1)
[2025-02-20T13:45:50.467+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=python_operator_dag_v01, task_id=greet, run_id=manual__2025-02-20T12:45:47.672325+00:00, map_index=-1, run_start_date=2025-02-20 12:45:50.135499+00:00, run_end_date=2025-02-20 12:45:50.210792+00:00, run_duration=0.075293, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-20 12:45:49.165594+00:00, queued_by_job_id=3, pid=20120
[2025-02-20T13:47:00.524+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:47:50.687+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [scheduled]>
[2025-02-20T13:47:50.688+0100] {scheduler_job_runner.py:507} INFO - DAG python_operator_dag_v01 has 0/16 running and queued tasks
[2025-02-20T13:47:50.688+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [scheduled]>
[2025-02-20T13:47:50.689+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T13:47:50.689+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag_v01', task_id='greet', run_id='manual__2025-02-20T12:45:47.672325+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T13:47:50.689+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag_v01', 'greet', 'manual__2025-02-20T12:45:47.672325+00:00', '--local', '--subdir', 'DAGS_FOLDER/python_operator_dag.py']
[2025-02-20T13:47:50.690+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag_v01', 'greet', 'manual__2025-02-20T12:45:47.672325+00:00', '--local', '--subdir', 'DAGS_FOLDER/python_operator_dag.py']
[2025-02-20T13:47:51.426+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:47:51.432+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/python_operator_dag.py
[2025-02-20T13:47:51.463+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T13:47:51.463+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:47:51.471+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:47:51.472+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T13:47:51.549+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:47:51.673+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:47:51.693+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:47:51.712+0100] {task_command.py:467} INFO - Running <TaskInstance: python_operator_dag_v01.greet manual__2025-02-20T12:45:47.672325+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T13:47:52.200+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag_v01', task_id='greet', run_id='manual__2025-02-20T12:45:47.672325+00:00', try_number=2, map_index=-1)
[2025-02-20T13:47:52.204+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=python_operator_dag_v01, task_id=greet, run_id=manual__2025-02-20T12:45:47.672325+00:00, map_index=-1, run_start_date=2025-02-20 12:47:51.801378+00:00, run_end_date=2025-02-20 12:47:51.882376+00:00, run_duration=0.080998, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=5, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-20 12:47:50.688773+00:00, queued_by_job_id=3, pid=20541
[2025-02-20T13:47:53.064+0100] {dagrun.py:854} INFO - Marking run <DagRun python_operator_dag_v01 @ 2025-02-20 12:45:47.672325+00:00: manual__2025-02-20T12:45:47.672325+00:00, state:running, queued_at: 2025-02-20 12:45:47.695066+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 12:45:49.155503+00:00 end:2025-02-20 12:47:53.065060+00:00
[2025-02-20T13:47:53.065+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=python_operator_dag_v01, execution_date=2025-02-20 12:45:47.672325+00:00, run_id=manual__2025-02-20T12:45:47.672325+00:00, run_start_date=2025-02-20 12:45:49.155503+00:00, run_end_date=2025-02-20 12:47:53.065060+00:00, run_duration=123.909557, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=63a521dc95b2fb566257e4ca39b73a60
Dag run  in running state
Dag information Queued at: 2025-02-20 12:47:54.550413+00:00 hash info: fef710e88783acc749ad58cf696b8c49
[2025-02-20T13:47:55.793+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: python_operator_dag_v02.greet manual__2025-02-20T12:47:54.542119+00:00 [scheduled]>
[2025-02-20T13:47:55.793+0100] {scheduler_job_runner.py:507} INFO - DAG python_operator_dag_v02 has 0/16 running and queued tasks
[2025-02-20T13:47:55.794+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: python_operator_dag_v02.greet manual__2025-02-20T12:47:54.542119+00:00 [scheduled]>
[2025-02-20T13:47:55.794+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: python_operator_dag_v02.greet manual__2025-02-20T12:47:54.542119+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T13:47:55.795+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='python_operator_dag_v02', task_id='greet', run_id='manual__2025-02-20T12:47:54.542119+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T13:47:55.795+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'python_operator_dag_v02', 'greet', 'manual__2025-02-20T12:47:54.542119+00:00', '--local', '--subdir', 'DAGS_FOLDER/python_operator_dag_v2.py']
[2025-02-20T13:47:55.796+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'python_operator_dag_v02', 'greet', 'manual__2025-02-20T12:47:54.542119+00:00', '--local', '--subdir', 'DAGS_FOLDER/python_operator_dag_v2.py']
[2025-02-20T13:47:56.529+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:47:56.548+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/python_operator_dag_v2.py
[2025-02-20T13:47:56.590+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T13:47:56.591+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:47:56.600+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:47:56.600+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T13:47:56.673+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T13:47:56.733+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T13:47:56.761+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T13:47:56.779+0100] {task_command.py:467} INFO - Running <TaskInstance: python_operator_dag_v02.greet manual__2025-02-20T12:47:54.542119+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T13:47:57.339+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='python_operator_dag_v02', task_id='greet', run_id='manual__2025-02-20T12:47:54.542119+00:00', try_number=1, map_index=-1)
[2025-02-20T13:47:57.348+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=python_operator_dag_v02, task_id=greet, run_id=manual__2025-02-20T12:47:54.542119+00:00, map_index=-1, run_start_date=2025-02-20 12:47:56.871921+00:00, run_end_date=2025-02-20 12:47:57.018817+00:00, run_duration=0.146896, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-20 12:47:55.794335+00:00, queued_by_job_id=3, pid=20553
[2025-02-20T13:47:58.328+0100] {dagrun.py:854} INFO - Marking run <DagRun python_operator_dag_v02 @ 2025-02-20 12:47:54.542119+00:00: manual__2025-02-20T12:47:54.542119+00:00, state:running, queued_at: 2025-02-20 12:47:54.550413+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 12:47:55.784617+00:00 end:2025-02-20 12:47:58.328776+00:00
[2025-02-20T13:47:58.328+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=python_operator_dag_v02, execution_date=2025-02-20 12:47:54.542119+00:00, run_id=manual__2025-02-20T12:47:54.542119+00:00, run_start_date=2025-02-20 12:47:55.784617+00:00, run_end_date=2025-02-20 12:47:58.328776+00:00, run_duration=2.544159, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=fef710e88783acc749ad58cf696b8c49
[2025-02-20T13:52:01.459+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T13:57:02.468+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:02:03.593+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:07:04.594+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:12:05.523+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
Dag run  in running state
Dag information Queued at: 2025-02-20 13:13:19.129957+00:00 hash info: 049d3840685b14baf181282c758808f8
[2025-02-20T14:13:20.843+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: xcom_dag_v1.get_name manual__2025-02-20T13:13:19.117889+00:00 [scheduled]>
[2025-02-20T14:13:20.843+0100] {scheduler_job_runner.py:507} INFO - DAG xcom_dag_v1 has 0/16 running and queued tasks
[2025-02-20T14:13:20.844+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag_v1.get_name manual__2025-02-20T13:13:19.117889+00:00 [scheduled]>
[2025-02-20T14:13:20.844+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: xcom_dag_v1.get_name manual__2025-02-20T13:13:19.117889+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T14:13:20.845+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='xcom_dag_v1', task_id='get_name', run_id='manual__2025-02-20T13:13:19.117889+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-20T14:13:20.845+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag_v1', 'get_name', 'manual__2025-02-20T13:13:19.117889+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v1.py']
[2025-02-20T14:13:20.845+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_dag_v1', 'get_name', 'manual__2025-02-20T13:13:19.117889+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v1.py']
[2025-02-20T14:13:21.438+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:21.443+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/xcom_dag_v1.py
[2025-02-20T14:13:21.473+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T14:13:21.473+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:21.481+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:21.481+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T14:13:21.556+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:21.633+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:21.651+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:21.665+0100] {task_command.py:467} INFO - Running <TaskInstance: xcom_dag_v1.get_name manual__2025-02-20T13:13:19.117889+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T14:13:22.067+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_dag_v1', task_id='get_name', run_id='manual__2025-02-20T13:13:19.117889+00:00', try_number=1, map_index=-1)
[2025-02-20T14:13:22.070+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=xcom_dag_v1, task_id=get_name, run_id=manual__2025-02-20T13:13:19.117889+00:00, map_index=-1, run_start_date=2025-02-20 13:13:21.748529+00:00, run_end_date=2025-02-20 13:13:21.812122+00:00, run_duration=0.063593, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=25, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-02-20 13:13:20.844275+00:00, queued_by_job_id=3, pid=23450
[2025-02-20T14:13:23.033+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: xcom_dag_v1.greet manual__2025-02-20T13:13:19.117889+00:00 [scheduled]>
[2025-02-20T14:13:23.034+0100] {scheduler_job_runner.py:507} INFO - DAG xcom_dag_v1 has 0/16 running and queued tasks
[2025-02-20T14:13:23.034+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag_v1.greet manual__2025-02-20T13:13:19.117889+00:00 [scheduled]>
[2025-02-20T14:13:23.034+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: xcom_dag_v1.greet manual__2025-02-20T13:13:19.117889+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T14:13:23.035+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='xcom_dag_v1', task_id='greet', run_id='manual__2025-02-20T13:13:19.117889+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T14:13:23.035+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag_v1', 'greet', 'manual__2025-02-20T13:13:19.117889+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v1.py']
[2025-02-20T14:13:23.036+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_dag_v1', 'greet', 'manual__2025-02-20T13:13:19.117889+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v1.py']
[2025-02-20T14:13:23.647+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:23.652+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/xcom_dag_v1.py
[2025-02-20T14:13:23.675+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T14:13:23.675+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:23.683+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:23.684+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T14:13:23.749+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:23.800+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:23.820+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:23.833+0100] {task_command.py:467} INFO - Running <TaskInstance: xcom_dag_v1.greet manual__2025-02-20T13:13:19.117889+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T14:13:24.167+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_dag_v1', task_id='greet', run_id='manual__2025-02-20T13:13:19.117889+00:00', try_number=1, map_index=-1)
[2025-02-20T14:13:24.170+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=xcom_dag_v1, task_id=greet, run_id=manual__2025-02-20T13:13:19.117889+00:00, map_index=-1, run_start_date=2025-02-20 13:13:23.911771+00:00, run_end_date=2025-02-20 13:13:23.974029+00:00, run_duration=0.062258, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-20 13:13:23.034426+00:00, queued_by_job_id=3, pid=23455
[2025-02-20T14:13:25.021+0100] {dagrun.py:854} INFO - Marking run <DagRun xcom_dag_v1 @ 2025-02-20 13:13:19.117889+00:00: manual__2025-02-20T13:13:19.117889+00:00, state:running, queued_at: 2025-02-20 13:13:19.129957+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 13:13:20.833697+00:00 end:2025-02-20 13:13:25.021883+00:00
[2025-02-20T14:13:25.021+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=xcom_dag_v1, execution_date=2025-02-20 13:13:19.117889+00:00, run_id=manual__2025-02-20T13:13:19.117889+00:00, run_start_date=2025-02-20 13:13:20.833697+00:00, run_end_date=2025-02-20 13:13:25.021883+00:00, run_duration=4.188186, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=049d3840685b14baf181282c758808f8
Dag run  in running state
Dag information Queued at: 2025-02-20 13:13:51.124886+00:00 hash info: 5d4a5a00c8bedb599b3e9b22004cae29
[2025-02-20T14:13:52.612+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: xcom_dag_v2.get_name manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>
	<TaskInstance: xcom_dag_v2.get_age manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>
[2025-02-20T14:13:52.613+0100] {scheduler_job_runner.py:507} INFO - DAG xcom_dag_v2 has 0/16 running and queued tasks
[2025-02-20T14:13:52.613+0100] {scheduler_job_runner.py:507} INFO - DAG xcom_dag_v2 has 1/16 running and queued tasks
[2025-02-20T14:13:52.613+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag_v2.get_name manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>
	<TaskInstance: xcom_dag_v2.get_age manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>
[2025-02-20T14:13:52.616+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: xcom_dag_v2.get_name manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>, <TaskInstance: xcom_dag_v2.get_age manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T14:13:52.617+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='xcom_dag_v2', task_id='get_name', run_id='manual__2025-02-20T13:13:51.111223+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-20T14:13:52.618+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag_v2', 'get_name', 'manual__2025-02-20T13:13:51.111223+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v2.py']
[2025-02-20T14:13:52.619+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='xcom_dag_v2', task_id='get_age', run_id='manual__2025-02-20T13:13:51.111223+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-20T14:13:52.619+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag_v2', 'get_age', 'manual__2025-02-20T13:13:51.111223+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v2.py']
[2025-02-20T14:13:52.622+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_dag_v2', 'get_name', 'manual__2025-02-20T13:13:51.111223+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v2.py']
[2025-02-20T14:13:53.465+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:53.471+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/xcom_dag_v2.py
[2025-02-20T14:13:53.507+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T14:13:53.508+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:53.516+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:53.517+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T14:13:53.594+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:53.664+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:53.682+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:53.697+0100] {task_command.py:467} INFO - Running <TaskInstance: xcom_dag_v2.get_name manual__2025-02-20T13:13:51.111223+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T14:13:54.085+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_dag_v2', 'get_age', 'manual__2025-02-20T13:13:51.111223+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v2.py']
[2025-02-20T14:13:54.697+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:54.705+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/xcom_dag_v2.py
[2025-02-20T14:13:54.730+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T14:13:54.730+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:54.738+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:54.738+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T14:13:54.802+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:54.865+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:54.895+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:54.912+0100] {task_command.py:467} INFO - Running <TaskInstance: xcom_dag_v2.get_age manual__2025-02-20T13:13:51.111223+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T14:13:55.256+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_dag_v2', task_id='get_name', run_id='manual__2025-02-20T13:13:51.111223+00:00', try_number=1, map_index=-1)
[2025-02-20T14:13:55.257+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_dag_v2', task_id='get_age', run_id='manual__2025-02-20T13:13:51.111223+00:00', try_number=1, map_index=-1)
[2025-02-20T14:13:55.261+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=xcom_dag_v2, task_id=get_age, run_id=manual__2025-02-20T13:13:51.111223+00:00, map_index=-1, run_start_date=2025-02-20 13:13:54.992172+00:00, run_end_date=2025-02-20 13:13:55.056319+00:00, run_duration=0.064147, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=28, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-02-20 13:13:52.613969+00:00, queued_by_job_id=3, pid=23505
[2025-02-20T14:13:55.261+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=xcom_dag_v2, task_id=get_name, run_id=manual__2025-02-20T13:13:51.111223+00:00, map_index=-1, run_start_date=2025-02-20 13:13:53.784330+00:00, run_end_date=2025-02-20 13:13:53.866885+00:00, run_duration=0.082555, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=27, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-02-20 13:13:52.613969+00:00, queued_by_job_id=3, pid=23502
[2025-02-20T14:13:56.158+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: xcom_dag_v2.greet manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>
[2025-02-20T14:13:56.158+0100] {scheduler_job_runner.py:507} INFO - DAG xcom_dag_v2 has 0/16 running and queued tasks
[2025-02-20T14:13:56.159+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag_v2.greet manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>
[2025-02-20T14:13:56.159+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: xcom_dag_v2.greet manual__2025-02-20T13:13:51.111223+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T14:13:56.159+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='xcom_dag_v2', task_id='greet', run_id='manual__2025-02-20T13:13:51.111223+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T14:13:56.160+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag_v2', 'greet', 'manual__2025-02-20T13:13:51.111223+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v2.py']
[2025-02-20T14:13:56.160+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'xcom_dag_v2', 'greet', 'manual__2025-02-20T13:13:51.111223+00:00', '--local', '--subdir', 'DAGS_FOLDER/xcom_dag_v2.py']
[2025-02-20T14:13:56.735+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:56.740+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/xcom_dag_v2.py
[2025-02-20T14:13:56.763+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T14:13:56.764+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:56.771+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:56.772+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T14:13:56.836+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T14:13:56.889+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T14:13:56.906+0100] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
[2025-02-20T14:13:56.919+0100] {task_command.py:467} INFO - Running <TaskInstance: xcom_dag_v2.greet manual__2025-02-20T13:13:51.111223+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T14:13:57.252+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='xcom_dag_v2', task_id='greet', run_id='manual__2025-02-20T13:13:51.111223+00:00', try_number=1, map_index=-1)
[2025-02-20T14:13:57.255+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=xcom_dag_v2, task_id=greet, run_id=manual__2025-02-20T13:13:51.111223+00:00, map_index=-1, run_start_date=2025-02-20 13:13:57.004159+00:00, run_end_date=2025-02-20 13:13:57.064351+00:00, run_duration=0.060192, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-02-20 13:13:56.159323+00:00, queued_by_job_id=3, pid=23512
[2025-02-20T14:13:58.181+0100] {dagrun.py:854} INFO - Marking run <DagRun xcom_dag_v2 @ 2025-02-20 13:13:51.111223+00:00: manual__2025-02-20T13:13:51.111223+00:00, state:running, queued_at: 2025-02-20 13:13:51.124886+00:00. externally triggered: True> successful
Dag run in success state
Dag run start:2025-02-20 13:13:52.601905+00:00 end:2025-02-20 13:13:58.181850+00:00
[2025-02-20T14:13:58.181+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=xcom_dag_v2, execution_date=2025-02-20 13:13:51.111223+00:00, run_id=manual__2025-02-20T13:13:51.111223+00:00, run_start_date=2025-02-20 13:13:52.601905+00:00, run_end_date=2025-02-20 13:13:58.181850+00:00, run_duration=5.579945, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=5d4a5a00c8bedb599b3e9b22004cae29
[2025-02-20T14:17:05.529+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:18:43.826+0100] {job.py:229} INFO - Heartbeat recovered after 98.30 seconds
[2025-02-20T14:23:36.516+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:28:37.421+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:33:54.127+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:38:55.101+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:42:01.891+0100] {manager.py:537} INFO - DAG dag_with_taskflow_api is missing and will be deactivated.
[2025-02-20T14:42:01.900+0100] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-02-20T14:42:01.904+0100] {manager.py:553} INFO - Deleted DAG dag_with_taskflow_api in serialized_dag table
[2025-02-20T14:43:56.000+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:48:56.808+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:53:57.826+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T14:58:59.478+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:04:00.411+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:09:01.985+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:14:03.025+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:19:04.422+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:24:05.268+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:29:06.152+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:34:06.229+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:39:06.582+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T15:42:55.933+0100] {job.py:229} INFO - Heartbeat recovered after 51.88 seconds
[2025-02-20T15:50:53.269+0100] {job.py:229} INFO - Heartbeat recovered after 430.76 seconds
[2025-02-20T16:00:02.014+0100] {job.py:229} INFO - Heartbeat recovered after 505.19 seconds
[2025-02-20T16:00:21.371+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:05:22.340+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:10:22.899+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:15:23.234+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:20:23.539+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:21:18.916+0100] {job.py:229} INFO - Heartbeat recovered after 32.16 seconds
[2025-02-20T16:25:24.542+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:30:25.217+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:35:25.735+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:40:29.873+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:43:08.209+0100] {job.py:229} INFO - Heartbeat recovered after 33.73 seconds
[2025-02-20T16:44:56.148+0100] {dag.py:4180} INFO - Setting next_dagrun for dag_with_taskflow_api to 2025-02-20 00:00:00+00:00, run_after=2025-02-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:44:56.133485+00:00 hash info: 8ca43449031315bf4ce428c56bc81462
[2025-02-20T16:44:56.166+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_with_taskflow_api.get_user_details scheduled__2025-02-19T00:00:00+00:00 [scheduled]>
[2025-02-20T16:44:56.167+0100] {scheduler_job_runner.py:507} INFO - DAG dag_with_taskflow_api has 0/16 running and queued tasks
[2025-02-20T16:44:56.167+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_with_taskflow_api.get_user_details scheduled__2025-02-19T00:00:00+00:00 [scheduled]>
[2025-02-20T16:44:56.168+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_with_taskflow_api.get_user_details scheduled__2025-02-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:44:56.169+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_with_taskflow_api', task_id='get_user_details', run_id='scheduled__2025-02-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-20T16:44:56.169+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_with_taskflow_api', 'get_user_details', 'scheduled__2025-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow_api.py']
[2025-02-20T16:44:56.171+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dag_with_taskflow_api', 'get_user_details', 'scheduled__2025-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow_api.py']
[2025-02-20T16:44:57.069+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/dag_with_taskflow_api.py
[2025-02-20T16:44:57.117+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:44:57.117+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:44:57.125+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:44:57.125+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:44:57.227+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:44:57.294+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:44:57.337+0100] {task_command.py:467} INFO - Running <TaskInstance: dag_with_taskflow_api.get_user_details scheduled__2025-02-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:44:57.756+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_with_taskflow_api', task_id='get_user_details', run_id='scheduled__2025-02-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:44:57.760+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_with_taskflow_api, task_id=get_user_details, run_id=scheduled__2025-02-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:44:57.421658+00:00, run_end_date=2025-02-20 15:44:57.495032+00:00, run_duration=0.073374, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=30, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-02-20 15:44:56.167967+00:00, queued_by_job_id=3, pid=35993
[2025-02-20T16:44:58.669+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dag_with_taskflow_api.greet scheduled__2025-02-19T00:00:00+00:00 [scheduled]>
[2025-02-20T16:44:58.670+0100] {scheduler_job_runner.py:507} INFO - DAG dag_with_taskflow_api has 0/16 running and queued tasks
[2025-02-20T16:44:58.670+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_with_taskflow_api.greet scheduled__2025-02-19T00:00:00+00:00 [scheduled]>
[2025-02-20T16:44:58.670+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dag_with_taskflow_api.greet scheduled__2025-02-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:44:58.671+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dag_with_taskflow_api', task_id='greet', run_id='scheduled__2025-02-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-20T16:44:58.671+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_with_taskflow_api', 'greet', 'scheduled__2025-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow_api.py']
[2025-02-20T16:44:58.672+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dag_with_taskflow_api', 'greet', 'scheduled__2025-02-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag_with_taskflow_api.py']
[2025-02-20T16:44:59.415+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/dag_with_taskflow_api.py
[2025-02-20T16:44:59.440+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:44:59.441+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:44:59.448+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:44:59.448+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:44:59.553+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:44:59.603+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:44:59.634+0100] {task_command.py:467} INFO - Running <TaskInstance: dag_with_taskflow_api.greet scheduled__2025-02-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:45:00.053+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_with_taskflow_api', task_id='greet', run_id='scheduled__2025-02-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:45:00.056+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dag_with_taskflow_api, task_id=greet, run_id=scheduled__2025-02-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:44:59.722200+00:00, run_end_date=2025-02-20 15:44:59.799959+00:00, run_duration=0.077759, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=5, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-02-20 15:44:58.670371+00:00, queued_by_job_id=3, pid=35998
[2025-02-20T16:45:01.004+0100] {dagrun.py:854} INFO - Marking run <DagRun dag_with_taskflow_api @ 2025-02-19 00:00:00+00:00: scheduled__2025-02-19T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:44:56.133485+00:00. externally triggered: False> successful
Dag run in success state
Dag run start:2025-02-20 15:44:56.153946+00:00 end:2025-02-20 15:45:01.004460+00:00
[2025-02-20T16:45:01.004+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_with_taskflow_api, execution_date=2025-02-19 00:00:00+00:00, run_id=scheduled__2025-02-19T00:00:00+00:00, run_start_date=2025-02-20 15:44:56.153946+00:00, run_end_date=2025-02-20 15:45:01.004460+00:00, run_duration=4.850514, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-02-19 00:00:00+00:00, data_interval_end=2025-02-20 00:00:00+00:00, dag_hash=8ca43449031315bf4ce428c56bc81462
[2025-02-20T16:45:01.007+0100] {dag.py:4180} INFO - Setting next_dagrun for dag_with_taskflow_api to 2025-02-20 00:00:00+00:00, run_after=2025-02-21 00:00:00+00:00
[2025-02-20T16:45:30.564+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:45:44.043+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-25 00:00:00+00:00, run_after=2024-03-26 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:45:44.039019+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:45:44.058+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:44.058+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:45:44.058+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:44.059+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:45:44.059+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-24T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:45:44.059+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:44.060+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:44.854+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:45:44.888+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:45:44.888+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:44.895+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:44.896+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:45:45.005+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:45.081+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:45.131+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:45:45.816+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-24T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:45:45.827+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:45:45.231856+00:00, run_end_date=2025-02-20 15:45:45.309228+00:00, run_duration=0.077372, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:45:44.058805+00:00, queued_by_job_id=3, pid=36063
[2025-02-20T16:45:47.875+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-26 00:00:00+00:00, run_after=2024-03-27 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:45:47.870539+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:45:47.893+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:47.893+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:45:47.893+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:47.894+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:45:47.894+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-25T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:45:47.894+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:47.895+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:48.879+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:45:48.907+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:45:48.907+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:48.916+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:48.916+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:45:49.016+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:49.077+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:49.117+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:45:49.574+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-25T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:45:49.579+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:45:49.209944+00:00, run_end_date=2025-02-20 15:45:49.270401+00:00, run_duration=0.060457, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:45:47.893910+00:00, queued_by_job_id=3, pid=36071
[2025-02-20T16:45:51.891+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-27 00:00:00+00:00, run_after=2024-03-28 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:45:51.887088+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:45:51.908+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:51.909+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:45:51.909+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:51.909+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:45:51.910+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-26T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:45:51.910+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:51.910+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:52.799+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:45:52.826+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:45:52.827+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:52.838+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:52.838+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:45:52.945+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:53.002+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:53.034+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:45:54.140+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-26T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:45:54.149+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:45:53.134466+00:00, run_end_date=2025-02-20 15:45:53.211807+00:00, run_duration=0.077341, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:45:51.909483+00:00, queued_by_job_id=3, pid=36082
[2025-02-20T16:45:55.648+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-28 00:00:00+00:00, run_after=2024-03-29 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:45:55.641910+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:45:55.749+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:55.750+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:45:55.750+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:55.751+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:45:55.751+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-27T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:45:55.751+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:55.752+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:56.542+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:45:56.568+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:45:56.568+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:56.576+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:56.576+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:45:56.672+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:56.721+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:56.755+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:45:57.192+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-27T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:45:57.197+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:45:56.842859+00:00, run_end_date=2025-02-20 15:45:56.924684+00:00, run_duration=0.081825, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:45:55.750564+00:00, queued_by_job_id=3, pid=36089
[2025-02-20T16:45:58.092+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-29 00:00:00+00:00, run_after=2024-03-30 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:45:58.089598+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:45:58.112+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:58.112+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:45:58.112+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [scheduled]>
[2025-02-20T16:45:58.113+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:45:58.113+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-28T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:45:58.113+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:58.116+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:45:58.859+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:45:58.885+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:45:58.885+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:58.893+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:58.893+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:45:58.994+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:45:59.044+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:45:59.076+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:45:59.738+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-28T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:45:59.741+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:45:59.161374+00:00, run_end_date=2025-02-20 15:45:59.390629+00:00, run_duration=0.229255, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:45:58.112573+00:00, queued_by_job_id=3, pid=36096
[2025-02-20T16:46:00.935+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-30 00:00:00+00:00, run_after=2024-03-31 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:00.931478+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:00.958+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:00.958+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:00.959+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:00.959+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:00.960+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-29T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:00.960+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:00.961+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:02.018+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:02.072+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:02.073+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:02.096+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:02.097+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:02.215+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:02.290+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:02.343+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:02.766+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-29T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:02.774+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:02.435290+00:00, run_end_date=2025-02-20 15:46:02.509621+00:00, run_duration=0.074331, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:00.959296+00:00, queued_by_job_id=3, pid=36110
[2025-02-20T16:46:03.638+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-31 00:00:00+00:00, run_after=2024-04-01 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:03.635788+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:03.659+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:03.660+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:03.660+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:03.660+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:03.661+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-30T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:03.661+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:03.661+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:04.453+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:04.482+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:04.483+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:04.493+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:04.493+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:04.590+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:04.639+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:04.670+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:05.099+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-30T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:05.103+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:04.753670+00:00, run_end_date=2025-02-20 15:46:04.826856+00:00, run_duration=0.073186, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:03.660375+00:00, queued_by_job_id=3, pid=36117
[2025-02-20T16:46:06.033+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-01 00:00:00+00:00, run_after=2024-04-02 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:06.031437+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:06.057+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:06.057+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:06.057+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:06.058+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:06.058+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-31T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:06.058+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:06.059+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:07.011+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:07.036+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:07.036+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:07.043+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:07.044+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:07.142+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:07.191+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:07.223+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:08.035+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-31T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:08.044+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:07.308074+00:00, run_end_date=2025-02-20 15:46:07.693906+00:00, run_duration=0.385832, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:06.057918+00:00, queued_by_job_id=3, pid=36127
[2025-02-20T16:46:10.147+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-02 00:00:00+00:00, run_after=2024-04-03 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:10.143838+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:10.175+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:10.175+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:10.176+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:10.176+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:10.177+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-01T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:10.177+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:10.177+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:11.037+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:11.065+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:11.066+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:11.073+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:11.073+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:11.169+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:11.219+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:11.251+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:11.918+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-01T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:11.925+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:11.348286+00:00, run_end_date=2025-02-20 15:46:11.423163+00:00, run_duration=0.074877, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:10.176323+00:00, queued_by_job_id=3, pid=36134
[2025-02-20T16:46:13.193+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-03 00:00:00+00:00, run_after=2024-04-04 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:13.188986+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:13.222+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:13.222+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:13.223+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:13.223+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:13.223+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-02T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:13.224+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:13.224+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:14.192+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:14.218+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:14.218+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:14.226+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:14.226+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:14.346+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:14.403+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:14.463+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:14.878+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-02T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:14.881+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:14.557274+00:00, run_end_date=2025-02-20 15:46:14.638755+00:00, run_duration=0.081481, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:13.223242+00:00, queued_by_job_id=3, pid=36141
[2025-02-20T16:46:15.718+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-04 00:00:00+00:00, run_after=2024-04-05 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:15.716270+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:15.744+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:15.744+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:15.745+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:15.745+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:15.745+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-03T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:15.746+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:15.746+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:16.797+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:16.827+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:16.828+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:16.835+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:16.836+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:16.935+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:16.988+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:17.021+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:17.497+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-03T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:17.504+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:17.097990+00:00, run_end_date=2025-02-20 15:46:17.187555+00:00, run_duration=0.089565, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:15.745311+00:00, queued_by_job_id=3, pid=36151
[2025-02-20T16:46:18.534+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-05 00:00:00+00:00, run_after=2024-04-06 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:18.531130+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:18.568+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:18.568+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:18.568+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:18.569+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:18.569+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-04T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:18.569+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:18.570+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:19.378+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:19.413+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:19.413+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:19.423+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:19.424+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:19.537+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:19.587+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:19.617+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:19.963+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-04T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:19.967+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:19.689783+00:00, run_end_date=2025-02-20 15:46:19.751600+00:00, run_duration=0.061817, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:18.568897+00:00, queued_by_job_id=3, pid=36156
[2025-02-20T16:46:20.971+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-06 00:00:00+00:00, run_after=2024-04-07 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:20.969545+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:21.000+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:21.000+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:21.000+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:21.001+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:21.001+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-05T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:21.001+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:21.002+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:21.852+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:21.877+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:21.877+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:21.885+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:21.885+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:21.983+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:22.034+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:22.067+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:22.481+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-05T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:22.487+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:22.148478+00:00, run_end_date=2025-02-20 15:46:22.215872+00:00, run_duration=0.067394, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=44, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:21.001095+00:00, queued_by_job_id=3, pid=36166
[2025-02-20T16:46:24.042+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-07 00:00:00+00:00, run_after=2024-04-08 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:24.033359+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:24.093+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:24.093+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:24.094+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:24.094+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:24.094+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-06T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:24.095+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:24.095+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:25.584+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:25.652+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:25.653+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:25.669+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:25.670+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:25.819+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:25.890+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:25.930+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:26.386+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-06T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:26.394+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:26.047182+00:00, run_end_date=2025-02-20 15:46:26.132879+00:00, run_duration=0.085697, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:24.094219+00:00, queued_by_job_id=3, pid=36171
[2025-02-20T16:46:27.266+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-08 00:00:00+00:00, run_after=2024-04-09 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:27.264023+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:27.301+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:27.301+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:27.302+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:27.302+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:27.303+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-07T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:27.303+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:27.303+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:28.063+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:28.090+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:28.090+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:28.098+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:28.098+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:28.196+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:28.253+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:28.284+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:28.690+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-07T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:28.694+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:28.367938+00:00, run_end_date=2025-02-20 15:46:28.441429+00:00, run_duration=0.073491, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=46, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:27.302251+00:00, queued_by_job_id=3, pid=36178
[2025-02-20T16:46:29.708+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 15:46:29.706890+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:46:29.744+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:29.744+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:46:29.744+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [scheduled]>
[2025-02-20T16:46:29.745+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:46:29.745+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-08T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:46:29.745+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:29.746+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:46:30.876+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:46:30.904+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:46:30.905+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:30.913+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:30.914+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:46:31.042+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:46:31.142+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:46:31.179+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:46:33.242+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-08T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:46:33.254+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:46:31.282839+00:00, run_end_date=2025-02-20 15:46:32.368878+00:00, run_duration=1.086039, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:46:29.744905+00:00, queued_by_job_id=3, pid=36185
[2025-02-20T16:50:33.502+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:50:47.285+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [scheduled]>
[2025-02-20T16:50:47.286+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:50:47.286+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [scheduled]>
[2025-02-20T16:50:47.287+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:50:47.289+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-24T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:50:47.290+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:50:47.291+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:50:48.566+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:50:48.610+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:50:48.610+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:50:48.618+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:50:48.618+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:50:48.715+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:50:48.780+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:50:48.815+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:50:49.410+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-24T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:50:49.419+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:50:48.907051+00:00, run_end_date=2025-02-20 15:50:49.044617+00:00, run_duration=0.137566, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:50:47.287062+00:00, queued_by_job_id=3, pid=36746
[2025-02-20T16:50:50.373+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [scheduled]>
[2025-02-20T16:50:50.373+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:50:50.373+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [scheduled]>
[2025-02-20T16:50:50.374+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:50:50.374+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-25T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:50:50.374+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:50:50.375+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:50:51.139+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:50:51.164+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:50:51.164+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:50:51.172+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:50:51.172+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:50:51.272+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:50:51.341+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:50:51.390+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-25T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:50:51.827+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-25T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:50:51.832+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-25T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:50:51.489267+00:00, run_end_date=2025-02-20 15:50:51.590401+00:00, run_duration=0.101134, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:50:50.373821+00:00, queued_by_job_id=3, pid=36753
[2025-02-20T16:50:53.050+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-24 00:00:00+00:00: scheduled__2024-03-24T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:45:44.039019+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-24T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:50:53.051+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-24 00:00:00+00:00, run_id=scheduled__2024-03-24T00:00:00+00:00, run_start_date=2025-02-20 15:45:44.048366+00:00, run_end_date=2025-02-20 15:50:53.051010+00:00, run_duration=309.002644, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-24 00:00:00+00:00, data_interval_end=2024-03-25 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:50:53.055+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-25 00:00:00+00:00, run_after=2024-03-26 00:00:00+00:00
[2025-02-20T16:50:56.595+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-26 00:00:00+00:00, run_after=2024-03-27 00:00:00+00:00
[2025-02-20T16:50:56.626+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-25 00:00:00+00:00: scheduled__2024-03-25T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:45:47.870539+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-25T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:50:56.627+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-25 00:00:00+00:00, run_id=scheduled__2024-03-25T00:00:00+00:00, run_start_date=2025-02-20 15:45:47.879356+00:00, run_end_date=2025-02-20 15:50:56.626948+00:00, run_duration=308.747592, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-25 00:00:00+00:00, data_interval_end=2024-03-26 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:50:56.628+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-26 00:00:00+00:00, run_after=2024-03-27 00:00:00+00:00
[2025-02-20T16:50:56.632+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [scheduled]>
[2025-02-20T16:50:56.633+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:50:56.633+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [scheduled]>
[2025-02-20T16:50:56.633+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:50:56.634+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-26T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:50:56.634+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:50:56.634+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-26T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:50:58.096+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:50:58.148+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:50:58.148+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:50:58.157+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:50:58.158+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:50:58.311+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:50:58.395+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:50:58.439+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-26T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:50:59.063+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-26T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:50:59.070+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-26T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:50:58.565745+00:00, run_end_date=2025-02-20 15:50:58.806444+00:00, run_duration=0.240699, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:50:56.633432+00:00, queued_by_job_id=3, pid=36764
[2025-02-20T16:51:00.715+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-27 00:00:00+00:00, run_after=2024-03-28 00:00:00+00:00
[2025-02-20T16:51:00.758+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:00.758+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:00.759+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T16:51:00.759+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:00.760+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:00.760+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-27T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:00.761+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:00.761+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-28T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:00.761+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:00.762+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-27T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:01.763+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:01.793+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:01.793+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:01.802+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:01.802+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:01.902+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:01.956+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:01.995+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-27T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:02.556+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-28T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:03.353+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:03.423+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:03.423+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:03.434+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:03.434+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:03.604+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:03.667+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:03.702+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-28T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:04.176+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-27T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:04.177+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-28T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:04.186+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-27T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:02.132466+00:00, run_end_date=2025-02-20 15:51:02.254804+00:00, run_duration=0.122338, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:00.759629+00:00, queued_by_job_id=3, pid=36775
[2025-02-20T16:51:04.187+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-28T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:03.783900+00:00, run_end_date=2025-02-20 15:51:03.910024+00:00, run_duration=0.126124, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:00.759629+00:00, queued_by_job_id=3, pid=36780
[2025-02-20T16:51:05.114+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-28 00:00:00+00:00, run_after=2024-03-29 00:00:00+00:00
[2025-02-20T16:51:05.143+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-26 00:00:00+00:00: scheduled__2024-03-26T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:45:51.887088+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-26T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:05.144+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-26 00:00:00+00:00, run_id=scheduled__2024-03-26T00:00:00+00:00, run_start_date=2025-02-20 15:45:51.895826+00:00, run_end_date=2025-02-20 15:51:05.144262+00:00, run_duration=313.248436, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-26 00:00:00+00:00, data_interval_end=2024-03-27 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:05.146+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-27 00:00:00+00:00, run_after=2024-03-28 00:00:00+00:00
[2025-02-20T16:51:05.150+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:05.151+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:05.151+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T16:51:05.151+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:05.152+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:05.152+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-29T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:05.152+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:05.152+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-30T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:05.152+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:05.153+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-29T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:06.307+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:06.344+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:06.345+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:06.353+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:06.353+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:06.455+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:06.505+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:06.539+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-29T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:06.973+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-30T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:07.838+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:07.870+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:07.870+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:07.881+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:07.883+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:08.046+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:08.126+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:08.200+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-30T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:08.925+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-29T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:08.926+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-30T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:08.933+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-29T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:06.636427+00:00, run_end_date=2025-02-20 15:51:06.724344+00:00, run_duration=0.087917, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:05.151554+00:00, queued_by_job_id=3, pid=36790
[2025-02-20T16:51:08.934+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-30T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:08.311555+00:00, run_end_date=2025-02-20 15:51:08.595042+00:00, run_duration=0.283487, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=54, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:05.151554+00:00, queued_by_job_id=3, pid=36795
[2025-02-20T16:51:09.876+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-28 00:00:00+00:00, run_after=2024-03-29 00:00:00+00:00
[2025-02-20T16:51:09.901+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-28 00:00:00+00:00: scheduled__2024-03-28T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:45:58.089598+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-28T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:09.902+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-28 00:00:00+00:00, run_id=scheduled__2024-03-28T00:00:00+00:00, run_start_date=2025-02-20 15:45:58.096435+00:00, run_end_date=2025-02-20 15:51:09.902050+00:00, run_duration=311.805615, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-28 00:00:00+00:00, data_interval_end=2024-03-29 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:09.904+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-29 00:00:00+00:00, run_after=2024-03-30 00:00:00+00:00
[2025-02-20T16:51:09.905+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-27 00:00:00+00:00: scheduled__2024-03-27T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:45:55.641910+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-27T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:09.905+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-27 00:00:00+00:00, run_id=scheduled__2024-03-27T00:00:00+00:00, run_start_date=2025-02-20 15:45:55.666447+00:00, run_end_date=2025-02-20 15:51:09.905256+00:00, run_duration=314.238809, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-27 00:00:00+00:00, data_interval_end=2024-03-28 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:09.906+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-28 00:00:00+00:00, run_after=2024-03-29 00:00:00+00:00
[2025-02-20T16:51:09.910+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:09.910+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:09.910+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:09.911+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:09.911+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-31T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:09.911+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:09.912+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-03-31T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:10.697+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:10.723+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:10.723+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:10.731+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:10.732+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:10.835+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:10.887+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:10.919+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-03-31T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:11.386+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-03-31T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:11.391+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-03-31T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:10.994798+00:00, run_end_date=2025-02-20 15:51:11.077777+00:00, run_duration=0.082979, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:09.911016+00:00, queued_by_job_id=3, pid=36812
[2025-02-20T16:51:12.272+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-29 00:00:00+00:00, run_after=2024-03-30 00:00:00+00:00
[2025-02-20T16:51:12.300+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-30 00:00:00+00:00: scheduled__2024-03-30T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:03.635788+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-30T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:12.301+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-30 00:00:00+00:00, run_id=scheduled__2024-03-30T00:00:00+00:00, run_start_date=2025-02-20 15:46:03.641934+00:00, run_end_date=2025-02-20 15:51:12.301001+00:00, run_duration=308.659067, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-30 00:00:00+00:00, data_interval_end=2024-03-31 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:12.303+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-31 00:00:00+00:00, run_after=2024-04-01 00:00:00+00:00
[2025-02-20T16:51:12.304+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-29 00:00:00+00:00: scheduled__2024-03-29T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:00.931478+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-29T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:12.304+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-29 00:00:00+00:00, run_id=scheduled__2024-03-29T00:00:00+00:00, run_start_date=2025-02-20 15:46:00.939609+00:00, run_end_date=2025-02-20 15:51:12.304828+00:00, run_duration=311.365219, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-29 00:00:00+00:00, data_interval_end=2024-03-30 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:12.306+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-03-30 00:00:00+00:00, run_after=2024-03-31 00:00:00+00:00
[2025-02-20T16:51:12.310+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:12.311+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:12.311+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:12.312+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:12.312+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-01T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:12.312+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:12.313+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:13.047+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:13.072+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:13.072+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:13.079+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:13.080+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:13.183+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:13.239+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:13.275+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-01T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:13.754+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-01T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:13.758+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-01T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:13.357320+00:00, run_end_date=2025-02-20 15:51:13.439911+00:00, run_duration=0.082591, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:12.311518+00:00, queued_by_job_id=3, pid=36819
[2025-02-20T16:51:14.790+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-10 00:00:00+00:00, run_after=2024-04-11 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:14.786186+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:14.812+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-03-31 00:00:00+00:00: scheduled__2024-03-31T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:06.031437+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-03-31T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:14.813+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-03-31 00:00:00+00:00, run_id=scheduled__2024-03-31T00:00:00+00:00, run_start_date=2025-02-20 15:46:06.037663+00:00, run_end_date=2025-02-20 15:51:14.813037+00:00, run_duration=308.775374, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-03-31 00:00:00+00:00, data_interval_end=2024-04-01 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:14.814+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-01 00:00:00+00:00, run_after=2024-04-02 00:00:00+00:00
[2025-02-20T16:51:14.818+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:14.818+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:14.818+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T16:51:14.819+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:14.819+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:14.820+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-02T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:14.820+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:14.820+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-09T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:14.820+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:14.821+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:15.570+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:15.595+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:15.595+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:15.602+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:15.603+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:15.702+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:15.752+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:15.782+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-02T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:16.192+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:16.919+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:16.945+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:16.945+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:16.953+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:16.953+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:17.048+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:17.103+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:17.135+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:17.569+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-02T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:17.569+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-09T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:17.573+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-02T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:15.867104+00:00, run_end_date=2025-02-20 15:51:15.970210+00:00, run_duration=0.103106, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=57, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:14.819226+00:00, queued_by_job_id=3, pid=36824
[2025-02-20T16:51:17.573+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:17.226116+00:00, run_end_date=2025-02-20 15:51:17.311356+00:00, run_duration=0.08524, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=58, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:14.819226+00:00, queued_by_job_id=3, pid=36827
[2025-02-20T16:51:18.409+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-02 00:00:00+00:00, run_after=2024-04-03 00:00:00+00:00
[2025-02-20T16:51:18.428+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-01 00:00:00+00:00: scheduled__2024-04-01T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:10.143838+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-01T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:18.428+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-01 00:00:00+00:00, run_id=scheduled__2024-04-01T00:00:00+00:00, run_start_date=2025-02-20 15:46:10.152038+00:00, run_end_date=2025-02-20 15:51:18.428389+00:00, run_duration=308.276351, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-01 00:00:00+00:00, data_interval_end=2024-04-02 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:18.430+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-02 00:00:00+00:00, run_after=2024-04-03 00:00:00+00:00
[2025-02-20T16:51:18.433+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:18.433+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:18.433+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:18.434+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:18.434+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-03T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:18.434+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:18.435+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:19.146+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:19.170+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:19.170+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:19.177+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:19.178+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:19.274+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:19.363+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:19.394+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-03T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:19.804+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-03T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:19.808+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-03T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:19.481078+00:00, run_end_date=2025-02-20 15:51:19.564885+00:00, run_duration=0.083807, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=59, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:18.434060+00:00, queued_by_job_id=3, pid=36832
[2025-02-20T16:51:20.677+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-03 00:00:00+00:00, run_after=2024-04-04 00:00:00+00:00
[2025-02-20T16:51:20.695+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-02 00:00:00+00:00: scheduled__2024-04-02T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:13.188986+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-02T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:20.695+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-02 00:00:00+00:00, run_id=scheduled__2024-04-02T00:00:00+00:00, run_start_date=2025-02-20 15:46:13.198332+00:00, run_end_date=2025-02-20 15:51:20.695691+00:00, run_duration=307.497359, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-02 00:00:00+00:00, data_interval_end=2024-04-03 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:20.697+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-03 00:00:00+00:00, run_after=2024-04-04 00:00:00+00:00
[2025-02-20T16:51:20.700+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:20.700+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:20.700+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:20.701+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:20.701+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-04T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:20.701+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:20.702+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:21.470+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:21.494+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:21.494+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:21.502+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:21.502+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:21.597+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:21.647+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:21.677+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-04T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:22.090+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-04T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:22.093+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-04T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:21.763112+00:00, run_end_date=2025-02-20 15:51:21.859716+00:00, run_duration=0.096604, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=60, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:20.701127+00:00, queued_by_job_id=3, pid=36837
[2025-02-20T16:51:22.921+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-04 00:00:00+00:00, run_after=2024-04-05 00:00:00+00:00
[2025-02-20T16:51:22.938+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-03 00:00:00+00:00: scheduled__2024-04-03T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:15.716270+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-03T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:22.938+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-03 00:00:00+00:00, run_id=scheduled__2024-04-03T00:00:00+00:00, run_start_date=2025-02-20 15:46:15.721604+00:00, run_end_date=2025-02-20 15:51:22.938703+00:00, run_duration=307.217099, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-03 00:00:00+00:00, data_interval_end=2024-04-04 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:22.940+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-04 00:00:00+00:00, run_after=2024-04-05 00:00:00+00:00
[2025-02-20T16:51:22.944+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:22.944+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:22.944+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:22.945+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:22.945+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-05T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:22.945+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:22.946+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:23.711+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:23.735+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:23.735+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:23.743+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:23.743+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:23.838+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:23.887+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:23.917+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-05T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:24.376+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-05T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:24.380+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-05T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:24.003583+00:00, run_end_date=2025-02-20 15:51:24.101020+00:00, run_duration=0.097437, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=61, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:22.945092+00:00, queued_by_job_id=3, pid=36842
[2025-02-20T16:51:25.216+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-05 00:00:00+00:00, run_after=2024-04-06 00:00:00+00:00
[2025-02-20T16:51:25.230+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-04 00:00:00+00:00: scheduled__2024-04-04T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:18.531130+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-04T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:25.230+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-04 00:00:00+00:00, run_id=scheduled__2024-04-04T00:00:00+00:00, run_start_date=2025-02-20 15:46:18.538155+00:00, run_end_date=2025-02-20 15:51:25.230646+00:00, run_duration=306.692491, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-04 00:00:00+00:00, data_interval_end=2024-04-05 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:25.232+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-05 00:00:00+00:00, run_after=2024-04-06 00:00:00+00:00
[2025-02-20T16:51:27.105+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-06 00:00:00+00:00, run_after=2024-04-07 00:00:00+00:00
[2025-02-20T16:51:27.118+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-05 00:00:00+00:00: scheduled__2024-04-05T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:20.969545+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-05T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:27.118+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-05 00:00:00+00:00, run_id=scheduled__2024-04-05T00:00:00+00:00, run_start_date=2025-02-20 15:46:20.974561+00:00, run_end_date=2025-02-20 15:51:27.118492+00:00, run_duration=306.143931, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-05 00:00:00+00:00, data_interval_end=2024-04-06 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:27.119+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-06 00:00:00+00:00, run_after=2024-04-07 00:00:00+00:00
[2025-02-20T16:51:27.123+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:27.123+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:27.123+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:27.124+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:27.124+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-06T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:27.124+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:27.125+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:27.869+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:27.894+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:27.894+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:27.901+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:27.901+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:27.995+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:28.046+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:28.077+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-06T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:28.502+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-06T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:28.506+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-06T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:28.164396+00:00, run_end_date=2025-02-20 15:51:28.251441+00:00, run_duration=0.087045, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=62, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:27.123925+00:00, queued_by_job_id=3, pid=36849
[2025-02-20T16:51:29.425+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-07 00:00:00+00:00, run_after=2024-04-08 00:00:00+00:00
[2025-02-20T16:51:29.441+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:29.441+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:29.441+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:29.442+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:29.442+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-07T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:29.442+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:29.443+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:30.151+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:30.175+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:30.175+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:30.182+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:30.183+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:30.279+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:30.330+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:30.360+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-07T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:30.771+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-07T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:30.774+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-07T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:30.446794+00:00, run_end_date=2025-02-20 15:51:30.532524+00:00, run_duration=0.08573, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=63, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:29.442073+00:00, queued_by_job_id=3, pid=36855
[2025-02-20T16:51:31.596+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-08 00:00:00+00:00, run_after=2024-04-09 00:00:00+00:00
[2025-02-20T16:51:31.607+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-06 00:00:00+00:00: scheduled__2024-04-06T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:24.033359+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-06T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:31.607+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-06 00:00:00+00:00, run_id=scheduled__2024-04-06T00:00:00+00:00, run_start_date=2025-02-20 15:46:24.049106+00:00, run_end_date=2025-02-20 15:51:31.607752+00:00, run_duration=307.558646, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-06 00:00:00+00:00, data_interval_end=2024-04-07 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:31.609+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-07 00:00:00+00:00, run_after=2024-04-08 00:00:00+00:00
[2025-02-20T16:51:33.499+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-08 00:00:00+00:00, run_after=2024-04-09 00:00:00+00:00
[2025-02-20T16:51:33.509+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-07 00:00:00+00:00: scheduled__2024-04-07T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:27.264023+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-07T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:33.509+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-07 00:00:00+00:00, run_id=scheduled__2024-04-07T00:00:00+00:00, run_start_date=2025-02-20 15:46:27.271546+00:00, run_end_date=2025-02-20 15:51:33.509177+00:00, run_duration=306.237631, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-07 00:00:00+00:00, data_interval_end=2024-04-08 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:33.510+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-08 00:00:00+00:00, run_after=2024-04-09 00:00:00+00:00
[2025-02-20T16:51:33.514+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:33.514+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:33.514+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:33.515+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:33.515+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-08T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:33.515+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:33.516+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:34.227+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:34.252+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:34.252+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:34.259+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:34.260+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:34.361+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:34.412+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:34.446+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-08T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:34.852+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-08T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:51:34.856+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-08T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:34.525275+00:00, run_end_date=2025-02-20 15:51:34.611396+00:00, run_duration=0.086121, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=64, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:33.514607+00:00, queued_by_job_id=3, pid=36862
[2025-02-20T16:51:35.747+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-09 00:00:00+00:00, run_after=2024-04-10 00:00:00+00:00
[2025-02-20T16:51:36.597+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-10 00:00:00+00:00, run_after=2024-04-11 00:00:00+00:00
[2025-02-20T16:51:36.604+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-08 00:00:00+00:00: scheduled__2024-04-08T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:46:29.706890+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-08T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:51:36.604+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-08 00:00:00+00:00, run_id=scheduled__2024-04-08T00:00:00+00:00, run_start_date=2025-02-20 15:46:29.711896+00:00, run_end_date=2025-02-20 15:51:36.604753+00:00, run_duration=306.892857, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-08 00:00:00+00:00, data_interval_end=2024-04-09 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:36.606+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-09 00:00:00+00:00, run_after=2024-04-10 00:00:00+00:00
[2025-02-20T16:51:38.618+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-10 00:00:00+00:00, run_after=2024-04-11 00:00:00+00:00
[2025-02-20T16:51:40.217+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-11 00:00:00+00:00, run_after=2024-04-12 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:40.215198+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:40.230+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:40.230+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:40.230+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:40.231+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:40.231+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-10T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:40.231+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:40.232+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:40.951+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:40.975+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:40.976+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:40.983+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:40.983+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:41.080+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:41.129+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:41.161+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:41.584+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-10T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:41.587+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:41.246138+00:00, run_end_date=2025-02-20 15:51:41.340351+00:00, run_duration=0.094213, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=65, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:40.230850+00:00, queued_by_job_id=3, pid=36873
[2025-02-20T16:51:42.400+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-12 00:00:00+00:00, run_after=2024-04-13 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:42.398713+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:42.415+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:42.415+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:42.415+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:42.416+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:42.416+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-11T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:42.416+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:42.417+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:43.130+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:43.154+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:43.155+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:43.162+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:43.163+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:43.258+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:43.308+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:43.345+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:43.758+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-11T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:43.762+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:43.430182+00:00, run_end_date=2025-02-20 15:51:43.513652+00:00, run_duration=0.08347, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=66, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:42.415953+00:00, queued_by_job_id=3, pid=36878
[2025-02-20T16:51:44.555+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-13 00:00:00+00:00, run_after=2024-04-14 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:44.553435+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:44.571+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:44.571+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:44.572+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:44.572+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:44.572+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:44.572+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:44.573+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:45.273+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:45.298+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:45.298+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:45.305+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:45.306+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:45.434+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:45.484+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:45.514+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:45.914+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-12T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:45.917+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:45.593432+00:00, run_end_date=2025-02-20 15:51:45.674366+00:00, run_duration=0.080934, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=67, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:44.572198+00:00, queued_by_job_id=3, pid=36883
[2025-02-20T16:51:46.749+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-14 00:00:00+00:00, run_after=2024-04-15 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:46.746917+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:46.766+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:46.767+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:46.767+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:46.767+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:46.767+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-13T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:46.768+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:46.768+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:47.504+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:47.528+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:47.528+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:47.535+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:47.536+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:47.630+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:47.680+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:47.710+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:48.117+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-13T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:48.120+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:47.794746+00:00, run_end_date=2025-02-20 15:51:47.875106+00:00, run_duration=0.08036, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=68, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:46.767417+00:00, queued_by_job_id=3, pid=36888
[2025-02-20T16:51:48.930+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-15 00:00:00+00:00, run_after=2024-04-16 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:48.928661+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:48.951+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:48.951+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:48.951+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:48.952+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:48.952+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-14T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:48.952+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:48.953+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:49.670+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:49.695+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:49.695+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:49.702+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:49.703+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:49.797+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:49.846+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:49.877+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:50.283+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-14T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:50.287+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:49.958357+00:00, run_end_date=2025-02-20 15:51:50.043474+00:00, run_duration=0.085117, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=69, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:48.951784+00:00, queued_by_job_id=3, pid=36893
[2025-02-20T16:51:51.153+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-16 00:00:00+00:00, run_after=2024-04-17 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:51.151141+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:51.173+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:51.173+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:51.173+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:51.174+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:51.174+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-15T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:51.174+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:51.175+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:51.928+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:51.952+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:51.952+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:51.959+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:51.960+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:52.054+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:52.103+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:52.133+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:52.544+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-15T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:52.547+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:52.218671+00:00, run_end_date=2025-02-20 15:51:52.300145+00:00, run_duration=0.081474, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=70, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:51.173979+00:00, queued_by_job_id=3, pid=36898
[2025-02-20T16:51:53.422+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-17 00:00:00+00:00, run_after=2024-04-18 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:53.420177+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:53.446+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:53.447+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:53.447+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:53.447+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:53.448+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-16T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:53.448+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:53.448+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:54.161+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:54.185+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:54.185+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:54.192+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:54.193+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:54.287+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:54.337+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:54.368+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:54.777+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-16T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:54.781+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:54.454552+00:00, run_end_date=2025-02-20 15:51:54.540982+00:00, run_duration=0.08643, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=71, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:53.447411+00:00, queued_by_job_id=3, pid=36903
[2025-02-20T16:51:55.614+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-18 00:00:00+00:00, run_after=2024-04-19 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:55.612448+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:55.638+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:55.638+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:55.638+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:55.639+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:55.639+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-17T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:55.639+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:55.640+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:56.364+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:56.388+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:56.388+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:56.395+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:56.396+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:56.490+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:56.539+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:56.570+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:56.977+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-17T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:56.980+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:56.652620+00:00, run_end_date=2025-02-20 15:51:56.740326+00:00, run_duration=0.087706, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=72, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:55.638944+00:00, queued_by_job_id=3, pid=36908
[2025-02-20T16:51:57.863+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-19 00:00:00+00:00, run_after=2024-04-20 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:51:57.861308+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:51:57.888+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:57.888+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:51:57.888+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [scheduled]>
[2025-02-20T16:51:57.888+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:51:57.889+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-18T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:51:57.889+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:57.889+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:51:58.626+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:51:58.650+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:51:58.650+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:58.658+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:58.658+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:51:58.753+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:51:58.802+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:51:58.832+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:51:59.247+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-18T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:51:59.250+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:51:58.916507+00:00, run_end_date=2025-02-20 15:51:59.006117+00:00, run_duration=0.08961, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=73, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:51:57.888594+00:00, queued_by_job_id=3, pid=36913
[2025-02-20T16:52:00.117+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-20 00:00:00+00:00, run_after=2024-04-21 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:52:00.115549+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:52:00.145+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:00.145+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:52:00.146+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:00.146+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:52:00.146+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-19T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:52:00.146+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:00.147+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:00.887+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:52:00.911+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:52:00.911+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:00.919+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:00.919+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:52:01.014+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:01.064+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:01.096+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:52:01.511+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-19T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:52:01.514+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-19T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:52:01.180934+00:00, run_end_date=2025-02-20 15:52:01.272556+00:00, run_duration=0.091622, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=74, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:52:00.146259+00:00, queued_by_job_id=3, pid=36919
[2025-02-20T16:52:02.360+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-21 00:00:00+00:00, run_after=2024-04-22 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:52:02.358344+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:52:02.392+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:02.392+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:52:02.392+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:02.393+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:52:02.393+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-20T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:52:02.393+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:02.394+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:03.103+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:52:03.127+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:52:03.128+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:03.135+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:03.135+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:52:03.234+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:03.285+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:03.316+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:52:03.725+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-20T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:52:03.729+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-20T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:52:03.402292+00:00, run_end_date=2025-02-20 15:52:03.498890+00:00, run_duration=0.096598, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=75, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:52:02.393021+00:00, queued_by_job_id=3, pid=36924
[2025-02-20T16:52:04.788+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-22 00:00:00+00:00, run_after=2024-04-23 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:52:04.785954+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:52:04.821+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-21T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:04.821+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:52:04.821+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-21T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:04.822+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-21T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:52:04.822+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-21T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:52:04.822+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:04.823+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:05.560+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:52:05.586+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:52:05.586+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:05.593+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:05.594+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:52:05.688+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:05.793+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:05.838+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-21T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:52:06.260+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-21T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:52:06.265+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-21T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:52:05.935973+00:00, run_end_date=2025-02-20 15:52:06.027016+00:00, run_duration=0.091043, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=76, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:52:04.821641+00:00, queued_by_job_id=3, pid=36929
[2025-02-20T16:52:07.362+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-23 00:00:00+00:00, run_after=2024-04-24 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:52:07.355126+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:52:07.395+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-22T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:07.396+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:52:07.396+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-22T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:07.397+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-22T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:52:07.397+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-22T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:52:07.397+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:07.398+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-22T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:08.125+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:52:08.149+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:52:08.149+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:08.157+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:08.157+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:52:08.252+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:08.302+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:08.337+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-22T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:52:08.738+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-22T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:52:08.741+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-22T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:52:08.422658+00:00, run_end_date=2025-02-20 15:52:08.526542+00:00, run_duration=0.103884, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=77, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:52:07.396575+00:00, queued_by_job_id=3, pid=36934
[2025-02-20T16:52:09.587+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-24 00:00:00+00:00, run_after=2024-04-25 00:00:00+00:00
Dag run  in running state
Dag information Queued at: 2025-02-20 15:52:09.585394+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:52:09.687+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-23T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:09.688+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:52:09.688+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-23T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:09.688+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-23T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:52:09.688+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-23T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:52:09.689+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:09.689+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-23T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:10.399+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:52:10.423+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:52:10.423+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:10.430+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:10.431+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:52:10.524+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:10.573+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:10.608+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-23T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:52:11.003+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-23T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:52:11.007+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-23T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:52:10.685357+00:00, run_end_date=2025-02-20 15:52:10.778095+00:00, run_duration=0.092738, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=78, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:52:09.688407+00:00, queued_by_job_id=3, pid=36939
[2025-02-20T16:52:12.049+0100] {scheduler_job_runner.py:1526} INFO - DAG etl_example is at (or above) max_active_runs (16 of 16), not creating any more runs
Dag run  in running state
Dag information Queued at: 2025-02-20 15:52:12.046588+00:00 hash info: 01075acecffac600a262bfe0b3565714
[2025-02-20T16:52:12.083+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-24T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:12.083+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:52:12.083+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-24T00:00:00+00:00 [scheduled]>
[2025-02-20T16:52:12.084+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-24T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:52:12.084+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-24T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:52:12.084+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:12.085+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-24T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:52:12.800+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:52:12.824+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:52:12.824+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:12.832+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:12.832+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:52:12.928+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:52:12.978+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:52:13.008+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-24T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:52:13.460+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-24T00:00:00+00:00', try_number=1, map_index=-1)
[2025-02-20T16:52:13.464+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-24T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:52:13.093593+00:00, run_end_date=2025-02-20 15:52:13.191936+00:00, run_duration=0.098343, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=79, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:52:12.083551+00:00, queued_by_job_id=3, pid=36944
[2025-02-20T16:55:34.413+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-20T16:56:17.416+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:17.417+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:17.417+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:17.418+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:17.418+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-09T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:17.418+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:17.419+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:18.149+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:18.180+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:18.180+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:18.188+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:18.188+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:18.293+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:18.358+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:18.392+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-09T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:18.830+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-09T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:18.833+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-09T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:18.482550+00:00, run_end_date=2025-02-20 15:56:18.586526+00:00, run_duration=0.103976, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=80, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:17.417952+00:00, queued_by_job_id=3, pid=37280
[2025-02-20T16:56:21.442+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-09 00:00:00+00:00: scheduled__2024-04-09T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:14.786186+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-09T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:21.442+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-09 00:00:00+00:00, run_id=scheduled__2024-04-09T00:00:00+00:00, run_start_date=2025-02-20 15:51:14.794065+00:00, run_end_date=2025-02-20 15:56:21.442679+00:00, run_duration=306.648614, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-09 00:00:00+00:00, data_interval_end=2024-04-10 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:21.445+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-10 00:00:00+00:00, run_after=2024-04-11 00:00:00+00:00
[2025-02-20T16:56:23.402+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-11 00:00:00+00:00, run_after=2024-04-12 00:00:00+00:00
[2025-02-20T16:56:25.362+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-12 00:00:00+00:00, run_after=2024-04-13 00:00:00+00:00
[2025-02-20T16:56:27.277+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-13 00:00:00+00:00, run_after=2024-04-14 00:00:00+00:00
[2025-02-20T16:56:29.263+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-14 00:00:00+00:00, run_after=2024-04-15 00:00:00+00:00
[2025-02-20T16:56:31.161+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-15 00:00:00+00:00, run_after=2024-04-16 00:00:00+00:00
[2025-02-20T16:56:32.333+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-16 00:00:00+00:00, run_after=2024-04-17 00:00:00+00:00
[2025-02-20T16:56:34.226+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-17 00:00:00+00:00, run_after=2024-04-18 00:00:00+00:00
[2025-02-20T16:56:36.432+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-18 00:00:00+00:00, run_after=2024-04-19 00:00:00+00:00
[2025-02-20T16:56:38.005+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-19 00:00:00+00:00, run_after=2024-04-20 00:00:00+00:00
[2025-02-20T16:56:39.959+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-20 00:00:00+00:00, run_after=2024-04-21 00:00:00+00:00
[2025-02-20T16:56:41.243+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-21 00:00:00+00:00, run_after=2024-04-22 00:00:00+00:00
[2025-02-20T16:56:43.176+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-22 00:00:00+00:00, run_after=2024-04-23 00:00:00+00:00
[2025-02-20T16:56:43.209+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:43.209+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:43.209+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:43.210+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:43.210+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-10T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:43.210+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:43.214+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:44.093+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:44.123+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:44.123+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:44.132+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:44.132+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:44.237+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:44.346+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:44.380+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-10T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:44.861+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-10T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:44.867+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-10T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:44.470695+00:00, run_end_date=2025-02-20 15:56:44.596042+00:00, run_duration=0.125347, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=81, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:43.209849+00:00, queued_by_job_id=3, pid=37313
[2025-02-20T16:56:45.754+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-23 00:00:00+00:00, run_after=2024-04-24 00:00:00+00:00
[2025-02-20T16:56:45.810+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:45.810+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:45.810+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T16:56:45.810+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:45.811+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:45.811+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-11T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:45.811+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:45.811+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-12T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:45.811+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:45.812+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:46.544+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:46.568+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:46.568+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:46.575+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:46.576+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:46.672+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:46.721+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:46.751+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-11T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:47.179+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:47.922+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:47.949+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:47.949+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:47.956+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:47.957+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:48.053+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:48.103+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:48.135+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-12T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:48.570+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-11T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:48.570+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-12T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:48.574+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-11T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:46.832907+00:00, run_end_date=2025-02-20 15:56:46.916505+00:00, run_duration=0.083598, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=82, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:45.811041+00:00, queued_by_job_id=3, pid=37318
[2025-02-20T16:56:48.574+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-12T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:48.225481+00:00, run_end_date=2025-02-20 15:56:48.310336+00:00, run_duration=0.084855, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=83, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:45.811041+00:00, queued_by_job_id=3, pid=37321
[2025-02-20T16:56:49.440+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-24 00:00:00+00:00, run_after=2024-04-25 00:00:00+00:00
[2025-02-20T16:56:49.475+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-10 00:00:00+00:00: scheduled__2024-04-10T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:40.215198+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-10T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:49.475+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-10 00:00:00+00:00, run_id=scheduled__2024-04-10T00:00:00+00:00, run_start_date=2025-02-20 15:51:40.220352+00:00, run_end_date=2025-02-20 15:56:49.475502+00:00, run_duration=309.25515, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-10 00:00:00+00:00, data_interval_end=2024-04-11 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:49.478+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-11 00:00:00+00:00, run_after=2024-04-12 00:00:00+00:00
[2025-02-20T16:56:49.482+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:49.483+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:49.483+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:49.483+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:49.484+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-13T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:49.484+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:49.485+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:50.250+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:50.274+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:50.274+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:50.281+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:50.282+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:50.388+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:50.441+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:50.471+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-13T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:50.885+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-13T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:50.889+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-13T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:50.559937+00:00, run_end_date=2025-02-20 15:56:50.653005+00:00, run_duration=0.093068, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=84, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:49.483406+00:00, queued_by_job_id=3, pid=37326
[2025-02-20T16:56:51.799+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-12 00:00:00+00:00, run_after=2024-04-13 00:00:00+00:00
[2025-02-20T16:56:51.825+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-12 00:00:00+00:00: scheduled__2024-04-12T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:44.553435+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-12T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:51.825+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-12 00:00:00+00:00, run_id=scheduled__2024-04-12T00:00:00+00:00, run_start_date=2025-02-20 15:51:44.559006+00:00, run_end_date=2025-02-20 15:56:51.825293+00:00, run_duration=307.266287, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-12 00:00:00+00:00, data_interval_end=2024-04-13 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:51.827+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-13 00:00:00+00:00, run_after=2024-04-14 00:00:00+00:00
[2025-02-20T16:56:51.828+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-11 00:00:00+00:00: scheduled__2024-04-11T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:42.398713+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-11T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:51.828+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-11 00:00:00+00:00, run_id=scheduled__2024-04-11T00:00:00+00:00, run_start_date=2025-02-20 15:51:42.404354+00:00, run_end_date=2025-02-20 15:56:51.828284+00:00, run_duration=309.42393, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-11 00:00:00+00:00, data_interval_end=2024-04-12 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:51.829+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-12 00:00:00+00:00, run_after=2024-04-13 00:00:00+00:00
[2025-02-20T16:56:51.833+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:51.833+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:51.833+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:51.833+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:51.834+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-14T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:51.834+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:51.834+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:52.569+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:52.601+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:52.601+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:52.608+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:52.609+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:52.704+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:52.753+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:52.784+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-14T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:53.217+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-14T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:53.220+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-14T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:52.872209+00:00, run_end_date=2025-02-20 15:56:52.976115+00:00, run_duration=0.103906, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=85, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:51.833534+00:00, queued_by_job_id=3, pid=37331
[2025-02-20T16:56:54.112+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-13 00:00:00+00:00, run_after=2024-04-14 00:00:00+00:00
[2025-02-20T16:56:54.136+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-13 00:00:00+00:00: scheduled__2024-04-13T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:46.746917+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-13T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:54.137+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-13 00:00:00+00:00, run_id=scheduled__2024-04-13T00:00:00+00:00, run_start_date=2025-02-20 15:51:46.752500+00:00, run_end_date=2025-02-20 15:56:54.137080+00:00, run_duration=307.38458, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-13 00:00:00+00:00, data_interval_end=2024-04-14 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:54.139+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-14 00:00:00+00:00, run_after=2024-04-15 00:00:00+00:00
[2025-02-20T16:56:54.143+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:54.143+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:54.143+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:54.143+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:54.144+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-15T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:54.144+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:54.144+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:54.863+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:54.887+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:54.888+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:54.895+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:54.895+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:54.993+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:55.042+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:55.073+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-15T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:55.547+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-15T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:55.551+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-15T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:55.151912+00:00, run_end_date=2025-02-20 15:56:55.246649+00:00, run_duration=0.094737, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=86, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:54.143565+00:00, queued_by_job_id=3, pid=37336
[2025-02-20T16:56:56.416+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-15 00:00:00+00:00, run_after=2024-04-16 00:00:00+00:00
[2025-02-20T16:56:56.438+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-14 00:00:00+00:00: scheduled__2024-04-14T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:48.928661+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-14T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:56.438+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-14 00:00:00+00:00, run_id=scheduled__2024-04-14T00:00:00+00:00, run_start_date=2025-02-20 15:51:48.934759+00:00, run_end_date=2025-02-20 15:56:56.438722+00:00, run_duration=307.503963, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-14 00:00:00+00:00, data_interval_end=2024-04-15 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:56.440+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-15 00:00:00+00:00, run_after=2024-04-16 00:00:00+00:00
[2025-02-20T16:56:56.444+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:56.444+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:56.444+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:56.445+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:56.445+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-16T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:56.445+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:56.446+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:57.180+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:57.204+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:57.205+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:57.212+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:57.212+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:57.310+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:57.396+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:57.442+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-16T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:56:57.867+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-16T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:56:57.872+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-16T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:57.529923+00:00, run_end_date=2025-02-20 15:56:57.635312+00:00, run_duration=0.105389, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=87, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:56.444607+00:00, queued_by_job_id=3, pid=37341
[2025-02-20T16:56:58.726+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-16 00:00:00+00:00, run_after=2024-04-17 00:00:00+00:00
[2025-02-20T16:56:58.748+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-15 00:00:00+00:00: scheduled__2024-04-15T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:51.151141+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-15T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:56:58.748+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-15 00:00:00+00:00, run_id=scheduled__2024-04-15T00:00:00+00:00, run_start_date=2025-02-20 15:51:51.156266+00:00, run_end_date=2025-02-20 15:56:58.748474+00:00, run_duration=307.592208, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-15 00:00:00+00:00, data_interval_end=2024-04-16 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:56:58.750+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-16 00:00:00+00:00, run_after=2024-04-17 00:00:00+00:00
[2025-02-20T16:56:58.753+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:58.754+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:56:58.754+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [scheduled]>
[2025-02-20T16:56:58.754+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:56:58.755+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-17T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:56:58.755+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:58.756+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:56:59.494+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:56:59.518+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:56:59.519+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:59.526+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:59.526+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:56:59.621+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:56:59.672+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:56:59.703+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-17T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:57:00.177+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-17T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:57:00.180+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-17T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:56:59.781606+00:00, run_end_date=2025-02-20 15:56:59.901002+00:00, run_duration=0.119396, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=88, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:56:58.754331+00:00, queued_by_job_id=3, pid=37350
[2025-02-20T16:57:01.067+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-17 00:00:00+00:00, run_after=2024-04-18 00:00:00+00:00
[2025-02-20T16:57:01.086+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-16 00:00:00+00:00: scheduled__2024-04-16T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:53.420177+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-16T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:57:01.086+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-16 00:00:00+00:00, run_id=scheduled__2024-04-16T00:00:00+00:00, run_start_date=2025-02-20 15:51:53.425956+00:00, run_end_date=2025-02-20 15:57:01.086912+00:00, run_duration=307.660956, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-16 00:00:00+00:00, data_interval_end=2024-04-17 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:57:01.088+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-17 00:00:00+00:00, run_after=2024-04-18 00:00:00+00:00
[2025-02-20T16:57:01.092+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [scheduled]>
[2025-02-20T16:57:01.092+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:57:01.092+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [scheduled]>
[2025-02-20T16:57:01.092+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:57:01.093+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-18T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:57:01.093+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:57:01.093+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:57:01.849+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:57:01.873+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:57:01.874+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:57:01.881+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:57:01.881+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:57:01.977+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:57:02.028+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:57:02.065+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-18T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
[2025-02-20T16:57:02.533+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-18T00:00:00+00:00', try_number=2, map_index=-1)
[2025-02-20T16:57:02.538+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=etl_example, task_id=extract_data, run_id=scheduled__2024-04-18T00:00:00+00:00, map_index=-1, run_start_date=2025-02-20 15:57:02.143230+00:00, run_end_date=2025-02-20 15:57:02.247114+00:00, run_duration=0.103884, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=89, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-02-20 15:57:01.092571+00:00, queued_by_job_id=3, pid=37359
[2025-02-20T16:57:03.576+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-18 00:00:00+00:00, run_after=2024-04-19 00:00:00+00:00
[2025-02-20T16:57:03.595+0100] {dagrun.py:823} ERROR - Marking run <DagRun etl_example @ 2024-04-17 00:00:00+00:00: scheduled__2024-04-17T00:00:00+00:00, state:running, queued_at: 2025-02-20 15:51:55.612448+00:00. externally triggered: False> failed
Dag run  in failure state
Dag information:etl_example Run id: scheduled__2024-04-17T00:00:00+00:00 external trigger: False
Failed with message: task_failure
[2025-02-20T16:57:03.595+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=etl_example, execution_date=2024-04-17 00:00:00+00:00, run_id=scheduled__2024-04-17T00:00:00+00:00, run_start_date=2025-02-20 15:51:55.617913+00:00, run_end_date=2025-02-20 15:57:03.595474+00:00, run_duration=307.977561, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-17 00:00:00+00:00, data_interval_end=2024-04-18 00:00:00+00:00, dag_hash=01075acecffac600a262bfe0b3565714
[2025-02-20T16:57:03.597+0100] {dag.py:4180} INFO - Setting next_dagrun for etl_example to 2024-04-18 00:00:00+00:00, run_after=2024-04-19 00:00:00+00:00
[2025-02-20T16:57:03.601+0100] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [scheduled]>
[2025-02-20T16:57:03.601+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 0/16 running and queued tasks
[2025-02-20T16:57:03.601+0100] {scheduler_job_runner.py:507} INFO - DAG etl_example has 1/16 running and queued tasks
[2025-02-20T16:57:03.601+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [scheduled]>
	<TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [scheduled]>
[2025-02-20T16:57:03.602+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [scheduled]>, <TaskInstance: etl_example.extract_data scheduled__2024-04-20T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-20T16:57:03.602+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-19T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:57:03.603+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:57:03.603+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='etl_example', task_id='extract_data', run_id='scheduled__2024-04-20T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-20T16:57:03.603+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:57:03.604+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'etl_example', 'extract_data', 'scheduled__2024-04-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py']
[2025-02-20T16:57:04.326+0100] {dagbag.py:588} INFO - Filling up the DagBag from /Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/airflow/dags/etl_dag.py
[2025-02-20T16:57:04.357+0100] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File "/Users/ajlakorman/Desktop/Data-Mining-Projects/Apache_Airflow/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2025-02-20T16:57:04.357+0100] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:57:04.365+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:57:04.366+0100] {tutorial_taskflow_api_virtualenv.py:29} WARNING - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.
[2025-02-20T16:57:04.542+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2025-02-20T16:57:04.594+0100] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2025-02-20T16:57:04.626+0100] {task_command.py:467} INFO - Running <TaskInstance: etl_example.extract_data scheduled__2024-04-19T00:00:00+00:00 [queued]> on host ajlas-macbook-air.local
